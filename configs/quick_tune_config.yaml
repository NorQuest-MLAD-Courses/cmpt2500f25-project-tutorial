# Quick Tuning Configuration for Fast Testing
# This config uses smaller hyperparameter grids for rapid experimentation
# Estimated time: 10-30 minutes for all models

# Model configuration
model:
  type: random_forest
  random_state: 42

# Default hyperparameters (same as train_config.yaml)
hyperparameters:
  logistic_regression:
    C: 1.0
    max_iter: 1000
    penalty: l2
    solver: lbfgs
    class_weight: null

  random_forest:
    n_estimators: 100
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    max_features: sqrt
    bootstrap: true
    class_weight: null

  decision_tree:
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    criterion: gini
    splitter: best
    max_features: null
    class_weight: null

  adaboost:
    n_estimators: 50
    learning_rate: 1.0
    algorithm: SAMME.R

  gradient_boosting:
    n_estimators: 100
    learning_rate: 0.1
    max_depth: 3
    min_samples_split: 2
    min_samples_leaf: 1
    subsample: 1.0
    max_features: null

  voting_classifier:
    voting: soft
    estimators:
      - gradient_boosting
      - logistic_regression
      - adaboost

# Training configuration
training:
  tune_hyperparameters: false
  test_size: 0.2
  random_state: 42
  stratify: true

# Paths
paths:
  data:
    input: data/processed/preprocessed_data.npy
    raw: data/raw
    processed: data/processed
  models:
    output_dir: models

# DVC configuration
dvc:
  remote: origin
  data_version: v1.0
  track_models: false

# MLflow configuration
mlflow:
  tracking_uri: null
  experiment_name: quick-tuning
  run_name_prefix: null
  artifact_location: null
  log_models: true
  log_artifacts: true
  tags:
    project: telecom-churn
    team: ml-team
    environment: development
    tuning_mode: quick
  autolog: false

# Quick Hyperparameter Tuning Grids
# Reduced search space for rapid testing and development
tuning_grids:
  logistic_regression:
    C: [0.01, 0.1, 1.0, 10.0]
    penalty: [l1, l2]
    solver: [liblinear, saga]
    max_iter: [1000]
    class_weight: [null, balanced]
    # Total combinations: 4 * 2 * 2 * 1 * 2 = 32

  random_forest:
    n_estimators: [50, 100, 200]
    max_depth: [10, 20, null]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
    max_features: [sqrt, log2]
    bootstrap: [true]
    class_weight: [null, balanced]
    # Total combinations: 3 * 3 * 3 * 3 * 2 * 1 * 2 = 324

  decision_tree:
    max_depth: [5, 10, 20, null]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
    criterion: [gini, entropy]
    splitter: [best]
    max_features: [sqrt, null]
    class_weight: [null, balanced]
    # Total combinations: 4 * 3 * 3 * 2 * 1 * 2 * 2 = 288

  adaboost:
    n_estimators: [50, 100, 200]
    learning_rate: [0.01, 0.1, 1.0]
    algorithm: [SAMME.R]
    # Total combinations: 3 * 3 * 1 = 9

  gradient_boosting:
    n_estimators: [50, 100, 200]
    learning_rate: [0.01, 0.1, 0.2]
    max_depth: [3, 5, 7]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2]
    subsample: [0.8, 1.0]
    max_features: [sqrt, null]
    # Total combinations: 3 * 3 * 3 * 3 * 2 * 2 * 2 = 648

# Cross-validation settings
cross_validation:
  cv_folds: 3  # Reduced from 5 for faster testing
  scoring: accuracy
  n_jobs: -1
  verbose: 1

# Notes:
# - Total search space: ~1,301 combinations
# - With 3-fold CV: ~3,903 individual model fits
# - Estimated time: 10-30 minutes on 8-core CPU
# - Use this config for rapid prototyping and testing
# - For production models, use configs/train_config.yaml
