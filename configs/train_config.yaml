# Training Configuration for Telecom Churn Prediction
# This file defines model hyperparameters, tuning grids, and training settings

# Model configuration
model:
  type: random_forest  # Default model type
  random_state: 42

# Default hyperparameters (used when tune_hyperparameters=false)
hyperparameters:
  logistic_regression:
    C: 1.0
    max_iter: 1000
    penalty: l2
    solver: lbfgs
    class_weight: null

  random_forest:
    n_estimators: 100
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    max_features: sqrt
    bootstrap: true
    class_weight: null

  decision_tree:
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    criterion: gini
    splitter: best
    max_features: null
    class_weight: null

  adaboost:
    n_estimators: 50
    learning_rate: 1.0
    algorithm: SAMME.R

  gradient_boosting:
    n_estimators: 100
    learning_rate: 0.1
    max_depth: 3
    min_samples_split: 2
    min_samples_leaf: 1
    subsample: 1.0
    max_features: null

  voting_classifier:
    voting: soft
    estimators:
      - gradient_boosting
      - logistic_regression
      - adaboost

# Training configuration
training:
  tune_hyperparameters: false
  test_size: 0.2
  random_state: 42
  stratify: true

# Paths
paths:
  data:
    input: data/processed/preprocessed_data.npy  # DVC-tracked
    raw: data/raw  # DVC-tracked
    processed: data/processed  # DVC-tracked
  models:
    output_dir: models  # Local storage + MLflow artifacts

# DVC configuration
dvc:
  remote: origin  # DagsHub remote
  data_version: v1.0
  track_models: false  # Use MLflow for models

# MLflow configuration
mlflow:
  tracking_uri: null  # null = local mlruns/, or set to DagsHub URL
  experiment_name: telecom-churn-prediction
  run_name_prefix: null  # null = auto-generate, or set custom prefix
  artifact_location: null  # null = use default (mlruns/), or set custom
  log_models: true  # Log models to MLflow
  log_artifacts: true  # Log additional artifacts (plots, etc.)
  tags:
    project: telecom-churn
    team: ml-team
    environment: development
  autolog: false  # Use manual logging for control

# Comprehensive Hyperparameter Tuning Grids
# These grids are used when tune_hyperparameters=true
# Designed for optimal model performance through extensive search
tuning_grids:
  logistic_regression:
    # Regularization strength (inverse) - wider range for better exploration
    C: [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 50.0, 100.0, 500.0]
    # Regularization penalty
    penalty: [l1, l2]
    # Optimization algorithms supporting both penalties
    solver: [liblinear, saga]
    # Maximum iterations for convergence
    max_iter: [1000, 2000]
    # Class weighting for imbalanced datasets
    class_weight: [null, balanced]
    # Total combinations: 12 * 2 * 2 * 2 * 2 = 192

  random_forest:
    # Number of trees in the forest
    n_estimators: [50, 100, 150, 200, 300, 500]
    # Maximum depth of trees (null = unlimited)
    max_depth: [5, 10, 15, 20, 30, 50, null]
    # Minimum samples required to split an internal node
    min_samples_split: [2, 5, 10, 15, 20]
    # Minimum samples required to be at a leaf node
    min_samples_leaf: [1, 2, 4, 6, 8]
    # Number of features to consider for best split
    max_features: [sqrt, log2, null]
    # Whether to use bootstrap samples
    bootstrap: [true, false]
    # Class weighting strategies
    class_weight: [null, balanced, balanced_subsample]
    # Total combinations: 6 * 7 * 5 * 5 * 3 * 2 * 3 = 18,900

  decision_tree:
    # Maximum depth of the tree
    max_depth: [3, 5, 7, 10, 15, 20, 30, 50, null]
    # Minimum samples required to split
    min_samples_split: [2, 5, 10, 15, 20, 30]
    # Minimum samples required at leaf node
    min_samples_leaf: [1, 2, 4, 6, 8, 10, 15]
    # Function to measure quality of split
    criterion: [gini, entropy, log_loss]
    # Strategy for choosing split at each node
    splitter: [best, random]
    # Number of features to consider
    max_features: [sqrt, log2, null]
    # Class weighting
    class_weight: [null, balanced]
    # Total combinations: 9 * 6 * 7 * 3 * 2 * 3 * 2 = 13,608

  adaboost:
    # Number of boosting stages
    n_estimators: [50, 75, 100, 150, 200, 300, 500]
    # Learning rate (shrinks contribution of each classifier)
    learning_rate: [0.001, 0.01, 0.05, 0.1, 0.2, 0.5, 0.8, 1.0, 1.5, 2.0]
    # Boosting algorithm
    algorithm: [SAMME, SAMME.R]
    # Total combinations: 7 * 10 * 2 = 140

  gradient_boosting:
    # Number of boosting stages
    n_estimators: [50, 100, 150, 200, 300, 500]
    # Learning rate shrinks contribution of each tree
    learning_rate: [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5]
    # Maximum depth of individual trees
    max_depth: [3, 4, 5, 6, 7, 8, 10]
    # Minimum samples required to split
    min_samples_split: [2, 5, 10, 15, 20]
    # Minimum samples required at leaf
    min_samples_leaf: [1, 2, 4, 6, 8]
    # Fraction of samples for fitting individual trees
    subsample: [0.6, 0.7, 0.8, 0.9, 1.0]
    # Number of features for best split
    max_features: [sqrt, log2, null]
    # Total combinations: 6 * 8 * 7 * 5 * 5 * 5 * 3 = 126,000

# Cross-validation settings
cross_validation:
  cv_folds: 5  # Number of cross-validation folds
  scoring: accuracy  # Metric to optimize during grid search
  n_jobs: -1  # Use all available CPU cores
  verbose: 1  # Verbosity level for GridSearchCV

# Notes:
# - Total search space for all models: ~158,840 combinations
# - With 5-fold CV: ~794,200 individual model fits
# - Estimated time: 6-20 hours on 8-core CPU
# - Recommendation: Train models individually, not all at once
# - For faster tuning, use configs/quick_tune_config.yaml
