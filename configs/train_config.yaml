# configs/train_config.yaml

# Model configuration
model:
  type: random_forest  # Options: logistic_regression, random_forest, decision_tree, adaboost, gradient_boosting, voting_classifier
  random_state: 42

# Model hyperparameters (used when tune=false)
hyperparameters:
  logistic_regression:
    C: 1.0
    max_iter: 1000
    penalty: l2
    solver: lbfgs
  
  random_forest:
    n_estimators: 100
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    max_features: sqrt
  
  decision_tree:
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    criterion: gini
  
  adaboost:
    n_estimators: 50
    learning_rate: 1.0
    algorithm: SAMME.R
  
  gradient_boosting:
    n_estimators: 100
    learning_rate: 0.1
    max_depth: 3
    min_samples_split: 2
    subsample: 1.0
  
  voting_classifier:
    voting: soft
    estimators:
      - gradient_boosting
      - logistic_regression
      - adaboost

# Training configuration
training:
  tune_hyperparameters: false  # Set to true to use GridSearchCV
  test_size: 0.2
  random_state: 42
  stratify: true

# Paths
paths:
  data:
    input: data/processed/preprocessed_data.npy
  models:
    output_dir: models

# Hyperparameter tuning grids (used when tune=true)
tuning_grids:
  random_forest:
    n_estimators: [50, 100, 200]
    max_depth: [10, 20, 30, null]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
    max_features: [sqrt, log2]
  
  logistic_regression:
    C: [0.001, 0.01, 0.1, 1, 10, 100]
    penalty: [l1, l2]
    solver: [liblinear, saga]
  
  gradient_boosting:
    n_estimators: [50, 100, 200]
    learning_rate: [0.01, 0.1, 0.2]
    max_depth: [3, 5, 7]
    min_samples_split: [2, 5, 10]
    subsample: [0.8, 0.9, 1.0]

# Cross-validation settings
cross_validation:
  cv_folds: 5
  scoring: accuracy
  n_jobs: -1
  verbose: 1