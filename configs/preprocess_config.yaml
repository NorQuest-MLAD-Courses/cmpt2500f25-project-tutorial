# Preprocessing Configuration for Telecom Churn Prediction
# This file defines data preprocessing parameters and pipeline settings

# Data configuration
data:
  input_file: data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv
  output_dir: data/processed
  target_column: Churn
  id_column: customerID

# DVC configuration
dvc:
  track_raw: true
  track_processed: true
  remote: origin  # DagsHub remote
  data_version: v1.0

# MLflow configuration
mlflow:
  track_preprocessing: false  # Set to true to log preprocessing as MLflow run
  experiment_name: telecom-churn-preprocessing
  log_artifacts: true  # Log preprocessing artifacts (pipeline, encoders)

# Missing value handling
missing_values:
  strategy: drop  # Strategy: 'drop', 'impute', or 'fill'
  threshold: 0.5  # Threshold for dropping columns with missing values
  fill_value: 0   # Value to use for filling (if strategy is 'fill')

# Feature scaling
scaling:
  method: standard  # Options: 'standard', 'minmax', 'robust', or 'none'
  with_mean: true   # Center data before scaling
  with_std: true    # Scale data to unit variance

# Feature columns
# These are automatically detected, but can be overridden here
features:
  categorical:
    - gender
    - Partner
    - Dependents
    - PhoneService
    - MultipleLines
    - InternetService
    - OnlineSecurity
    - OnlineBackup
    - DeviceProtection
    - TechSupport
    - StreamingTV
    - StreamingMovies
    - Contract
    - PaperlessBilling
    - PaymentMethod

  numerical:
    - SeniorCitizen
    - tenure
    - MonthlyCharges
    - TotalCharges

  # Columns to drop from the dataset
  drop_columns:
    - customerID

# Target encoding
target:
  column: Churn
  encoding:
    positive_class: "Yes"  # Value indicating churn
    negative_class: "No"   # Value indicating no churn

# Train-test split
split:
  test_size: 0.2      # Proportion of data for testing (0.0 to 1.0)
  random_state: 42    # Random seed for reproducibility
  stratify: true      # Whether to stratify split by target variable
  shuffle: true       # Whether to shuffle data before splitting

# Pipeline options
pipeline:
  use_sklearn_pipeline: true  # Use sklearn ColumnTransformer pipeline
  save_artifacts: true        # Save preprocessing artifacts (pipeline, encoders)

  # Output filenames
  output_files:
    data: preprocessed_data.npy          # Preprocessed train/test data
    pipeline: preprocessing_pipeline.pkl  # Fitted preprocessing pipeline
    label_encoder: label_encoder.pkl      # Target label encoder

# Encoding options
encoding:
  categorical:
    method: onehot              # Options: 'onehot', 'label', 'ordinal'
    handle_unknown: ignore      # How to handle unknown categories in transform
    drop_first: false           # Whether to drop first category (avoid multicollinearity)
    sparse_output: false        # Whether to use sparse matrices

  # Special handling for specific features
  ordinal_features: []  # List of features to encode ordinally (if any)
  ordinal_categories: {}  # Dict mapping ordinal features to their category order

# Data validation
validation:
  check_missing: true         # Check for missing values
  check_duplicates: true      # Check for duplicate rows
  check_dtypes: true          # Validate data types
  check_target_balance: true  # Check target class balance

  # Thresholds for warnings
  min_target_ratio: 0.1   # Warn if minority class < 10%
  max_missing_ratio: 0.3  # Warn if any column has > 30% missing

# Logging configuration
logging:
  level: INFO                  # Logging level: DEBUG, INFO, WARNING, ERROR
  log_statistics: true         # Log dataset statistics
  log_transformations: true    # Log each transformation step
  log_class_distribution: true # Log target class distribution

# Notes:
# - SeniorCitizen is treated as numerical (0/1) but could be categorical
# - TotalCharges may have missing values for new customers (handled automatically)
# - Config can be overridden via CLI arguments
# - Use --no-scale flag to disable feature scaling
# - Use --legacy flag for non-pipeline preprocessing
