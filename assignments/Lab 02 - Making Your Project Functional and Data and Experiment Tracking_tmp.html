<!DOCTYPE html>
<html>
<head>
<title>Lab 02 - Making Your Project Functional and Data and Experiment Tracking.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="lab-02-making-your-project-functional-with-data-and-experiment-tracking">Lab 02: Making Your Project Functional with Data and Experiment Tracking</h1>
<h2 id="overview">Overview</h2>
<p>Welcome to Lab 02! In this lab, you'll transform your well-structured ML project from Lab 01 into a fully functional, production-ready system with proper environment management, command-line interfaces, data versioning, and experiment tracking.</p>
<p>In Lab 01, you created a clean project structure and modularized your code. Now it's time to add the tools and practices that make your project:</p>
<ul>
<li><strong>Reproducible</strong> - Anyone can recreate your environment and results</li>
<li><strong>Accessible</strong> - Command-line interfaces make your code easy to use</li>
<li><strong>Trackable</strong> - Version control for data and experiments</li>
<li><strong>Testable</strong> - Automated tests ensure code quality</li>
<li><strong>Production-ready</strong> - Ready for deployment and collaboration</li>
</ul>
<h3 id="learning-objectives">Learning Objectives</h3>
<p>By the end of this lab, you will:</p>
<ol>
<li><strong>Environment Management</strong>: Understand computational environments and create isolated Python virtual environments</li>
<li><strong>Dependency Management</strong>: Create and manage <code>requirements.txt</code> files</li>
<li><strong>CLI Development</strong>: Build command-line interfaces using argparse</li>
<li><strong>Best Practices</strong>: Implement hyperparameter tuning and scikit-learn pipelines</li>
<li><strong>Configuration Management</strong>: Use YAML files for flexible configuration</li>
<li><strong>Data Versioning</strong>: Use DVC (Data Version Control) with DagsHub</li>
<li><strong>Experiment Tracking</strong>: Use MLflow to track experiments and model performance</li>
<li><strong>Testing</strong>: Write unit tests with pytest</li>
</ol>
<h3 id="prerequisites">Prerequisites</h3>
<ul>
<li>Completed Lab 01 (modular project structure)</li>
<li>Python 3.12.x installed</li>
<li>Git repository set up</li>
<li>Basic understanding of command line</li>
</ul>
<hr>
<h2 id="part-1-understanding-computational-environments">Part 1: Understanding Computational Environments</h2>
<h3 id="what-is-a-computational-environment">What is a Computational Environment?</h3>
<p>A <strong>computational environment</strong> is the complete software ecosystem where your code runs, including:</p>
<ul>
<li>Operating system</li>
<li>Python interpreter version</li>
<li>Installed packages and their versions</li>
<li>System libraries</li>
<li>Environment variables</li>
</ul>
<h3 id="the-%22it-works-on-my-machine%22-problem">The &quot;It Works on My Machine&quot; Problem</h3>
<p>Consider this scenario:</p>
<pre class="hljs"><code><div>You: &quot;My code works perfectly!&quot;
Teammate: &quot;It crashes on my machine...&quot;
You: &quot;But it works for me! 🤔&quot;
</div></code></pre>
<p><strong>Why does this happen?</strong></p>
<ul>
<li>Different Python versions (3.10 vs 3.12)</li>
<li>Different package versions (numpy 1.24 vs 2.3)</li>
<li>Missing dependencies</li>
<li>Different operating systems</li>
</ul>
<h3 id="why-standardized-environments-matter">Why Standardized Environments Matter</h3>
<p><strong>In Development</strong>:</p>
<ul>
<li>Ensures code works identically for all team members</li>
<li>Prevents &quot;dependency hell&quot;</li>
<li>Makes onboarding new developers easier</li>
<li>Enables reproducible research</li>
</ul>
<p><strong>In Production</strong>:</p>
<ul>
<li>Guarantees consistent behavior in deployment</li>
<li>Enables rolling back to previous versions</li>
<li>Facilitates automated testing and CI/CD</li>
<li>Prevents production failures due to environment mismatches</li>
</ul>
<h3 id="the-solution-containerization-and-virtual-environments">The Solution: Containerization and Virtual Environments</h3>
<p><strong>Long-term solution</strong> (coming in Lab 04): <strong>Docker</strong></p>
<ul>
<li>Containers package your code AND its entire environment</li>
<li>Works identically everywhere (local, cloud, different OS)</li>
<li>Industry standard for deployment</li>
</ul>
<p><strong>Today's solution</strong>: Python <strong>Virtual Environments</strong></p>
<ul>
<li>Isolated Python environment for your project</li>
<li>Project-specific package versions</li>
<li>Prevents conflicts between projects</li>
<li>Lightweight and easy to use</li>
</ul>
<p><strong>Why start with virtual environments</strong>?
Our project currently uses only Python and doesn't require system-level dependencies. A Python virtual environment is sufficient for now. Later, when we deploy to production or need to ensure OS-level consistency, we'll containerize with Docker.</p>
<hr>
<h2 id="part-2-creating-and-managing-virtual-environments">Part 2: Creating and Managing Virtual Environments</h2>
<h3 id="what-is-a-virtual-environment">What is a Virtual Environment?</h3>
<p>A Python virtual environment is an isolated Python installation that:</p>
<ul>
<li>Has its own Python interpreter copy</li>
<li>Has its own <code>site-packages</code> directory (where packages install)</li>
<li>Doesn't interfere with system Python or other projects</li>
<li>Can have different package versions per project</li>
</ul>
<p><strong>Analogy</strong>: Think of it as a separate apartment for each project - each has its own furniture (packages) and doesn't share with others.</p>
<h3 id="creating-a-virtual-environment">Creating a Virtual Environment</h3>
<p>Python includes the <code>venv</code> module for creating virtual environments:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Create a virtual environment named .venv</span>
python -m venv .venv
</div></code></pre>
<p><strong>Breaking down the command</strong>:</p>
<ul>
<li><code>python</code> - Run Python interpreter</li>
<li><code>-m venv</code> - Run the venv module as a script</li>
<li><code>.venv</code> - Name of the directory to create</li>
</ul>
<p><strong>Why <code>.venv</code> (with a dot)?</strong></p>
<ul>
<li>Hidden directory (dot prefix on Unix/Linux/Mac)</li>
<li>Industry convention</li>
<li>Most IDEs auto-detect <code>.venv</code></li>
<li>Clearly indicates it's a virtual environment</li>
</ul>
<p><strong>What gets created?</strong></p>
<pre class="hljs"><code><div>.venv/
├── bin/              # Executables (Mac/Linux)
│   ├── python        # Python interpreter copy
│   ├── pip           # Package installer
│   └── activate      # Activation script
├── Scripts/          # Executables (Windows)
│   ├── python.exe
│   ├── pip.exe
│   └── activate.bat
├── lib/              # Installed packages
│   └── python3.12/
│       └── site-packages/
└── pyvenv.cfg        # Configuration
</div></code></pre>
<h3 id="activating-the-virtual-environment">Activating the Virtual Environment</h3>
<p><strong>On Mac/Linux</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-built_in">source</span> .venv/bin/activate
</div></code></pre>
<p><strong>On Windows (Command Prompt)</strong>:</p>
<pre class="hljs"><code><div>.venv\Scripts\activate.bat
</div></code></pre>
<p><strong>On Windows (PowerShell)</strong>:</p>
<pre class="hljs"><code><div>.venv\Scripts\Activate.ps1
</div></code></pre>
<p><strong>How to tell it's activated?</strong>
Your command prompt changes:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Before activation</span>
user@computer:~/project$

<span class="hljs-comment"># After activation</span>
(.venv) user@computer:~/project$
</div></code></pre>
<p>The <code>(.venv)</code> prefix indicates you're in the virtual environment!</p>
<h3 id="verify-activation">Verify Activation</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Check which Python is being used (should point to .venv)</span>
<span class="hljs-built_in">which</span> python    <span class="hljs-comment"># Mac/Linux</span>
<span class="hljs-built_in">where</span> python    <span class="hljs-comment"># Windows</span>

<span class="hljs-comment"># Output should be something like:</span>
<span class="hljs-comment"># /path/to/your/project/.venv/bin/python</span>

<span class="hljs-comment"># Check Python version</span>
python --version
<span class="hljs-comment"># Python 3.12.12</span>

<span class="hljs-comment"># List installed packages (should be minimal at first)</span>
pip list
<span class="hljs-comment"># Package    Version</span>
<span class="hljs-comment"># ---------- -------</span>
<span class="hljs-comment"># pip        24.0</span>
<span class="hljs-comment"># setuptools 65.5.0</span>
</div></code></pre>
<h3 id="deactivating-the-virtual-environment">Deactivating the Virtual Environment</h3>
<p>When you're done working:</p>
<pre class="hljs"><code><div>deactivate
</div></code></pre>
<p>Your prompt returns to normal:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># After deactivation</span>
user@computer:~/project$
</div></code></pre>
<h3 id="best-practices">Best Practices</h3>
<p>✅ <strong>DO</strong>:</p>
<ul>
<li>Create one virtual environment per project</li>
<li>Name it <code>.venv</code> (convention)</li>
<li>Activate before installing packages</li>
<li>Commit requirements.txt, NOT the .venv folder</li>
<li>Document activation steps in README</li>
</ul>
<p>❌ <strong>DON'T</strong>:</p>
<ul>
<li>Commit the <code>.venv</code> folder to Git</li>
<li>Share virtual environments between projects</li>
<li>Install packages globally</li>
<li>Forget to activate before working</li>
</ul>
<h3 id="adding-venv-to-gitignore">Adding .venv to .gitignore</h3>
<p>Ensure your <code>.gitignore</code> includes:</p>
<pre class="hljs"><code><div># Virtual environments
.venv/
venv/
env/
ENV/
</div></code></pre>
<p>This prevents the (large!) virtual environment from being committed to Git.</p>
<hr>
<h2 id="part-3-dependency-management-with-requirementstxt">Part 3: Dependency Management with requirements.txt</h2>
<h3 id="why-document-dependencies">Why Document Dependencies?</h3>
<p>Imagine you want to share your project:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Your code</span>
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> sklearn
<span class="hljs-keyword">import</span> some_package_you_installed_6_months_ago
</div></code></pre>
<p><strong>Without documentation</strong>:</p>
<ul>
<li>Users don't know what packages to install</li>
<li>Users don't know which versions you used</li>
<li>Code may break with different versions</li>
</ul>
<p><strong>With requirements.txt</strong>:</p>
<ul>
<li>Clear list of all dependencies</li>
<li>Exact or compatible versions specified</li>
<li>One command installs everything</li>
<li>Reproducible environment</li>
</ul>
<h3 id="the-two-approaches">The Two Approaches</h3>
<h4 id="approach-1-pip-freeze-automatic">Approach 1: pip freeze (Automatic)</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Activate your virtual environment</span>
<span class="hljs-built_in">source</span> .venv/bin/activate

<span class="hljs-comment"># Generate requirements.txt with all installed packages</span>
pip freeze &gt; requirements.txt
</div></code></pre>
<p><strong>Example output</strong>:</p>
<pre class="hljs"><code><div>catboost==1.2.8
certifi==2025.1.12
charset-normalizer==3.4.1
contourpy==1.3.1
cycler==0.12.1
dvc==3.63.0
fonttools==4.55.3
idna==3.10
joblib==1.5.2
kiwisolver==1.4.7
...many more...
threadpoolctl==3.6.0
tzdata==2025.2
urllib3==2.3.0
</div></code></pre>
<p><strong>Pros</strong>:</p>
<ul>
<li>Quick and automatic</li>
<li>Captures exact versions</li>
<li>Includes all dependencies</li>
</ul>
<p><strong>Cons</strong>:</p>
<ul>
<li>Lists sub-dependencies (packages you didn't directly install)</li>
<li>Hard to read and understand</li>
<li>Can cause conflicts</li>
<li>Difficult to maintain</li>
</ul>
<h4 id="approach-2-hand-curated-recommended">Approach 2: Hand-Curated (Recommended)</h4>
<p>Manually create <code>requirements.txt</code> with only direct dependencies:</p>
<pre class="hljs"><code><div># Core ML and Data Science
numpy==2.3.4
pandas==2.3.3
scikit-learn==1.7.2

# Model Persistence
joblib==1.5.2

# Visualization
matplotlib==3.10.7
seaborn==0.13.2
plotly==6.3.1

# Advanced Models
xgboost==3.1.1
catboost==1.2.8

# Configuration
PyYAML==6.0.3

# Data Version Control (DagsHub remote - recommended)
dagshub==0.6.3
dvc-s3==3.2.2

# Testing
pytest==8.4.2
pytest-cov==7.0.0

# Development
jupyter==1.1.1
</div></code></pre>
<p><strong>Pros</strong>:</p>
<ul>
<li>Clean and readable</li>
<li>Only direct dependencies</li>
<li>Easy to maintain</li>
<li>Documents what you actually use</li>
<li>Less likely to cause conflicts</li>
</ul>
<p><strong>Cons</strong>:</p>
<ul>
<li>Requires manual maintenance</li>
<li>Need to remember to update when adding packages</li>
</ul>
<h3 id="version-pinning-strategies">Version Pinning Strategies</h3>
<p><strong>Exact pinning (==)</strong>:</p>
<pre class="hljs"><code><div>pandas==2.3.3  # Exact version
</div></code></pre>
<ul>
<li>Most reproducible</li>
<li>Safest for production</li>
<li>May miss bug fixes</li>
</ul>
<p><strong>Compatible release (~=)</strong>:</p>
<pre class="hljs"><code><div>pandas~=2.3.3  # Compatible: &gt;=2.3.3, &lt;2.4.0
</div></code></pre>
<ul>
<li>Allows patch updates</li>
<li>Balance of stability and updates</li>
</ul>
<p><strong>Minimum version (&gt;=)</strong>:</p>
<pre class="hljs"><code><div>pandas&gt;=2.3.0  # Any version 2.3.0 or higher
</div></code></pre>
<ul>
<li>Most flexible</li>
<li>Can break compatibility</li>
<li>Use with caution</li>
</ul>
<p><strong>Recommended approach</strong>:</p>
<pre class="hljs"><code><div># Production code - exact pinning
pandas==2.3.3

# Library development - compatible release
pandas~=2.3.3

# Quick experiments - minimum version
pandas&gt;=2.3.0
</div></code></pre>
<h3 id="installing-from-requirementstxt">Installing from requirements.txt</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Activate virtual environment first!</span>
<span class="hljs-built_in">source</span> .venv/bin/activate

<span class="hljs-comment"># Install all requirements</span>
pip install -r requirements.txt

<span class="hljs-comment"># Upgrade pip first (recommended)</span>
pip install --upgrade pip
pip install -r requirements.txt

<span class="hljs-comment"># Force reinstall if needed</span>
pip install --force-reinstall -r requirements.txt
</div></code></pre>
<h3 id="best-practices-for-requirementstxt">Best Practices for requirements.txt</h3>
<p>✅ <strong>DO</strong>:</p>
<ul>
<li>Use hand-curated approach</li>
<li>Group packages logically with comments</li>
<li>Pin exact versions for production</li>
<li>Update when adding new packages</li>
<li>Test installation on clean environment</li>
</ul>
<p>❌ <strong>DON'T</strong>:</p>
<ul>
<li>Use <code>pip freeze</code> blindly</li>
<li>Include OS-specific packages</li>
<li>Commit without testing</li>
<li>Use vague version specifiers in production</li>
</ul>
<hr>
<h2 id="part-4-command-line-interfaces-with-argparse">Part 4: Command-Line Interfaces with argparse</h2>
<h3 id="why-cli-interfaces-matter">Why CLI Interfaces Matter</h3>
<p><strong>Before CLI (hard-coded values)</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># train.py</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>
    data_path = <span class="hljs-string">"data/processed/train.npy"</span>  <span class="hljs-comment"># Hard-coded!</span>
    model_type = <span class="hljs-string">"random_forest"</span>            <span class="hljs-comment"># Hard-coded!</span>
    tune = <span class="hljs-literal">False</span>                             <span class="hljs-comment"># Hard-coded!</span>
    
    <span class="hljs-comment"># To change: Edit the file, save, run again</span>
</div></code></pre>
<p><strong>After CLI (argparse)</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Flexible usage from command line</span>
python -m src.train --data data/processed/train.npy --model random_forest --tune
python -m src.train --data data/processed/train.npy --model logistic_regression
python -m src.train --data other_data.npy --model gradient_boosting --tune
</div></code></pre>
<p><strong>Benefits</strong>:</p>
<ul>
<li>✅ No code changes needed for different runs</li>
<li>✅ Easy to automate (scripts, CI/CD)</li>
<li>✅ Standard interface everyone understands</li>
<li>✅ Self-documenting with <code>--help</code></li>
<li>✅ Type checking and validation</li>
</ul>
<h3 id="introduction-to-argparse">Introduction to argparse</h3>
<p>Python's <code>argparse</code> module is the standard library tool for creating CLI interfaces.</p>
<p><strong>Basic example</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> argparse

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-comment"># Create parser</span>
    parser = argparse.ArgumentParser(
        description=<span class="hljs-string">'Train a machine learning model'</span>
    )
    
    <span class="hljs-comment"># Add arguments</span>
    parser.add_argument(<span class="hljs-string">'--data'</span>, type=str, required=<span class="hljs-literal">True</span>,
                       help=<span class="hljs-string">'Path to training data'</span>)
    parser.add_argument(<span class="hljs-string">'--model'</span>, type=str, default=<span class="hljs-string">'random_forest'</span>,
                       help=<span class="hljs-string">'Model type to train'</span>)
    parser.add_argument(<span class="hljs-string">'--tune'</span>, action=<span class="hljs-string">'store_true'</span>,
                       help=<span class="hljs-string">'Enable hyperparameter tuning'</span>)
    
    <span class="hljs-comment"># Parse arguments</span>
    args = parser.parse_args()
    
    <span class="hljs-comment"># Use arguments</span>
    print(<span class="hljs-string">f"Data: <span class="hljs-subst">{args.data}</span>"</span>)
    print(<span class="hljs-string">f"Model: <span class="hljs-subst">{args.model}</span>"</span>)
    print(<span class="hljs-string">f"Tune: <span class="hljs-subst">{args.tune}</span>"</span>)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    main()
</div></code></pre>
<p><strong>Usage</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Get help</span>
python train.py --<span class="hljs-built_in">help</span>

<span class="hljs-comment"># Run with arguments</span>
python train.py --data data/train.npy --model random_forest
python train.py --data data/train.npy --model logistic_regression --tune
</div></code></pre>
<h3 id="argument-types">Argument Types</h3>
<p><strong>1. Required arguments</strong>:</p>
<pre class="hljs"><code><div>parser.add_argument(<span class="hljs-string">'--data'</span>, type=str, required=<span class="hljs-literal">True</span>,
                   help=<span class="hljs-string">'Path to training data'</span>)
</div></code></pre>
<p><strong>2. Optional arguments with defaults</strong>:</p>
<pre class="hljs"><code><div>parser.add_argument(<span class="hljs-string">'--model'</span>, type=str, default=<span class="hljs-string">'random_forest'</span>,
                   help=<span class="hljs-string">'Model type'</span>)
</div></code></pre>
<p><strong>3. Boolean flags</strong>:</p>
<pre class="hljs"><code><div>parser.add_argument(<span class="hljs-string">'--tune'</span>, action=<span class="hljs-string">'store_true'</span>,
                   help=<span class="hljs-string">'Enable tuning'</span>)
</div></code></pre>
<ul>
<li>Present → True</li>
<li>Absent → False</li>
</ul>
<p><strong>4. Choices (restricted values)</strong>:</p>
<pre class="hljs"><code><div>parser.add_argument(<span class="hljs-string">'--model'</span>, type=str,
                   choices=[<span class="hljs-string">'random_forest'</span>, <span class="hljs-string">'logistic_regression'</span>, <span class="hljs-string">'decision_tree'</span>],
                   help=<span class="hljs-string">'Model type'</span>)
</div></code></pre>
<p><strong>5. Multiple values</strong>:</p>
<pre class="hljs"><code><div>parser.add_argument(<span class="hljs-string">'--features'</span>, nargs=<span class="hljs-string">'+'</span>, type=str,
                   help=<span class="hljs-string">'Feature columns'</span>)
</div></code></pre>
<h3 id="complete-training-cli-example">Complete Training CLI Example</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># src/train.py</span>
<span class="hljs-keyword">import</span> argparse
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>
    parser = argparse.ArgumentParser(
        description=<span class="hljs-string">'Train machine learning models for churn prediction'</span>,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=<span class="hljs-string">'''
Examples:
  # Train Random Forest with default parameters
  python -m src.train --data data/processed/preprocessed_data.npy --model random_forest
  
  # Train with hyperparameter tuning
  python -m src.train --data data/processed/preprocessed_data.npy --model random_forest --tune
  
  # Train all models
  python -m src.train --data data/processed/preprocessed_data.npy --model all
        '''</span>
    )
    
    <span class="hljs-comment"># Required arguments</span>
    parser.add_argument(<span class="hljs-string">'--data'</span>, type=str, required=<span class="hljs-literal">True</span>,
                       help=<span class="hljs-string">'Path to preprocessed training data (.npy file)'</span>)
    
    <span class="hljs-comment"># Optional arguments</span>
    parser.add_argument(<span class="hljs-string">'--model'</span>, type=str, 
                       default=<span class="hljs-string">'random_forest'</span>,
                       choices=[<span class="hljs-string">'logistic_regression'</span>, <span class="hljs-string">'random_forest'</span>, 
                               <span class="hljs-string">'decision_tree'</span>, <span class="hljs-string">'adaboost'</span>, 
                               <span class="hljs-string">'gradient_boosting'</span>, <span class="hljs-string">'voting_classifier'</span>, <span class="hljs-string">'all'</span>],
                       help=<span class="hljs-string">'Model type to train (default: random_forest)'</span>)
    
    parser.add_argument(<span class="hljs-string">'--tune'</span>, action=<span class="hljs-string">'store_true'</span>,
                       help=<span class="hljs-string">'Enable hyperparameter tuning with GridSearchCV'</span>)
    
    parser.add_argument(<span class="hljs-string">'--output-dir'</span>, type=str, default=<span class="hljs-string">'models'</span>,
                       help=<span class="hljs-string">'Directory to save trained models (default: models)'</span>)
    
    <span class="hljs-comment"># Parse arguments</span>
    args = parser.parse_args()
    
    <span class="hljs-comment"># Load data</span>
    print(<span class="hljs-string">f"\n<span class="hljs-subst">{<span class="hljs-string">'='</span>*<span class="hljs-number">60</span>}</span>"</span>)
    print(<span class="hljs-string">f"Training Configuration"</span>)
    print(<span class="hljs-string">f"<span class="hljs-subst">{<span class="hljs-string">'='</span>*<span class="hljs-number">60</span>}</span>"</span>)
    print(<span class="hljs-string">f"Data: <span class="hljs-subst">{args.data}</span>"</span>)
    print(<span class="hljs-string">f"Model: <span class="hljs-subst">{args.model}</span>"</span>)
    print(<span class="hljs-string">f"Hyperparameter Tuning: <span class="hljs-subst">{args.tune}</span>"</span>)
    print(<span class="hljs-string">f"Output Directory: <span class="hljs-subst">{args.output_dir}</span>"</span>)
    print(<span class="hljs-string">f"<span class="hljs-subst">{<span class="hljs-string">'='</span>*<span class="hljs-number">60</span>}</span>\n"</span>)
    
    <span class="hljs-comment"># Load data</span>
    data = np.load(args.data, allow_pickle=<span class="hljs-literal">True</span>).item()
    X_train = data[<span class="hljs-string">'X_train'</span>]
    y_train = data[<span class="hljs-string">'y_train'</span>]
    
    print(<span class="hljs-string">f"Loaded data: X_train shape = <span class="hljs-subst">{X_train.shape}</span>, y_train shape = <span class="hljs-subst">{y_train.shape}</span>"</span>)
    
    <span class="hljs-comment"># Train model(s)</span>
    <span class="hljs-keyword">if</span> args.model == <span class="hljs-string">'all'</span>:
        <span class="hljs-comment"># Train all models</span>
        models = [<span class="hljs-string">'logistic_regression'</span>, <span class="hljs-string">'random_forest'</span>, <span class="hljs-string">'decision_tree'</span>,
                 <span class="hljs-string">'adaboost'</span>, <span class="hljs-string">'gradient_boosting'</span>, <span class="hljs-string">'voting_classifier'</span>]
        <span class="hljs-keyword">for</span> model_type <span class="hljs-keyword">in</span> models:
            train_and_save_model(model_type, X_train, y_train, args.tune, args.output_dir)
    <span class="hljs-keyword">else</span>:
        train_and_save_model(args.model, X_train, y_train, args.tune, args.output_dir)
    
    print(<span class="hljs-string">f"\n<span class="hljs-subst">{<span class="hljs-string">'='</span>*<span class="hljs-number">60</span>}</span>"</span>)
    print(<span class="hljs-string">"Training Complete!"</span>)
    print(<span class="hljs-string">f"<span class="hljs-subst">{<span class="hljs-string">'='</span>*<span class="hljs-number">60</span>}</span>\n"</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_and_save_model</span><span class="hljs-params">(model_type, X_train, y_train, tune, output_dir)</span>:</span>
    <span class="hljs-string">"""Train and save a single model."""</span>
    print(<span class="hljs-string">f"\nTraining <span class="hljs-subst">{model_type}</span>..."</span>)
    start_time = datetime.now()
    
    <span class="hljs-comment"># Train model (import your training functions)</span>
    <span class="hljs-keyword">from</span> src.train <span class="hljs-keyword">import</span> (train_logistic_regression, train_random_forest,
                          train_decision_tree, train_adaboost,
                          train_gradient_boosting, train_voting_classifier)
    
    train_funcs = {
        <span class="hljs-string">'logistic_regression'</span>: train_logistic_regression,
        <span class="hljs-string">'random_forest'</span>: train_random_forest,
        <span class="hljs-string">'decision_tree'</span>: train_decision_tree,
        <span class="hljs-string">'adaboost'</span>: train_adaboost,
        <span class="hljs-string">'gradient_boosting'</span>: train_gradient_boosting,
        <span class="hljs-string">'voting_classifier'</span>: train_voting_classifier
    }
    
    model = train_funcs[model_type](X_train, y_train, tune_hyperparameters=tune)
    
    <span class="hljs-comment"># Save model</span>
    Path(output_dir).mkdir(parents=<span class="hljs-literal">True</span>, exist_ok=<span class="hljs-literal">True</span>)
    timestamp = datetime.now().strftime(<span class="hljs-string">"%Y%m%d_%H%M%S"</span>)
    model_path = <span class="hljs-string">f"<span class="hljs-subst">{output_dir}</span>/<span class="hljs-subst">{model_type}</span>_<span class="hljs-subst">{timestamp}</span>.pkl"</span>
    
    <span class="hljs-keyword">import</span> joblib
    joblib.dump(model, model_path)
    
    elapsed = datetime.now() - start_time
    print(<span class="hljs-string">f"✓ Model saved: <span class="hljs-subst">{model_path}</span>"</span>)
    print(<span class="hljs-string">f"  Training time: <span class="hljs-subst">{elapsed.total_seconds():<span class="hljs-number">.2</span>f}</span>s"</span>)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    main()
</div></code></pre>
<h3 id="cli-best-practices">CLI Best Practices</h3>
<p>✅ <strong>DO</strong>:</p>
<ul>
<li>Provide clear, descriptive help messages</li>
<li>Use sensible defaults</li>
<li>Include usage examples in epilog</li>
<li>Validate input files/paths</li>
<li>Print informative output</li>
<li>Use <code>--help</code> flag</li>
</ul>
<p>❌ <strong>DON'T</strong>:</p>
<ul>
<li>Make everything required (use defaults)</li>
<li>Use cryptic argument names</li>
<li>Skip help messages</li>
<li>Forget to validate inputs</li>
<li>Run silently without output</li>
</ul>
<hr>
<h2 id="part-5-code-enhancements">Part 5: Code Enhancements</h2>
<h3 id="hyperparameter-tuning-with-gridsearchcv">Hyperparameter Tuning with GridSearchCV</h3>
<p><strong>What is Hyperparameter Tuning?</strong></p>
<p>Hyperparameters are settings you choose before training (e.g., number of trees, learning rate). Unlike model parameters (learned during training), hyperparameters significantly affect model performance.</p>
<p><strong>Manual tuning</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Try different values manually</span>
model1 = RandomForestClassifier(n_estimators=<span class="hljs-number">50</span>, max_depth=<span class="hljs-number">5</span>)
model2 = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, max_depth=<span class="hljs-number">10</span>)
model3 = RandomForestClassifier(n_estimators=<span class="hljs-number">200</span>, max_depth=<span class="hljs-number">15</span>)
<span class="hljs-comment"># ... tedious and time-consuming</span>
</div></code></pre>
<p><strong>GridSearchCV (automated)</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier

<span class="hljs-comment"># Define parameter grid</span>
param_grid = {
    <span class="hljs-string">'n_estimators'</span>: [<span class="hljs-number">50</span>, <span class="hljs-number">100</span>, <span class="hljs-number">200</span>],
    <span class="hljs-string">'max_depth'</span>: [<span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">15</span>, <span class="hljs-literal">None</span>],
    <span class="hljs-string">'min_samples_split'</span>: [<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>],
    <span class="hljs-string">'min_samples_leaf'</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>]
}

<span class="hljs-comment"># Create model</span>
model = RandomForestClassifier(random_state=<span class="hljs-number">42</span>)

<span class="hljs-comment"># GridSearchCV tries all combinations with cross-validation</span>
grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    cv=<span class="hljs-number">5</span>,                    <span class="hljs-comment"># 5-fold cross-validation</span>
    scoring=<span class="hljs-string">'accuracy'</span>,
    n_jobs=<span class="hljs-number">-1</span>,              <span class="hljs-comment"># Use all CPU cores</span>
    verbose=<span class="hljs-number">1</span>
)

<span class="hljs-comment"># Fit on data</span>
grid_search.fit(X_train, y_train)

<span class="hljs-comment"># Best model</span>
best_model = grid_search.best_estimator_
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(<span class="hljs-string">f"Best parameters: <span class="hljs-subst">{best_params}</span>"</span>)
print(<span class="hljs-string">f"Best CV score: <span class="hljs-subst">{best_score:<span class="hljs-number">.4</span>f}</span>"</span>)
</div></code></pre>
<p><strong>Implementation in train.py</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_random_forest</span><span class="hljs-params">(X_train, y_train, tune_hyperparameters=False)</span>:</span>
    <span class="hljs-string">"""
    Train Random Forest Classifier.
    
    Args:
        X_train: Training features
        y_train: Training labels
        tune_hyperparameters: If True, use GridSearchCV
        
    Returns:
        Trained model
    """</span>
    <span class="hljs-keyword">if</span> tune_hyperparameters:
        print(<span class="hljs-string">"Training with hyperparameter tuning (this may take a while)..."</span>)
        
        param_grid = {
            <span class="hljs-string">'n_estimators'</span>: [<span class="hljs-number">50</span>, <span class="hljs-number">100</span>, <span class="hljs-number">200</span>],
            <span class="hljs-string">'max_depth'</span>: [<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-literal">None</span>],
            <span class="hljs-string">'min_samples_split'</span>: [<span class="hljs-number">2</span>, <span class="hljs-number">5</span>],
            <span class="hljs-string">'min_samples_leaf'</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]
        }
        
        model = RandomForestClassifier(random_state=<span class="hljs-number">42</span>)
        grid_search = GridSearchCV(
            estimator=model,
            param_grid=param_grid,
            cv=<span class="hljs-number">5</span>,
            scoring=<span class="hljs-string">'accuracy'</span>,
            n_jobs=<span class="hljs-number">-1</span>,
            verbose=<span class="hljs-number">1</span>
        )
        
        grid_search.fit(X_train, y_train)
        
        print(<span class="hljs-string">f"Best parameters: <span class="hljs-subst">{grid_search.best_params_}</span>"</span>)
        print(<span class="hljs-string">f"Best CV score: <span class="hljs-subst">{grid_search.best_score_:<span class="hljs-number">.4</span>f}</span>"</span>)
        
        <span class="hljs-keyword">return</span> grid_search.best_estimator_
    
    <span class="hljs-keyword">else</span>:
        print(<span class="hljs-string">"Training with default hyperparameters..."</span>)
        model = RandomForestClassifier(
            n_estimators=<span class="hljs-number">100</span>,
            max_depth=<span class="hljs-number">10</span>,
            random_state=<span class="hljs-number">42</span>
        )
        model.fit(X_train, y_train)
        <span class="hljs-keyword">return</span> model
</div></code></pre>
<p><strong>Usage</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Fast: default hyperparameters (~30 seconds)</span>
python -m src.train --data data/processed/preprocessed_data.npy --model random_forest

<span class="hljs-comment"># Slow but better: tuned hyperparameters (~5-10 minutes)</span>
python -m src.train --data data/processed/preprocessed_data.npy --model random_forest --tune
</div></code></pre>
<h3 id="scikit-learn-pipelines-for-preprocessing">Scikit-learn Pipelines for Preprocessing</h3>
<p><strong>Problem with manual preprocessing</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Training</span>
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
model.fit(X_train_scaled, y_train)
joblib.dump(scaler, <span class="hljs-string">'scaler.pkl'</span>)  <span class="hljs-comment"># Save separately!</span>
joblib.dump(model, <span class="hljs-string">'model.pkl'</span>)    <span class="hljs-comment"># Save separately!</span>

<span class="hljs-comment"># Prediction</span>
scaler = joblib.load(<span class="hljs-string">'scaler.pkl'</span>)  <span class="hljs-comment"># Load both!</span>
model = joblib.load(<span class="hljs-string">'model.pkl'</span>)
X_new_scaled = scaler.transform(X_new)  <span class="hljs-comment"># Don't forget to scale!</span>
predictions = model.predict(X_new_scaled)
</div></code></pre>
<p><strong>Issues</strong>:</p>
<ul>
<li>❌ Easy to forget preprocessing step</li>
<li>❌ Two files to manage</li>
<li>❌ Can apply wrong scaler</li>
<li>❌ Hard to ensure consistency</li>
</ul>
<p><strong>Solution</strong>: Scikit-learn Pipeline</p>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> Pipeline
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier

<span class="hljs-comment"># Create pipeline</span>
pipeline = Pipeline([
    (<span class="hljs-string">'scaler'</span>, StandardScaler()),
    (<span class="hljs-string">'classifier'</span>, RandomForestClassifier())
])

<span class="hljs-comment"># Train (preprocessing happens automatically)</span>
pipeline.fit(X_train, y_train)

<span class="hljs-comment"># Save (saves entire pipeline as one object)</span>
joblib.dump(pipeline, <span class="hljs-string">'model_pipeline.pkl'</span>)

<span class="hljs-comment"># Predict (preprocessing happens automatically)</span>
pipeline = joblib.load(<span class="hljs-string">'model_pipeline.pkl'</span>)
predictions = pipeline.predict(X_new)  <span class="hljs-comment"># Scaling applied automatically!</span>
</div></code></pre>
<p><strong>Benefits</strong>:</p>
<ul>
<li>✅ Preprocessing and model bundled together</li>
<li>✅ One file to save/load</li>
<li>✅ Automatic preprocessing during prediction</li>
<li>✅ Prevents preprocessing errors</li>
<li>✅ Cleaner, more maintainable code</li>
</ul>
<p><strong>Advanced</strong>: ColumnTransformer for Different Feature Types</p>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> sklearn.compose <span class="hljs-keyword">import</span> ColumnTransformer
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler, OneHotEncoder

<span class="hljs-comment"># Define feature groups</span>
numerical_features = [<span class="hljs-string">'tenure'</span>, <span class="hljs-string">'MonthlyCharges'</span>, <span class="hljs-string">'TotalCharges'</span>]
categorical_features = [<span class="hljs-string">'gender'</span>, <span class="hljs-string">'Contract'</span>, <span class="hljs-string">'PaymentMethod'</span>]

<span class="hljs-comment"># Create preprocessing pipeline</span>
preprocessor = ColumnTransformer(
    transformers=[
        (<span class="hljs-string">'num'</span>, StandardScaler(), numerical_features),
        (<span class="hljs-string">'cat'</span>, OneHotEncoder(handle_unknown=<span class="hljs-string">'ignore'</span>), categorical_features)
    ]
)

<span class="hljs-comment"># Complete pipeline</span>
pipeline = Pipeline([
    (<span class="hljs-string">'preprocessor'</span>, preprocessor),
    (<span class="hljs-string">'classifier'</span>, RandomForestClassifier())
])

<span class="hljs-comment"># Use it</span>
pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
</div></code></pre>
<p><strong>Implementation in preprocess.py</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_preprocessing_pipeline</span><span class="hljs-params">(numerical_features, categorical_features)</span>:</span>
    <span class="hljs-string">"""
    Create sklearn preprocessing pipeline.
    
    Args:
        numerical_features: List of numerical feature names
        categorical_features: List of categorical feature names
        
    Returns:
        ColumnTransformer pipeline
    """</span>
    <span class="hljs-keyword">from</span> sklearn.compose <span class="hljs-keyword">import</span> ColumnTransformer
    <span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler, OneHotEncoder
    
    preprocessor = ColumnTransformer(
        transformers=[
            (<span class="hljs-string">'num'</span>, StandardScaler(), numerical_features),
            (<span class="hljs-string">'cat'</span>, OneHotEncoder(drop=<span class="hljs-string">'first'</span>, handle_unknown=<span class="hljs-string">'ignore'</span>), 
             categorical_features)
        ],
        remainder=<span class="hljs-string">'passthrough'</span>  <span class="hljs-comment"># Keep other columns as-is</span>
    )
    
    <span class="hljs-keyword">return</span> preprocessor

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-comment"># ... load data ...</span>
    
    <span class="hljs-comment"># Define feature groups</span>
    numerical_features = [<span class="hljs-string">'tenure'</span>, <span class="hljs-string">'MonthlyCharges'</span>, <span class="hljs-string">'TotalCharges'</span>, <span class="hljs-string">'SeniorCitizen'</span>]
    categorical_features = [<span class="hljs-string">'gender'</span>, <span class="hljs-string">'Partner'</span>, <span class="hljs-string">'Dependents'</span>, <span class="hljs-string">'PhoneService'</span>,
                           <span class="hljs-string">'MultipleLines'</span>, <span class="hljs-string">'InternetService'</span>, <span class="hljs-string">'OnlineSecurity'</span>,
                           <span class="hljs-string">'OnlineBackup'</span>, <span class="hljs-string">'DeviceProtection'</span>, <span class="hljs-string">'TechSupport'</span>,
                           <span class="hljs-string">'StreamingTV'</span>, <span class="hljs-string">'StreamingMovies'</span>, <span class="hljs-string">'Contract'</span>,
                           <span class="hljs-string">'PaperlessBilling'</span>, <span class="hljs-string">'PaymentMethod'</span>]
    
    <span class="hljs-comment"># Create pipeline</span>
    pipeline = create_preprocessing_pipeline(numerical_features, categorical_features)
    
    <span class="hljs-comment"># Fit and transform</span>
    X_train_transformed = pipeline.fit_transform(X_train)
    X_test_transformed = pipeline.transform(X_test)
    
    <span class="hljs-comment"># Save pipeline</span>
    joblib.dump(pipeline, <span class="hljs-string">'data/processed/preprocessing_pipeline.pkl'</span>)
    
    <span class="hljs-comment"># Save data</span>
    data = {
        <span class="hljs-string">'X_train'</span>: X_train_transformed,
        <span class="hljs-string">'X_test'</span>: X_test_transformed,
        <span class="hljs-string">'y_train'</span>: y_train,
        <span class="hljs-string">'y_test'</span>: y_test
    }
    np.save(<span class="hljs-string">'data/processed/preprocessed_data.npy'</span>, data)
</div></code></pre>
<h3 id="import-organization-pep-8">Import Organization (PEP 8)</h3>
<p><strong>PEP 8</strong> is Python's style guide. Proper import organization makes code more readable and professional.</p>
<p><strong>Bad (disorganized)</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> sys
<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path
<span class="hljs-keyword">import</span> argparse
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">import</span> os
</div></code></pre>
<p><strong>Good (PEP 8 compliant)</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Standard library imports</span>
<span class="hljs-keyword">import</span> argparse
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> sys
<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path

<span class="hljs-comment"># Third-party imports</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split

<span class="hljs-comment"># Local imports</span>
<span class="hljs-keyword">from</span> src.utils.config <span class="hljs-keyword">import</span> Config
<span class="hljs-keyword">from</span> src.utils.helpers <span class="hljs-keyword">import</span> load_data, save_model
</div></code></pre>
<p><strong>PEP 8 Import Order</strong>:</p>
<ol>
<li><strong>Standard library</strong> (built into Python)</li>
<li><strong>Third-party packages</strong> (installed via pip)</li>
<li><strong>Local application</strong> (your own modules)</li>
</ol>
<p><strong>Blank lines</strong>:</p>
<ul>
<li>One blank line between groups</li>
<li>Alphabetical order within groups (optional but recommended)</li>
</ul>
<p><strong>Why it matters</strong>:</p>
<ul>
<li>✅ Immediately see what external dependencies exist</li>
<li>✅ Easier to identify missing imports</li>
<li>✅ Professional, maintainable code</li>
<li>✅ Follows Python community standards</li>
<li>✅ Better for code reviews</li>
</ul>
<hr>
<h2 id="part-6-configuration-management-with-yaml">Part 6: Configuration Management with YAML</h2>
<h3 id="why-use-configuration-files">Why Use Configuration Files?</h3>
<p><strong>Problem</strong>: Hard-coded values scattered everywhere</p>
<pre class="hljs"><code><div><span class="hljs-comment"># train.py</span>
model = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, max_depth=<span class="hljs-number">10</span>, random_state=<span class="hljs-number">42</span>)
test_size = <span class="hljs-number">0.2</span>
batch_size = <span class="hljs-number">32</span>

<span class="hljs-comment"># preprocess.py</span>
scaler_type = <span class="hljs-string">'standard'</span>
handle_missing = <span class="hljs-string">'mean'</span>

<span class="hljs-comment"># predict.py</span>
threshold = <span class="hljs-number">0.5</span>
</div></code></pre>
<p><strong>To change anything</strong>: Edit multiple files, risk introducing bugs</p>
<p><strong>Solution</strong>: Centralized Configuration</p>
<pre class="hljs"><code><div><span class="hljs-comment"># configs/train_config.yaml</span>
<span class="hljs-attr">model:</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">random_forest</span>
  <span class="hljs-attr">params:</span>
    <span class="hljs-attr">n_estimators:</span> <span class="hljs-number">100</span>
    <span class="hljs-attr">max_depth:</span> <span class="hljs-number">10</span>
    <span class="hljs-attr">random_state:</span> <span class="hljs-number">42</span>

<span class="hljs-attr">training:</span>
  <span class="hljs-attr">test_size:</span> <span class="hljs-number">0.2</span>
  <span class="hljs-attr">batch_size:</span> <span class="hljs-number">32</span>

<span class="hljs-attr">preprocessing:</span>
  <span class="hljs-attr">scaler:</span> <span class="hljs-string">standard</span>
  <span class="hljs-attr">missing_strategy:</span> <span class="hljs-string">mean</span>

<span class="hljs-attr">prediction:</span>
  <span class="hljs-attr">threshold:</span> <span class="hljs-number">0.5</span>
</div></code></pre>
<p><strong>To change anything</strong>: Edit one YAML file, no code changes needed!</p>
<h3 id="yaml-syntax-basics">YAML Syntax Basics</h3>
<p>YAML (YAML Ain't Markup Language) is a human-readable data format.</p>
<p><strong>Key-value pairs</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-attr">name:</span> <span class="hljs-string">John</span> <span class="hljs-string">Doe</span>
<span class="hljs-attr">age:</span> <span class="hljs-number">30</span>
<span class="hljs-attr">active:</span> <span class="hljs-literal">true</span>
</div></code></pre>
<p><strong>Nested structures</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-attr">person:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">John</span> <span class="hljs-string">Doe</span>
  <span class="hljs-attr">age:</span> <span class="hljs-number">30</span>
  <span class="hljs-attr">address:</span>
    <span class="hljs-attr">street:</span> <span class="hljs-number">123</span> <span class="hljs-string">Main</span> <span class="hljs-string">St</span>
    <span class="hljs-attr">city:</span> <span class="hljs-string">New</span> <span class="hljs-string">York</span>
</div></code></pre>
<p><strong>Lists</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-attr">fruits:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">apple</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">banana</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">orange</span>

<span class="hljs-comment"># Or inline</span>
<span class="hljs-attr">colors:</span> <span class="hljs-string">[red,</span> <span class="hljs-string">green,</span> <span class="hljs-string">blue]</span>
</div></code></pre>
<p><strong>Comments</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># This is a comment</span>
<span class="hljs-attr">name:</span> <span class="hljs-string">John</span>  <span class="hljs-comment"># Inline comment</span>
</div></code></pre>
<p><strong>Data types</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-attr">string:</span> <span class="hljs-string">"Hello"</span>
<span class="hljs-attr">integer:</span> <span class="hljs-number">42</span>
<span class="hljs-attr">float:</span> <span class="hljs-number">3.14</span>
<span class="hljs-attr">boolean:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">null_value:</span> <span class="hljs-literal">null</span>
</div></code></pre>
<h3 id="example-configuration-files">Example Configuration Files</h3>
<p><strong>1. Main Configuration (<code>configs/config.yaml</code>)</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Project Configuration</span>

<span class="hljs-comment"># Project metadata</span>
<span class="hljs-attr">project:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">cmpt2500f25-project-tutorial</span>
  <span class="hljs-attr">version:</span> <span class="hljs-number">2.0</span><span class="hljs-number">.0</span>
  <span class="hljs-attr">description: Tutorial project for CMPT 2500:</span> <span class="hljs-string">Customer</span> <span class="hljs-string">churn</span> <span class="hljs-string">prediction</span> <span class="hljs-string">for</span> <span class="hljs-string">telecommunications</span>

<span class="hljs-comment"># Paths</span>
<span class="hljs-attr">paths:</span>
  <span class="hljs-attr">data:</span>
    <span class="hljs-attr">raw:</span> <span class="hljs-string">data/raw/</span>
    <span class="hljs-attr">processed:</span> <span class="hljs-string">data/processed/</span>
    <span class="hljs-attr">external:</span> <span class="hljs-string">data/external/</span>
  <span class="hljs-attr">models:</span> <span class="hljs-string">models/</span>
  <span class="hljs-attr">outputs:</span> <span class="hljs-string">outputs/</span>
  <span class="hljs-attr">logs:</span> <span class="hljs-string">logs/</span>

<span class="hljs-comment"># Random seed for reproducibility</span>
<span class="hljs-attr">random_state:</span> <span class="hljs-number">42</span>

<span class="hljs-comment"># Logging</span>
<span class="hljs-attr">logging:</span>
  <span class="hljs-attr">level:</span> <span class="hljs-string">INFO</span>  <span class="hljs-comment"># DEBUG, INFO, WARNING, ERROR, CRITICAL</span>
  <span class="hljs-attr">format:</span> <span class="hljs-string">"%(asctime)s - %(name)s - %(levelname)s - %(message)s"</span>
  <span class="hljs-attr">file:</span> <span class="hljs-string">logs/app.log</span>
</div></code></pre>
<p><strong>2. Training Configuration (<code>configs/train_config.yaml</code>)</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Training Configuration</span>

<span class="hljs-comment"># Model selection</span>
<span class="hljs-attr">model:</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">random_forest</span>  <span class="hljs-comment"># Options: logistic_regression, random_forest, decision_tree, etc.</span>
  <span class="hljs-attr">random_state:</span> <span class="hljs-number">42</span>

<span class="hljs-comment"># Model-specific parameters</span>
<span class="hljs-attr">params:</span>
  <span class="hljs-attr">logistic_regression:</span>
    <span class="hljs-attr">max_iter:</span> <span class="hljs-number">1000</span>
    <span class="hljs-attr">solver:</span> <span class="hljs-string">lbfgs</span>
    <span class="hljs-attr">C:</span> <span class="hljs-number">1.0</span>
  
  <span class="hljs-attr">random_forest:</span>
    <span class="hljs-attr">n_estimators:</span> <span class="hljs-number">100</span>
    <span class="hljs-attr">max_depth:</span> <span class="hljs-number">10</span>
    <span class="hljs-attr">min_samples_split:</span> <span class="hljs-number">2</span>
    <span class="hljs-attr">min_samples_leaf:</span> <span class="hljs-number">1</span>
  
  <span class="hljs-attr">decision_tree:</span>
    <span class="hljs-attr">max_depth:</span> <span class="hljs-number">10</span>
    <span class="hljs-attr">min_samples_split:</span> <span class="hljs-number">2</span>
  
  <span class="hljs-attr">gradient_boosting:</span>
    <span class="hljs-attr">n_estimators:</span> <span class="hljs-number">100</span>
    <span class="hljs-attr">learning_rate:</span> <span class="hljs-number">0.1</span>
    <span class="hljs-attr">max_depth:</span> <span class="hljs-number">5</span>

<span class="hljs-comment"># Training settings</span>
<span class="hljs-attr">training:</span>
  <span class="hljs-attr">test_size:</span> <span class="hljs-number">0.2</span>
  <span class="hljs-attr">stratify:</span> <span class="hljs-literal">true</span>  <span class="hljs-comment"># Stratify by target</span>
  <span class="hljs-attr">tune:</span> <span class="hljs-literal">false</span>  <span class="hljs-comment"># Enable hyperparameter tuning</span>

<span class="hljs-comment"># Hyperparameter tuning (used when tune=true)</span>
<span class="hljs-attr">tuning:</span>
  <span class="hljs-attr">cv:</span> <span class="hljs-number">5</span>  <span class="hljs-comment"># Cross-validation folds</span>
  <span class="hljs-attr">scoring:</span> <span class="hljs-string">accuracy</span>
  <span class="hljs-attr">n_jobs:</span> <span class="hljs-number">-1</span>  <span class="hljs-comment"># Use all CPU cores</span>
  <span class="hljs-attr">verbose:</span> <span class="hljs-number">1</span>
  
  <span class="hljs-comment"># Parameter grids for tuning</span>
  <span class="hljs-attr">param_grids:</span>
    <span class="hljs-attr">random_forest:</span>
      <span class="hljs-attr">n_estimators:</span> <span class="hljs-string">[50,</span> <span class="hljs-number">100</span><span class="hljs-string">,</span> <span class="hljs-number">200</span><span class="hljs-string">]</span>
      <span class="hljs-attr">max_depth:</span> <span class="hljs-string">[10,</span> <span class="hljs-number">20</span><span class="hljs-string">,</span> <span class="hljs-string">None]</span>
      <span class="hljs-attr">min_samples_split:</span> <span class="hljs-string">[2,</span> <span class="hljs-number">5</span><span class="hljs-string">,</span> <span class="hljs-number">10</span><span class="hljs-string">]</span>
      <span class="hljs-attr">min_samples_leaf:</span> <span class="hljs-string">[1,</span> <span class="hljs-number">2</span><span class="hljs-string">,</span> <span class="hljs-number">4</span><span class="hljs-string">]</span>
    
    <span class="hljs-attr">gradient_boosting:</span>
      <span class="hljs-attr">n_estimators:</span> <span class="hljs-string">[50,</span> <span class="hljs-number">100</span><span class="hljs-string">,</span> <span class="hljs-number">200</span><span class="hljs-string">]</span>
      <span class="hljs-attr">learning_rate:</span> <span class="hljs-string">[0.01,</span> <span class="hljs-number">0.1</span><span class="hljs-string">,</span> <span class="hljs-number">0.2</span><span class="hljs-string">]</span>
      <span class="hljs-attr">max_depth:</span> <span class="hljs-string">[3,</span> <span class="hljs-number">5</span><span class="hljs-string">,</span> <span class="hljs-number">7</span><span class="hljs-string">]</span>

<span class="hljs-comment"># Model evaluation</span>
<span class="hljs-attr">evaluation:</span>
  <span class="hljs-attr">metrics:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">accuracy</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">precision</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">recall</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">f1_score</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">roc_auc</span>
  <span class="hljs-attr">save_confusion_matrix:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">save_classification_report:</span> <span class="hljs-literal">true</span>
</div></code></pre>
<p><strong>3. Preprocessing Configuration (<code>configs/preprocess_config.yaml</code>)</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Preprocessing Configuration</span>

<span class="hljs-comment"># Data source</span>
<span class="hljs-attr">data:</span>
  <span class="hljs-attr">filename:</span> <span class="hljs-string">WA_Fn-UseC_-Telco-Customer-Churn.csv</span>
  <span class="hljs-attr">target_column:</span> <span class="hljs-string">Churn</span>
  <span class="hljs-attr">id_column:</span> <span class="hljs-string">customerID</span>

<span class="hljs-comment"># Missing value handling</span>
<span class="hljs-attr">missing_values:</span>
  <span class="hljs-attr">strategy:</span> <span class="hljs-string">drop</span>  <span class="hljs-comment"># Options: drop, mean, median, mode, forward_fill, backward_fill</span>
  <span class="hljs-attr">threshold:</span> <span class="hljs-number">0.5</span>  <span class="hljs-comment"># Drop columns with &gt;50% missing values</span>

<span class="hljs-comment"># Feature scaling</span>
<span class="hljs-attr">scaling:</span>
  <span class="hljs-attr">method:</span> <span class="hljs-string">standard</span>  <span class="hljs-comment"># Options: standard, minmax, robust, none</span>
  <span class="hljs-attr">with_mean:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">with_std:</span> <span class="hljs-literal">true</span>

<span class="hljs-comment"># Feature columns</span>
<span class="hljs-attr">features:</span>
  <span class="hljs-attr">categorical:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">gender</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">Partner</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">Dependents</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">PhoneService</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">MultipleLines</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">InternetService</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">OnlineSecurity</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">OnlineBackup</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">DeviceProtection</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">TechSupport</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">StreamingTV</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">StreamingMovies</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">Contract</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">PaperlessBilling</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">PaymentMethod</span>
  
  <span class="hljs-attr">numerical:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">SeniorCitizen</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">tenure</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">MonthlyCharges</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">TotalCharges</span>

<span class="hljs-comment"># Train-test split</span>
<span class="hljs-attr">split:</span>
  <span class="hljs-attr">test_size:</span> <span class="hljs-number">0.2</span>
  <span class="hljs-attr">random_state:</span> <span class="hljs-number">42</span>
  <span class="hljs-attr">stratify:</span> <span class="hljs-literal">true</span>  <span class="hljs-comment"># Stratify by target</span>

<span class="hljs-comment"># Pipeline options</span>
<span class="hljs-attr">pipeline:</span>
  <span class="hljs-attr">use_sklearn_pipeline:</span> <span class="hljs-literal">true</span>  <span class="hljs-comment"># Recommended: true</span>
  <span class="hljs-attr">save_artifacts:</span> <span class="hljs-literal">true</span>  <span class="hljs-comment"># Save pipeline and encoders</span>
</div></code></pre>
<p><strong>4. Prediction Configuration (<code>configs/predict_config.yaml</code>)</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Prediction Configuration</span>

<span class="hljs-comment"># Model</span>
<span class="hljs-attr">model:</span>
  <span class="hljs-attr">path:</span> <span class="hljs-string">models/random_forest_20241027_143022.pkl</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">random_forest</span>

<span class="hljs-comment"># Preprocessing</span>
<span class="hljs-attr">preprocessing:</span>
  <span class="hljs-attr">pipeline_path:</span> <span class="hljs-string">data/processed/preprocessing_pipeline.pkl</span>
  <span class="hljs-attr">label_encoder_path:</span> <span class="hljs-string">data/processed/label_encoder.pkl</span>

<span class="hljs-comment"># Input data</span>
<span class="hljs-attr">input:</span>
  <span class="hljs-attr">format:</span> <span class="hljs-string">csv</span>  <span class="hljs-comment"># Options: csv, json, numpy</span>
  <span class="hljs-attr">path:</span> <span class="hljs-string">data/new_data.csv</span>

<span class="hljs-comment"># Output</span>
<span class="hljs-attr">output:</span>
  <span class="hljs-attr">format:</span> <span class="hljs-string">csv</span>  <span class="hljs-comment"># Options: csv, json, numpy</span>
  <span class="hljs-attr">path:</span> <span class="hljs-string">predictions/predictions.csv</span>
  <span class="hljs-attr">include_probabilities:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">include_confidence:</span> <span class="hljs-literal">true</span>

<span class="hljs-comment"># Batch processing</span>
<span class="hljs-attr">batch:</span>
  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">batch_size:</span> <span class="hljs-number">1000</span>  <span class="hljs-comment"># Process 1000 samples at a time</span>
</div></code></pre>
<h3 id="loading-yaml-in-python">Loading YAML in Python</h3>
<p><strong>Install PyYAML</strong>:</p>
<pre class="hljs"><code><div>pip install PyYAML
</div></code></pre>
<p><strong>Load configuration</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> yaml

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_config</span><span class="hljs-params">(config_path)</span>:</span>
    <span class="hljs-string">"""
    Load YAML configuration file.
    
    Args:
        config_path: Path to YAML file
        
    Returns:
        Dictionary with configuration
    """</span>
    <span class="hljs-keyword">with</span> open(config_path, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:
        config = yaml.safe_load(f)
    <span class="hljs-keyword">return</span> config

<span class="hljs-comment"># Usage</span>
config = load_config(<span class="hljs-string">'configs/train_config.yaml'</span>)

<span class="hljs-comment"># Access nested values</span>
random_state = config[<span class="hljs-string">'model'</span>][<span class="hljs-string">'random_state'</span>]
n_estimators = config[<span class="hljs-string">'params'</span>][<span class="hljs-string">'random_forest'</span>][<span class="hljs-string">'n_estimators'</span>]
test_size = config[<span class="hljs-string">'training'</span>][<span class="hljs-string">'test_size'</span>]

print(<span class="hljs-string">f"Random state: <span class="hljs-subst">{random_state}</span>"</span>)
print(<span class="hljs-string">f"N estimators: <span class="hljs-subst">{n_estimators}</span>"</span>)
print(<span class="hljs-string">f"Test size: <span class="hljs-subst">{test_size}</span>"</span>)
</div></code></pre>
<h3 id="using-configuration-in-code">Using Configuration in Code</h3>
<p><strong>Example</strong>: Training with YAML config</p>
<pre class="hljs"><code><div><span class="hljs-comment"># train.py</span>
<span class="hljs-keyword">import</span> yaml
<span class="hljs-keyword">from</span> src.train <span class="hljs-keyword">import</span> train_random_forest, save_model

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-comment"># Load configuration</span>
    <span class="hljs-keyword">with</span> open(<span class="hljs-string">'configs/train_config.yaml'</span>, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:
        config = yaml.safe_load(f)
    
    <span class="hljs-comment"># Extract settings</span>
    model_type = config[<span class="hljs-string">'model'</span>][<span class="hljs-string">'type'</span>]
    model_params = config[<span class="hljs-string">'params'</span>][model_type]
    random_state = config[<span class="hljs-string">'model'</span>][<span class="hljs-string">'random_state'</span>]
    tune = config[<span class="hljs-string">'training'</span>][<span class="hljs-string">'tune'</span>]
    
    <span class="hljs-comment"># Load data</span>
    data_path = config[<span class="hljs-string">'paths'</span>][<span class="hljs-string">'data'</span>][<span class="hljs-string">'processed'</span>] + <span class="hljs-string">'train_data.npy'</span>
    data = np.load(data_path, allow_pickle=<span class="hljs-literal">True</span>).item()
    X_train = data[<span class="hljs-string">'X_train'</span>]
    y_train = data[<span class="hljs-string">'y_train'</span>]
    
    <span class="hljs-comment"># Train model with config parameters</span>
    model = train_random_forest(
        X_train,
        y_train,
        tune_hyperparameters=tune,
        **model_params
    )
    
    <span class="hljs-comment"># Save model</span>
    output_dir = config[<span class="hljs-string">'paths'</span>][<span class="hljs-string">'models'</span>]
    save_model(model, model_type, output_dir)
    
    print(<span class="hljs-string">f"Model trained and saved using config: <span class="hljs-subst">{config_path}</span>"</span>)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    main()
</div></code></pre>
<h3 id="combining-cli-and-yaml">Combining CLI and YAML</h3>
<p><strong>Best practice</strong>: Use YAML for defaults, CLI for overrides</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>
    parser = argparse.ArgumentParser()
    
    parser.add_argument(<span class="hljs-string">'--config'</span>, type=str,
                       default=<span class="hljs-string">'configs/train_config.yaml'</span>,
                       help=<span class="hljs-string">'Path to config file'</span>)
    parser.add_argument(<span class="hljs-string">'--tune'</span>, action=<span class="hljs-string">'store_true'</span>,
                       help=<span class="hljs-string">'Override config: enable tuning'</span>)
    parser.add_argument(<span class="hljs-string">'--model'</span>, type=str,
                       help=<span class="hljs-string">'Override config: model type'</span>)
    
    args = parser.parse_args()
    
    <span class="hljs-comment"># Load YAML config</span>
    <span class="hljs-keyword">with</span> open(args.config, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:
        config = yaml.safe_load(f)
    
    <span class="hljs-comment"># Override with CLI arguments if provided</span>
    <span class="hljs-keyword">if</span> args.tune:
        config[<span class="hljs-string">'training'</span>][<span class="hljs-string">'tune'</span>] = <span class="hljs-literal">True</span>
    <span class="hljs-keyword">if</span> args.model:
        config[<span class="hljs-string">'model'</span>][<span class="hljs-string">'type'</span>] = args.model
    
    <span class="hljs-comment"># Use merged configuration</span>
    <span class="hljs-comment"># ...</span>
</div></code></pre>
<p><strong>Usage</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Use defaults from YAML</span>
python train.py

<span class="hljs-comment"># Override specific settings</span>
python train.py --tune
python train.py --model gradient_boosting --tune
python train.py --config configs/experimental_config.yaml
</div></code></pre>
<h3 id="multiple-configurations-for-different-scenarios">Multiple Configurations for Different Scenarios</h3>
<p>Create different configs for different use cases:</p>
<pre class="hljs"><code><div>configs/
├── train_config.yaml          <span class="hljs-comment"># Default training</span>
├── train_config_fast.yaml     <span class="hljs-comment"># Quick experiments (no tuning)</span>
├── train_config_production.yaml  <span class="hljs-comment"># Production (with tuning)</span>
├── preprocess_config.yaml     <span class="hljs-comment"># Default preprocessing</span>
└── predict_config.yaml        <span class="hljs-comment"># Prediction settings</span>
</div></code></pre>
<p><strong>Example usage</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Fast experimentation</span>
python train.py --config configs/train_config_fast.yaml

<span class="hljs-comment"># Production training</span>
python train.py --config configs/train_config_production.yaml
</div></code></pre>
<h3 id="benefits-of-yaml-configuration">Benefits of YAML Configuration</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Hard-coded / config.py</th>
<th>YAML Configuration</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Readability</strong></td>
<td>❌ Python syntax</td>
<td>✅ Plain text</td>
</tr>
<tr>
<td><strong>Comments</strong></td>
<td>✅ Yes</td>
<td>✅ Yes, inline</td>
</tr>
<tr>
<td><strong>Nested Data</strong></td>
<td>⚠️ Verbose</td>
<td>✅ Clean</td>
</tr>
<tr>
<td><strong>Hot Reload</strong></td>
<td>❌ Restart needed</td>
<td>✅ Just reload file</td>
</tr>
<tr>
<td><strong>Non-programmers</strong></td>
<td>❌ Hard to edit</td>
<td>✅ Easy to edit</td>
</tr>
<tr>
<td><strong>Version Control</strong></td>
<td>✅ Yes</td>
<td>✅ Yes</td>
</tr>
<tr>
<td><strong>Multiple Configs</strong></td>
<td>❌ Complex</td>
<td>✅ Easy</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="summary-of-parts-1-6">Summary of Parts 1-6</h2>
<p>Congratulations! You've now:</p>
<p>✅ <strong>Understood computational environments</strong> and why they matter
✅ <strong>Created virtual environments</strong> for isolated Python installations
✅ <strong>Managed dependencies</strong> with <code>requirements.txt</code>
✅ <strong>Built CLI interfaces</strong> using argparse
✅ <strong>Added hyperparameter tuning</strong> to improve models
✅ <strong>Implemented scikit-learn pipelines</strong> for better preprocessing
✅ <strong>Organized imports</strong> following PEP 8 style
✅ <strong>Created YAML configurations</strong> for flexible settings</p>
<h3 id="your-updated-project-structure">Your Updated Project Structure</h3>
<pre class="hljs"><code><div>your-project/
├── .venv/                      # Virtual environment (not in Git)
├── configs/                    # YAML configuration files
│   ├── train_config.yaml
│   ├── preprocess_config.yaml
│   └── predict_config.yaml
├── data/
│   ├── raw/
│   └── processed/
│       ├── preprocessed_data.npy
│       ├── preprocessing_pipeline.pkl  # NEW: Saved pipeline
│       └── label_encoder.pkl           # NEW: Saved encoder
├── src/
│   ├── preprocess.py           # UPDATED: With sklearn pipelines &amp; CLI
│   ├── train.py                # UPDATED: With tuning &amp; CLI
│   ├── predict.py              # UPDATED: With CLI
│   ├── evaluate.py             # UPDATED: With CLI
│   └── utils/
│       └── config.py
├── models/                     # Saved models
├── requirements.txt            # NEW: Dependencies
└── README.md
</div></code></pre>
<h3 id="quick-command-reference">Quick Command Reference</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Virtual Environment</span>
python -m venv .venv
<span class="hljs-built_in">source</span> .venv/bin/activate  <span class="hljs-comment"># Mac/Linux</span>
.venv\Scripts\activate     <span class="hljs-comment"># Windows</span>

<span class="hljs-comment"># Install Dependencies</span>
pip install -r requirements.txt

<span class="hljs-comment"># Preprocessing (with pipeline)</span>
python -m src.preprocess --input data/raw/data.csv

<span class="hljs-comment"># Training (with tuning)</span>
python -m src.train --data data/processed/preprocessed_data.npy --model random_forest --tune

<span class="hljs-comment"># Prediction</span>
python -m src.predict --model models/model.pkl --data data/processed/preprocessed_data.npy

<span class="hljs-comment"># Evaluation</span>
python -m src.evaluate --model models/model.pkl --data data/processed/preprocessed_data.npy
</div></code></pre>
<hr>
<h2 id="part-7-data-version-control-with-dvc">Part 7: Data Version Control with DVC</h2>
<h3 id="why-version-control-data">Why Version Control Data?</h3>
<p><strong>The Problem</strong>:</p>
<pre class="hljs"><code><div>project/
├── data/
│   ├── train_data_v1.csv
│   ├── train_data_v2.csv
│   ├── train_data_v2_final.csv
│   ├── train_data_v2_final_ACTUALLY_FINAL.csv
│   └── train_data_v2_final_use_this_one.csv  # 😱
</div></code></pre>
<p>Sound familiar? Data versioning is hard:</p>
<ul>
<li>❌ Large files don't belong in Git</li>
<li>❌ Manual versioning is error-prone</li>
<li>❌ Hard to track which data produced which model</li>
<li>❌ Collaboration becomes messy</li>
<li>❌ Can't easily rollback to previous versions</li>
</ul>
<p><strong>The Solution</strong>: DVC (Data Version Control)</p>
<p>DVC is like Git, but for data:</p>
<ul>
<li>✅ Version control for large files</li>
<li>✅ Lightweight metadata in Git</li>
<li>✅ Data stored in cloud (S3, Google Drive, DagsHub)</li>
<li>✅ Track data-model relationships</li>
<li>✅ Easy collaboration</li>
<li>✅ Reproducible pipelines</li>
</ul>
<p><strong>How DVC Works</strong>:</p>
<pre class="hljs"><code><div>Git Repository (lightweight):
├── data/
│   └── raw.dvc             # Metadata file (small, in Git)
├── models/
│   └── model.pkl.dvc       # Metadata file (small, in Git)
└── .dvc/
    └── config              # DVC configuration (in Git)

DVC Remote Storage (cloud):
└── Large actual files:
    ├── data/raw/train.csv           # Actual data (not in Git)
    └── models/model.pkl             # Actual model (not in Git)
</div></code></pre>
<p><strong>Git tracks</strong>: Code + DVC metadata files (<code>.dvc</code> files)<br>
<strong>DVC tracks</strong>: Actual data + models (stored in cloud)</p>
<h3 id="part-71-installing-and-initializing-dvc">Part 7.1: Installing and Initializing DVC</h3>
<h4 id="install-dvc">Install DVC</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Activate your virtual environment</span>
<span class="hljs-built_in">source</span> .venv/bin/activate

<span class="hljs-comment"># Install DVC</span>
pip install dvc

<span class="hljs-comment"># Verify installation</span>
dvc version
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>DVC version: 3.63.0 (pip)
</div></code></pre>
<h4 id="initialize-dvc-in-your-project">Initialize DVC in Your Project</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Navigate to project root</span>
<span class="hljs-built_in">cd</span> /path/to/your/project

<span class="hljs-comment"># Initialize DVC</span>
dvc init
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>Initialized DVC repository.

You can now commit the changes to git.

+---------------------------------------------------------------------+
|                                                                     |
|        DVC has enabled anonymous aggregate usage analytics.        |
|     Read the analytics documentation (and how to opt-out) here:    |
|             &lt;https://dvc.org/doc/user-guide/analytics&gt;             |
|                                                                     |
+---------------------------------------------------------------------+

What's next?
------------
- Check out the documentation: &lt;https://dvc.org/doc&gt;
- Get help and share ideas: &lt;https://dvc.org/chat&gt;
- Star us on GitHub: &lt;https://github.com/iterative/dvc&gt;
</div></code></pre>
<h4 id="what-dvc-created">What DVC Created</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Check what was created</span>
ls -la .dvc/
</div></code></pre>
<p><strong>You'll see</strong>:</p>
<pre class="hljs"><code><div>.dvc/
├── .gitignore          # Ignores DVC cache
├── config              # DVC configuration
└── tmp/                # Temporary files
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-comment"># Check Git status</span>
git status
</div></code></pre>
<p><strong>Expected</strong>:</p>
<pre class="hljs"><code><div>Changes to be committed:
  new file:   .dvc/.gitignore
  new file:   .dvc/config
  new file:   .dvcignore
  modified:   .gitignore
</div></code></pre>
<p>DVC automatically:</p>
<ul>
<li>Created <code>.dvc/</code> directory for metadata</li>
<li>Created <code>.dvc/config</code> for settings</li>
<li>Modified <code>.gitignore</code> to ignore DVC cache</li>
<li>Staged files for Git commit</li>
</ul>
<h4 id="commit-dvc-initialization">Commit DVC Initialization</h4>
<pre class="hljs"><code><div>git add .dvc .dvcignore .gitignore
git commit -m <span class="hljs-string">"chore: Initialize DVC for data version control"</span>
git push
</div></code></pre>
<h3 id="part-72-tracking-data-with-dvc">Part 7.2: Tracking Data with DVC</h3>
<h4 id="understanding-the-transition">Understanding the Transition</h4>
<p>Your data is currently tracked by Git:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># See what's in Git now</span>
git ls-files data/
</div></code></pre>
<p><strong>Output</strong>:</p>
<pre class="hljs"><code><div>data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv
data/processed/preprocessed_data.npy
data/processed/preprocessing_pipeline.pkl
data/processed/label_encoder.pkl
</div></code></pre>
<p><strong>Goal</strong>: Move these large files to DVC tracking, keep only metadata in Git.</p>
<h4 id="remove-data-from-git-tracking">Remove Data from Git Tracking</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Remove raw data from Git (but keep files on disk)</span>
git rm -r --cached data/raw

<span class="hljs-comment"># Remove processed data from Git (but keep files on disk)</span>
git rm -r --cached data/processed
</div></code></pre>
<p><strong>Note</strong>: <code>--cached</code> flag means &quot;remove from Git tracking but keep the actual files on your disk.&quot;</p>
<p><strong>Verify files still exist</strong>:</p>
<pre class="hljs"><code><div>ls -lh data/raw/
ls -lh data/processed/
<span class="hljs-comment"># Files should still be there!</span>
</div></code></pre>
<h4 id="add-data-to-dvc-tracking">Add Data to DVC Tracking</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Track raw data with DVC</span>
dvc add data/raw
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>100% Adding...|████████████████████████████████|1/1 [00:XX, XX file/s]

To track the changes with git, run:

        git add data/raw.dvc data/.gitignore
</div></code></pre>
<p><strong>What happened?</strong></p>
<ul>
<li>DVC created <code>data/raw.dvc</code> (metadata file)</li>
<li>DVC created/updated <code>data/.gitignore</code> (to ignore actual data)</li>
<li>DVC moved actual data to <code>.dvc/cache/</code> (local cache)</li>
<li>Actual files still accessible at original location (DVC creates links)</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-comment"># Track processed data with DVC</span>
dvc add data/processed
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>100% Adding...|████████████████████████████████|1/1 [00:XX, XX file/s]

To track the changes with git, run:

        git add data/processed.dvc data/.gitignore
</div></code></pre>
<h4 id="examine-dvc-metadata-files">Examine DVC Metadata Files</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># View the metadata file</span>
cat data/raw.dvc
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-attr">outs:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">md5:</span> <span class="hljs-string">063d451250fb0faa73bc60935e759442.dir</span>
  <span class="hljs-attr">size:</span> <span class="hljs-number">977501</span>
  <span class="hljs-attr">nfiles:</span> <span class="hljs-number">1</span>
  <span class="hljs-attr">hash:</span> <span class="hljs-string">md5</span>
  <span class="hljs-attr">path:</span> <span class="hljs-string">raw</span>
</div></code></pre>
<p>This file contains:</p>
<ul>
<li><strong>md5</strong>: Hash of directory contents (for tracking changes)</li>
<li><strong>size</strong>: Total size in bytes</li>
<li><strong>nfiles</strong>: Number of files</li>
<li><strong>path</strong>: Path to data directory</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-comment"># Check the .gitignore DVC created</span>
cat data/.gitignore
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>/raw
/processed
</div></code></pre>
<p>This tells Git to ignore the actual data directories (since DVC is now managing them).</p>
<h4 id="add-dvc-files-to-git">Add DVC Files to Git</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Add DVC metadata files to Git</span>
git add data/raw.dvc data/processed.dvc data/.gitignore

<span class="hljs-comment"># Check status</span>
git status
</div></code></pre>
<p><strong>Expected</strong>:</p>
<pre class="hljs"><code><div>Changes to be committed:
  new file:   data/.gitignore
  new file:   data/processed.dvc
  new file:   data/raw.dvc
  deleted:    data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv
  deleted:    data/processed/...
</div></code></pre>
<h4 id="commit-changes">Commit Changes</h4>
<pre class="hljs"><code><div>git commit -m <span class="hljs-string">"feat: Track data with DVC instead of Git

- Remove large data files from Git tracking
- Add data/raw/ to DVC (977KB)
- Add data/processed/ to DVC (1.3MB)
- DVC metadata files tracked in Git

Data now version controlled with DVC, not Git."</span>

git push
</div></code></pre>
<h3 id="part-73-setting-up-remote-storage">Part 7.3: Setting Up Remote Storage</h3>
<p>Your data is now tracked by DVC locally, but to collaborate or backup, you need <strong>remote storage</strong> (like GitHub for code, but for data).</p>
<p><strong>Remote Storage Options</strong>:</p>
<ol>
<li>
<p><strong>DagsHub</strong> (Recommended) ⭐</p>
<ul>
<li>Purpose-built for ML projects</li>
<li>Free tier includes DVC + MLflow hosting</li>
<li>S3-compatible (industry standard)</li>
<li>Easy setup</li>
<li>Web UI to browse data</li>
</ul>
</li>
<li>
<p><strong>Google Drive</strong> (Alternative)</p>
<ul>
<li>Free 15GB storage</li>
<li>Familiar interface</li>
<li>Good for small projects</li>
<li><strong>Limitation</strong>: OAuth issues in cloud environments (CodeSpaces)</li>
</ul>
</li>
<li>
<p><strong>Amazon S3</strong> (Production)</p>
<ul>
<li>Industry standard</li>
<li>Highly scalable</li>
<li>Pay-as-you-go</li>
<li>Best for production</li>
</ul>
</li>
<li>
<p><strong>Others</strong>:</p>
<ul>
<li>Google Cloud Storage</li>
<li>Azure Blob Storage</li>
<li>SSH/SFTP servers</li>
</ul>
</li>
</ol>
<p><strong>For this lab, we'll use DagsHub (recommended).</strong></p>
<hr>
<h3 id="part-74-dagshub-setup-recommended">Part 7.4: DagsHub Setup (Recommended)</h3>
<h4 id="why-dagshub">Why DagsHub?</h4>
<ul>
<li>✅ <strong>Free for students/educators</strong></li>
<li>✅ <strong>DVC + MLflow in one place</strong> (we'll use MLflow next!)</li>
<li>✅ <strong>S3-compatible</strong> (same as Amazon S3 - the most common storage in industry)</li>
<li>✅ <strong>Works in all environments</strong> (local, cloud, CodeSpaces)</li>
<li>✅ <strong>Web UI</strong> to browse data versions</li>
<li>✅ <strong>Git integration</strong> (syncs with GitHub)</li>
</ul>
<h4 id="step-1-create-dagshub-account">Step 1: Create DagsHub Account</h4>
<ol>
<li>Go to <a href="https://dagshub.com/">https://dagshub.com/</a></li>
<li>Click <strong>&quot;Sign Up&quot;</strong> or <strong>&quot;Sign in with GitHub&quot;</strong> (recommended - uses your GitHub account)</li>
<li>Verify your email if prompted</li>
</ol>
<h4 id="step-2-install-dagshub-and-dvc-s3-package">Step 2: Install DagsHub and DVC-S3 Package</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Install DagsHub CLI and authentication tools</span>
pip install dagshub --upgrade
pip install dvc-s3
</div></code></pre>
<h4 id="step-3-authenticate-with-dagshub">Step 3: Authenticate with DagsHub</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Login to DagsHub (creates authentication token)</span>
dagshub login
</div></code></pre>
<p><strong>This will</strong>:</p>
<ol>
<li>Open a browser window (or provide a URL to open)</li>
<li>Ask you to authorize DagsHub CLI</li>
<li>Prompt you to select token expiration time</li>
</ol>
<p><strong>Important Notes</strong>:</p>
<ul>
<li>⚠️ <strong>Token Expiration</strong>: Choose a timeframe that covers your course duration (e.g., 3 months for a semester course). The token will expire after this period.</li>
<li>⚠️ <strong>New CodeSpaces Instances</strong>: If you create a new CodeSpaces environment, you'll need to run <code>dagshub login</code> again to re-authenticate.</li>
<li>✅ <strong>Token Storage</strong>: The authentication token is stored locally in <code>~/.dagshub/config</code> (not in your project, not in Git).</li>
</ul>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>DagsHub login successful!
Authentication token saved to ~/.dagshub/config
</div></code></pre>
<h4 id="step-4-create-dagshub-repository">Step 4: Create DagsHub Repository</h4>
<ol>
<li>Click <strong>&quot;+ New Repository&quot;</strong> (top right)</li>
<li>Fill in details:
<ul>
<li><strong>Repository name</strong>: Match your GitHub repo name</li>
<li><strong>Description</strong>: &quot;Describe your problem&quot;</li>
<li><strong>Visibility</strong>: Public or Private (your choice; private if you are asked by data provider not to share)</li>
<li><strong>Initialize with</strong>: Leave all unchecked (we already have a repo)</li>
</ul>
</li>
<li>Click <strong>&quot;Create Repository&quot;</strong></li>
</ol>
<p>You'll see an empty repository page with setup instructions.</p>
<p><strong>Example (instructor's repo)</strong>:</p>
<pre class="hljs"><code><div>https://dagshub.com/ajallooe/cmpt2500f25-project-tutorial
</div></code></pre>
<h4 id="step-5-get-dagshub-credentials">Step 5: Get DagsHub Credentials</h4>
<p>On your DagsHub repository page:</p>
<ol>
<li>Look for <strong>&quot;Connection credentials&quot;</strong> box (usually bottom right)</li>
<li>Click <strong>&quot;Simple Data Upload&quot;</strong> tab</li>
<li>You'll see:
<ul>
<li><strong>Bucket name</strong>: Your repository name</li>
<li><strong>Endpoint URL</strong>: <code>https://dagshub.com/api/v1/repo-buckets/s3/your-username</code></li>
<li><strong>Access Key ID</strong>: (a token)</li>
<li><strong>Secret Access Key</strong>: (same token - DagsHub uses the same value for both)</li>
<li><strong>Region</strong>: <code>us-east-1</code></li>
</ul>
</li>
</ol>
<p><strong>Keep this page open - you'll need these credentials!</strong></p>
<h4 id="step-6-configure-dvc-remote">Step 6: Configure DVC Remote</h4>
<p>Back in your terminal:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Add DagsHub as DVC remote</span>
<span class="hljs-comment"># Replace with YOUR values from DagsHub</span>
dvc remote add origin s3://dvc
dvc remote modify origin endpointurl https://dagshub.com/your-username/your-project-name.s3

<span class="hljs-comment"># Set as default remote</span>
dvc remote default origin
</div></code></pre>
<p><strong>Example (with actual values)</strong>:</p>
<pre class="hljs"><code><div>dvc remote add origin s3://dvc
dvc remote modify origin endpointurl https://dagshub.com/ajallooe/cmpt2500f25-project-tutorial.s3
dvc remote default origin
</div></code></pre>
<h4 id="step-7-add-credentials-stored-locally-only">Step 7: Add Credentials (Stored Locally Only)</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Add credentials (replace YOUR_TOKEN with actual token from DagsHub)</span>
dvc remote modify origin --<span class="hljs-built_in">local</span> access_key_id YOUR_TOKEN
dvc remote modify origin --<span class="hljs-built_in">local</span> secret_access_key YOUR_TOKEN
</div></code></pre>
<p><strong>Important</strong>: The <code>--local</code> flag stores credentials in <code>.dvc/config.local</code>, which is automatically ignored by Git. Your credentials stay secure on your machine only!</p>
<h4 id="step-8-verify-configuration">Step 8: Verify Configuration</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Check main config (will be committed to Git)</span>
cat .dvc/config
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>[core]
    remote = origin
['remote &quot;origin&quot;']
    url = s3://dvc
    endpointurl = https://dagshub.com/your-username/your-repo.s3
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-comment"># Check local config (NOT committed - has credentials)</span>
cat .dvc/config.local
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>['remote &quot;origin&quot;']
    access_key_id = YOUR_TOKEN
    secret_access_key = YOUR_TOKEN
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-comment"># Verify remote is set</span>
dvc remote list
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>origin  s3://dvc    (default)
</div></code></pre>
<h4 id="step-9-commit-remote-configuration">Step 9: Commit Remote Configuration</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Add config to Git (NOT config.local - that's gitignored)</span>
git add .dvc/config
git commit -m <span class="hljs-string">"chore: Configure DagsHub as DVC remote storage

- Add DagsHub S3-compatible remote
- Set as default remote
- Credentials stored locally (not committed)"</span>

git push
</div></code></pre>
<h4 id="step-10-push-data-to-dagshub">Step 10: Push Data to DagsHub</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Push data to remote storage</span>
dvc push
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>Collecting
Pushing
2 files pushed
</div></code></pre>
<p>This uploads:</p>
<ul>
<li><code>data/raw/</code> (955KB)</li>
<li><code>data/processed/</code> (1.3MB)</li>
</ul>
<p><strong>First push might take a minute depending on your internet speed.</strong></p>
<h4 id="step-11-verify-upload">Step 11: Verify Upload</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Check DVC status</span>
dvc status -c
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>Cache and remote 'origin' are in sync.
</div></code></pre>
<p>This confirms your local data and remote data match!</p>
<p><strong>Optional</strong>: Check DagsHub Web UI</p>
<ol>
<li>Go to your DagsHub repository</li>
<li>Click <strong>&quot;Files&quot;</strong> tab</li>
<li>Navigate to <code>data/</code> directory</li>
<li>You should see <code>.dvc</code> metadata files (Git tracks these)</li>
<li>The actual data is in DagsHub's storage (not visible in files, but in Storage tab)</li>
</ol>
<p><strong>Note</strong>: DagsHub UI can take a few minutes to sync. The important thing is that <code>dvc status -c</code> shows everything is in sync.</p>
<h3 id="part-75-testing-dvc-workflow">Part 7.5: Testing DVC Workflow</h3>
<p>Let's test that DVC actually works by simulating a fresh clone:</p>
<h4 id="simulate-fresh-clone">Simulate Fresh Clone</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Remove local cache</span>
rm -rf .dvc/cache

<span class="hljs-comment"># Verify cache is gone</span>
ls -la .dvc/
<span class="hljs-comment"># Should NOT see cache/ directory</span>

<span class="hljs-comment"># Remove actual data</span>
rm -rf data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv
rm -rf data/processed/*.pkl data/processed/*.npy

<span class="hljs-comment"># Verify data is gone</span>
ls -la data/raw/
ls -la data/processed/
<span class="hljs-comment"># Should be empty</span>
</div></code></pre>
<h4 id="pull-data-from-dagshub">Pull Data from DagsHub</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Pull data from remote</span>
dvc pull
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>Collecting
Fetching
2 files fetched
</div></code></pre>
<h4 id="verify-data-restored">Verify Data Restored</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Check if data is back</span>
ls -lh data/raw/
ls -lh data/processed/
</div></code></pre>
<p><strong>Expected</strong>: All your files should be back with correct sizes!</p>
<pre class="hljs"><code><div>data/raw/:
  WA_Fn-UseC_-Telco-Customer-Churn.csv (955KB)

data/processed/:
  preprocessed_data.npy (1.2MB)
  preprocessing_pipeline.pkl (48KB)
  label_encoder.pkl (484B)
</div></code></pre>
<p><strong>Success!</strong> ✅ DVC is working correctly. You can now:</p>
<ul>
<li>Version control your data</li>
<li>Collaborate with teammates (they just <code>dvc pull</code>)</li>
<li>Rollback to previous data versions</li>
<li>Track which data produced which model</li>
</ul>
<hr>
<h3 id="part-76-google-drive-setup-alternative">Part 7.6: Google Drive Setup (Alternative)</h3>
<p><strong>⚠️ Important Note</strong>: Google Drive remote has limitations in cloud environments (GitHub CodeSpaces, AWS Cloud9, etc.) due to OAuth authentication restrictions. <strong>DagsHub is strongly recommended for cloud development.</strong></p>
<p>Google Drive works well for:</p>
<ul>
<li>✅ Local development (your own computer)</li>
<li>✅ Small projects</li>
<li>✅ Personal learning</li>
</ul>
<p>Use Google Drive only if:</p>
<ul>
<li>You're working on your local machine (not cloud)</li>
<li>Your project is small (&lt;15GB)</li>
<li>You want to learn alternative remotes</li>
</ul>
<h4 id="prerequisites-for-google-drive">Prerequisites for Google Drive</h4>
<p><strong>If you want to use Google Drive, you'll need to</strong>:</p>
<ol>
<li>
<p><strong>Install Google Drive support</strong>:</p>
<pre class="hljs"><code><div>pip install dvc-gdrive
</div></code></pre>
</li>
<li>
<p><strong>Create a Google Drive folder</strong>:</p>
<ul>
<li>Go to <a href="https://drive.google.com">https://drive.google.com</a></li>
<li>Create folder: <code>dvc-your-project-name</code></li>
<li>Share with team members and instructor</li>
<li>Get the folder ID from URL (after <code>/folders/</code>)</li>
</ul>
</li>
<li>
<p><strong>Configure DVC remote</strong>:</p>
<pre class="hljs"><code><div>dvc remote add -d gdrive gdrive://YOUR_FOLDER_ID
</div></code></pre>
</li>
<li>
<p><strong>Authenticate (local machine only)</strong>:</p>
<pre class="hljs"><code><div>dvc push
<span class="hljs-comment"># This will open browser for Google OAuth</span>
<span class="hljs-comment"># Follow prompts to authenticate</span>
</div></code></pre>
</li>
</ol>
<h4 id="oauth-limitation-in-cloud-environments">OAuth Limitation in Cloud Environments</h4>
<p><strong>Why it doesn't work in CodeSpaces/cloud</strong>:</p>
<p>When you run <code>dvc push</code> with Google Drive, it tries to:</p>
<ol>
<li>Open a browser window for Google authentication</li>
<li>Ask you to grant permissions to PyDrive2 (DVC's Google Drive library)</li>
<li>Get an authorization code back</li>
</ol>
<p><strong>The problem</strong>:</p>
<ul>
<li>Cloud environments (CodeSpaces, Cloud9) can't open browsers</li>
<li>Google blocks &quot;unverified apps&quot; for security</li>
<li>PyDrive2 is considered an &quot;unverified app&quot;</li>
</ul>
<p><strong>Error you'll see</strong>:</p>
<blockquote>
<p>This app is blocked
This app tried to access sensitive info in your Google Account.
To keep your account safe, Google blocked this access.</p>
</blockquote>
<h4 id="workaround-service-account-advanced">Workaround: Service Account (Advanced)</h4>
<p>For Google Drive to work in cloud environments, you need to:</p>
<ol>
<li>Create a Google Cloud Project</li>
<li>Enable Google Drive API</li>
<li>Create a service account</li>
<li>Generate credentials JSON file</li>
<li>Share your Drive folder with service account email</li>
<li>Configure DVC with service account JSON</li>
</ol>
<p><strong>This process</strong>:</p>
<ul>
<li>Takes 15-20 minutes to set up</li>
<li>Requires Google Cloud Console access</li>
<li>Is more complex than DagsHub</li>
<li><strong>Not tested in CodeSpaces for this lab</strong></li>
</ul>
<p><strong>We provide this information for reference, but recommend DagsHub for ease of use.</strong></p>
<hr>
<h3 id="part-77-dvc-branch-management">Part 7.7: DVC Branch Management</h3>
<p>Remember we created a <code>dvc-google-drive</code> branch? Let's understand the branching strategy:</p>
<h4 id="main-branch-dagshub">Main Branch (DagsHub)</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Ensure you're on main</span>
git checkout main

<span class="hljs-comment"># Verify DagsHub remote</span>
dvc remote list
</div></code></pre>
<p><strong>Output</strong>:</p>
<pre class="hljs"><code><div>origin  s3://dvc    (default)
</div></code></pre>
<p><strong>This branch is</strong>:</p>
<ul>
<li>✅ Fully working and tested</li>
<li>✅ Recommended for all students</li>
<li>✅ Works in cloud environments</li>
<li>✅ Production-ready</li>
</ul>
<h4 id="google-drive-branch-alternative">Google Drive Branch (Alternative)</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Switch to Google Drive branch (for reference)</span>
git checkout dvc-google-drive

<span class="hljs-comment"># Verify Google Drive remote</span>
dvc remote list
</div></code></pre>
<p><strong>Output</strong>:</p>
<pre class="hljs"><code><div>gdrive  gdrive://FOLDER_ID    (default)
</div></code></pre>
<p><strong>This branch is</strong>:</p>
<ul>
<li>⚠️ For local development only</li>
<li>⚠️ OAuth issues in cloud environments</li>
<li>⚠️ Documented but not fully tested</li>
<li>ℹ️ Learning resource for alternative remotes</li>
</ul>
<p><strong>Don't worry about this branch for now. Stick with main (DagsHub)!</strong></p>
<hr>
<h3 id="part-78-dvc-workflow-summary">Part 7.8: DVC Workflow Summary</h3>
<h4 id="daily-workflow">Daily Workflow</h4>
<p><strong>1. Make changes to data</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Preprocess data (creates/updates files)</span>
python -m src.preprocess --input data/raw/new_data.csv

<span class="hljs-comment"># DVC notices files changed</span>
dvc status
</div></code></pre>
<p><strong>2. Track changes with DVC</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Add changed files to DVC</span>
dvc add data/processed

<span class="hljs-comment"># Commit DVC metadata to Git</span>
git add data/processed.dvc
git commit -m <span class="hljs-string">"feat: Update processed data with new preprocessing"</span>
git push
</div></code></pre>
<p><strong>3. Push data to remote</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Upload actual data to DagsHub</span>
dvc push
</div></code></pre>
<p><strong>4. Teammates get your changes</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Teammate pulls code</span>
git pull

<span class="hljs-comment"># Teammate pulls data</span>
dvc pull
</div></code></pre>
<h4 id="common-commands">Common Commands</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Check what changed</span>
dvc status

<span class="hljs-comment"># Check if local and remote are in sync</span>
dvc status -c

<span class="hljs-comment"># Add/update data tracking</span>
dvc add data/raw
dvc add data/processed

<span class="hljs-comment"># Push data to remote</span>
dvc push

<span class="hljs-comment"># Pull data from remote</span>
dvc pull

<span class="hljs-comment"># List remotes</span>
dvc remote list

<span class="hljs-comment"># Check out specific data version (like git checkout for data)</span>
git checkout &lt;commit-hash&gt;
dvc pull
</div></code></pre>
<h3 id="part-79-updating-requirementstxt">Part 7.9: Updating requirements.txt</h3>
<p>Don't forget to add DVC to your dependencies!</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Check if DVC is already in requirements.txt</span>
grep dvc requirements.txt
</div></code></pre>
<p>If not present, add it:</p>
<pre class="hljs"><code><div># Add to requirements.txt

# Data Version Control (DagsHub remote - recommended)
dagshub==0.6.3
dvc-s3==3.2.2
</div></code></pre>
<p><strong>Note</strong>: If using Google Drive alternative, add:</p>
<pre class="hljs"><code><div>dvc-gdrive==3.0.1
</div></code></pre>
<p><strong>Commit the update</strong>:</p>
<pre class="hljs"><code><div>git add requirements.txt
git commit -m <span class="hljs-string">"chore: Add DVC dependencies to requirements.txt"</span>
git push
</div></code></pre>
<hr>
<h3 id="part-710-dvc-best-practices">Part 7.10: DVC Best Practices</h3>
<p>✅ <strong>DO</strong>:</p>
<ul>
<li>Track large files (&gt;10MB) with DVC</li>
<li>Track data and trained models with DVC</li>
<li>Keep small artifacts (&lt;1MB) in Git (preprocessing_pipeline.pkl is borderline)</li>
<li>Commit <code>.dvc</code> files to Git</li>
<li>Use <code>dvc push</code> after <code>dvc add</code></li>
<li>Document remote setup in README</li>
</ul>
<p>❌ <strong>DON'T</strong>:</p>
<ul>
<li>Track code with DVC (use Git for code)</li>
<li>Commit <code>.dvc/cache/</code> to Git (it's gitignored for a reason)</li>
<li>Commit <code>.dvc/config.local</code> to Git (contains credentials!)</li>
<li>Forget to push data (<code>dvc push</code> after changes)</li>
<li>Mix data versions across branches without care</li>
</ul>
<h3 id="part-711-troubleshooting-dvc">Part 7.11: Troubleshooting DVC</h3>
<p><strong>Problem</strong>: <code>dvc push</code> fails with &quot;Permission denied&quot;</p>
<p><strong>Solution</strong>: Check credentials:</p>
<pre class="hljs"><code><div>cat .dvc/config.local
<span class="hljs-comment"># Verify tokens are correct</span>
</div></code></pre>
<p><strong>Problem</strong>: <code>dvc pull</code> says &quot;Unable to find file&quot;</p>
<p><strong>Solution</strong>: Ensure remote has the data:</p>
<pre class="hljs"><code><div>dvc status -c
<span class="hljs-comment"># If out of sync, check Git commit and data version match</span>
</div></code></pre>
<p><strong>Problem</strong>: &quot;Output 'data/raw' is already tracked by SCM&quot;</p>
<p><strong>Solution</strong>: Remove from Git first:</p>
<pre class="hljs"><code><div>git rm -r --cached data/raw
dvc add data/raw
</div></code></pre>
<p><strong>Problem</strong>: Large data file accidentally in Git</p>
<p><strong>Solution</strong>: Remove from history:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Use BFG or git filter-branch (advanced)</span>
<span class="hljs-comment"># Better: Prevent by using .gitignore from start</span>
</div></code></pre>
<hr>
<h2 id="summary-of-part-7-dvc">Summary of Part 7: DVC</h2>
<p>Congratulations! You've now:</p>
<p>✅ <strong>Installed and initialized DVC</strong>
✅ <strong>Tracked data with DVC</strong> (removed from Git)
✅ <strong>Set up DagsHub remote</strong> (S3-compatible storage)
✅ <strong>Pushed data to cloud storage</strong>
✅ <strong>Tested pull workflow</strong> (simulated fresh clone)
✅ <strong>Understood Google Drive alternative</strong> (with limitations)
✅ <strong>Updated requirements.txt</strong> with DVC dependencies</p>
<h3 id="updated-project-structure">Updated Project Structure</h3>
<pre class="hljs"><code><div>your-project/
├── .dvc/
│   ├── .gitignore          # Ignores cache
│   ├── config              # Remote config (IN GIT)
│   ├── config.local        # Credentials (NOT in Git)
│   └── cache/              # Local data cache (NOT in Git)
├── data/
│   ├── .gitignore          # Created by DVC
│   ├── raw.dvc             # Metadata (IN GIT)
│   ├── processed.dvc       # Metadata (IN GIT)
│   ├── raw/                # Actual data (IN DVC CACHE)
│   └── processed/          # Actual data (IN DVC CACHE)
├── src/
│   ├── preprocess.py
│   ├── train.py
│   ├── predict.py
│   └── evaluate.py
├── models/
├── requirements.txt        # Now includes: dvc, dagshub, dvc-s3
└── README.md
</div></code></pre>
<p><strong>What's tracked where</strong>:</p>
<table>
<thead>
<tr>
<th>Item</th>
<th>Git</th>
<th>DVC</th>
<th>DagsHub</th>
</tr>
</thead>
<tbody>
<tr>
<td>Code (<code>.py</code>)</td>
<td>✅ Yes</td>
<td>❌ No</td>
<td>✅ Synced from Git</td>
</tr>
<tr>
<td>Config (<code>.yaml</code>)</td>
<td>✅ Yes</td>
<td>❌ No</td>
<td>✅ Synced from Git</td>
</tr>
<tr>
<td>DVC metadata (<code>.dvc</code>)</td>
<td>✅ Yes</td>
<td>❌ No</td>
<td>✅ Synced from Git</td>
</tr>
<tr>
<td>Data files</td>
<td>❌ No</td>
<td>✅ Yes</td>
<td>✅ Yes (storage)</td>
</tr>
<tr>
<td>Model files</td>
<td>❌ No</td>
<td>✅ Yes</td>
<td>✅ Yes (storage)</td>
</tr>
<tr>
<td>Credentials</td>
<td>❌ No</td>
<td>🔒 Local only</td>
<td>❌ No</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="part-8-experiment-tracking-with-mlflow">Part 8: Experiment Tracking with MLflow</h2>
<h3 id="introduction-the-experiment-tracking-problem">Introduction: The Experiment Tracking Problem</h3>
<p>Imagine you're training multiple machine learning models:</p>
<pre class="hljs"><code><div>Monday: Random Forest, n_estimators=100, max_depth=10 → Accuracy: 79.3%
Tuesday: Random Forest, n_estimators=200, max_depth=15 → Accuracy: 80.1%
Wednesday: Random Forest, n_estimators=150, max_depth=12 → Accuracy: ???
</div></code></pre>
<p><strong>Questions you can't answer</strong>:</p>
<ul>
<li>❌ Which hyperparameters gave the best results?</li>
<li>❌ What was the exact configuration of Monday's model?</li>
<li>❌ How do I compare all experiments side-by-side?</li>
<li>❌ Where did I save that good model from Tuesday?</li>
<li>❌ What data version was used for each experiment?</li>
</ul>
<p><strong>Traditional &quot;solutions&quot; (all problematic)</strong>:</p>
<ul>
<li>Spreadsheet tracking (manual, error-prone)</li>
<li>Text file logging (unstructured, hard to analyze)</li>
<li>Print statements (lost after terminal closes)</li>
<li>Manual naming conventions (inconsistent, confusing)</li>
</ul>
<h4 id="what-is-mlflow">What is MLflow?</h4>
<p><strong>MLflow</strong> is an open-source platform for managing the end-to-end machine learning lifecycle.</p>
<p><strong>Created by</strong>: Databricks (2018)<br>
<strong>Industry Adoption</strong>: Microsoft, Toyota, Netflix, Walmart<br>
<strong>Why it's standard</strong>: Works with any ML library (scikit-learn, TensorFlow, PyTorch)</p>
<hr>
<h3 id="part-81-installing-mlflow">Part 8.1: Installing MLflow</h3>
<h4 id="step-1-install-mlflow">Step 1: Install MLflow</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Activate virtual environment</span>
<span class="hljs-built_in">source</span> .venv/bin/activate

<span class="hljs-comment"># Install MLflow</span>
pip install mlflow

<span class="hljs-comment"># Verify installation</span>
mlflow --version
</div></code></pre>
<h4 id="step-2-update-requirementstxt">Step 2: Update requirements.txt</h4>
<pre class="hljs"><code><div><span class="hljs-built_in">echo</span> <span class="hljs-string">"mlflow==3.5.1"</span> &gt;&gt; requirements.txt
</div></code></pre>
<p>Or manually add to <code>requirements.txt</code>:</p>
<pre class="hljs"><code><div>mlflow==3.5.1
</div></code></pre>
<p><strong>Commit</strong>:</p>
<pre class="hljs"><code><div>git add requirements.txt
git commit -m <span class="hljs-string">"chore: Add MLflow for experiment tracking"</span>
git push
</div></code></pre>
<hr>
<h3 id="part-82-understanding-mlflow-tracking">Part 8.2: Understanding MLflow Tracking</h3>
<h4 id="mlflow-hierarchy">MLflow Hierarchy</h4>
<pre class="hljs"><code><div>Experiment (e.g., &quot;telecom-churn-prediction&quot;)
├── Run 1 (Random Forest, 100 trees)
│   ├── Parameters: n_estimators=100, max_depth=10
│   ├── Metrics: accuracy=0.793, f1=0.585
│   └── Artifacts: model.pkl, confusion_matrix.png
├── Run 2 (Random Forest, 200 trees)
│   ├── Parameters: n_estimators=200, max_depth=15
│   ├── Metrics: accuracy=0.801, f1=0.603
│   └── Artifacts: model.pkl, confusion_matrix.png
</div></code></pre>
<p><strong>Key Concepts</strong>:</p>
<ul>
<li><strong>Experiment</strong>: Collection of related runs</li>
<li><strong>Run</strong>: One execution of training code</li>
<li><strong>Parameters</strong>: Input settings (hyperparameters)</li>
<li><strong>Metrics</strong>: Output measurements (accuracy, loss)</li>
<li><strong>Artifacts</strong>: Output files (models, plots)</li>
<li><strong>Tags</strong>: Metadata for organization</li>
</ul>
<hr>
<h3 id="part-83-updating-trainpy-with-mlflow">Part 8.3: Updating train.py with MLflow</h3>
<h4 id="step-1-add-mlflow-imports">Step 1: Add MLflow Imports</h4>
<p>At the top of <code>src/train.py</code>:</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> mlflow
<span class="hljs-keyword">import</span> mlflow.sklearn
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, confusion_matrix
)
</div></code></pre>
<h4 id="step-2-set-mlflow-experiment">Step 2: Set MLflow Experiment</h4>
<p>In your <code>main()</code> function:</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-comment"># ... argument parsing ...</span>
    
    <span class="hljs-comment"># Set MLflow experiment</span>
    mlflow.set_experiment(<span class="hljs-string">"telecom-churn-prediction"</span>)
    
    <span class="hljs-comment"># ... rest of code ...</span>
</div></code></pre>
<h4 id="step-3-add-mlflow-logging-to-training-functions">Step 3: Add MLflow Logging to Training Functions</h4>
<p>Update each training function to log parameters:</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_random_forest</span><span class="hljs-params">(X_train, y_train, tune_hyperparameters=False, **kwargs)</span>:</span>
    <span class="hljs-string">"""Train Random Forest with MLflow logging."""</span>
    
    <span class="hljs-keyword">if</span> tune_hyperparameters:
        param_grid = {...}
        mlflow.log_param(<span class="hljs-string">"param_grid"</span>, str(param_grid))  <span class="hljs-comment"># Log grid</span>
        
        <span class="hljs-comment"># ... GridSearchCV code ...</span>
        
        <span class="hljs-comment"># Log best parameters</span>
        <span class="hljs-keyword">for</span> param, value <span class="hljs-keyword">in</span> grid_search.best_params_.items():
            mlflow.log_param(<span class="hljs-string">f"best_<span class="hljs-subst">{param}</span>"</span>, value)
        mlflow.log_metric(<span class="hljs-string">"cv_best_score"</span>, grid_search.best_score_)
    <span class="hljs-keyword">else</span>:
        <span class="hljs-comment"># Log default parameters</span>
        params = {<span class="hljs-string">'n_estimators'</span>: <span class="hljs-number">100</span>, <span class="hljs-string">'max_depth'</span>: <span class="hljs-literal">None</span>}
        <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> params.items():
            mlflow.log_param(key, value)
    
    <span class="hljs-comment"># ... rest of training ...</span>
</div></code></pre>
<h4 id="step-4-wrap-training-in-mlflow-run">Step 4: Wrap Training in MLflow Run</h4>
<p>In your <code>main()</code> function, wrap training:</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-comment"># ... setup code ...</span>
    
    <span class="hljs-keyword">if</span> args.model == <span class="hljs-string">'all'</span>:
        <span class="hljs-comment"># Train each model in its own MLflow run</span>
        <span class="hljs-keyword">for</span> model_name <span class="hljs-keyword">in</span> [<span class="hljs-string">'random_forest'</span>, <span class="hljs-string">'gradient_boosting'</span>, ...]:
            <span class="hljs-keyword">with</span> mlflow.start_run(run_name=<span class="hljs-string">f"<span class="hljs-subst">{model_name}</span>_<span class="hljs-subst">{timestamp}</span>"</span>):
                <span class="hljs-comment"># Log tags</span>
                mlflow.set_tag(<span class="hljs-string">"model_type"</span>, model_name)
                mlflow.set_tag(<span class="hljs-string">"tuning"</span>, <span class="hljs-string">"enabled"</span> <span class="hljs-keyword">if</span> args.tune <span class="hljs-keyword">else</span> <span class="hljs-string">"disabled"</span>)
                
                <span class="hljs-comment"># Log parameters</span>
                mlflow.log_param(<span class="hljs-string">"model_type"</span>, model_name)
                mlflow.log_param(<span class="hljs-string">"tune_hyperparameters"</span>, args.tune)
                
                <span class="hljs-comment"># Train model</span>
                model = train_model(...)
                
                <span class="hljs-comment"># Evaluate and log metrics</span>
                metrics = evaluate_model(model, X_test, y_test)
                <span class="hljs-keyword">for</span> metric_name, value <span class="hljs-keyword">in</span> metrics.items():
                    mlflow.log_metric(metric_name, value)
                
                <span class="hljs-comment"># Log model</span>
                mlflow.sklearn.log_model(model, <span class="hljs-string">"model"</span>)
                
                <span class="hljs-comment"># Log local file as artifact</span>
                mlflow.log_artifact(model_path, <span class="hljs-string">"local_models"</span>)
</div></code></pre>
<h4 id="step-5-add-evaluation-function">Step 5: Add Evaluation Function</h4>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate_model</span><span class="hljs-params">(model, X_test, y_test)</span>:</span>
    <span class="hljs-string">"""Evaluate model and return metrics."""</span>
    yhat = model.predict(X_test)
    
    metrics = {
        <span class="hljs-string">'accuracy'</span>: accuracy_score(y_test, yhat),
        <span class="hljs-string">'precision'</span>: precision_score(y_test, yhat, average=<span class="hljs-string">'binary'</span>, pos_label=<span class="hljs-string">'Yes'</span>),
        <span class="hljs-string">'recall'</span>: recall_score(y_test, yhat, average=<span class="hljs-string">'binary'</span>, pos_label=<span class="hljs-string">'Yes'</span>),
        <span class="hljs-string">'f1_score'</span>: f1_score(y_test, yhat, average=<span class="hljs-string">'binary'</span>, pos_label=<span class="hljs-string">'Yes'</span>)
    }
    
    <span class="hljs-comment"># ROC-AUC if model supports predict_proba</span>
    <span class="hljs-keyword">if</span> hasattr(model, <span class="hljs-string">'predict_proba'</span>):
        y_proba = model.predict_proba(X_test)[:, <span class="hljs-number">1</span>]
        metrics[<span class="hljs-string">'roc_auc'</span>] = roc_auc_score(y_test == <span class="hljs-string">'Yes'</span>, y_proba)
    
    <span class="hljs-keyword">return</span> metrics
</div></code></pre>
<hr>
<h3 id="part-84-running-training-with-mlflow">Part 8.4: Running Training with MLflow</h3>
<h4 id="train-a-single-model">Train a Single Model</h4>
<pre class="hljs"><code><div>python -m src.train \
    --data data/processed/preprocessed_data.npy \
    --model random_forest

<span class="hljs-comment"># Output shows MLflow run ID:</span>
<span class="hljs-comment"># MLflow run ID: abc123def456...</span>
</div></code></pre>
<h4 id="train-with-hyperparameter-tuning">Train with Hyperparameter Tuning</h4>
<pre class="hljs"><code><div>python -m src.train \
    --data data/processed/preprocessed_data.npy \
    --model random_forest \
    --tune
</div></code></pre>
<h4 id="train-all-models">Train All Models</h4>
<pre class="hljs"><code><div>python -m src.train \
    --data data/processed/preprocessed_data.npy \
    --model all
</div></code></pre>
<hr>
<h3 id="part-85-accessing-mlflow-ui-in-codespaces">Part 8.5: Accessing MLflow UI in CodeSpaces</h3>
<h4 id="critical-port-forwarding-required">CRITICAL: Port Forwarding Required</h4>
<p><strong>Why</strong>: CodeSpaces is a remote virtual machine. When you run <code>mlflow ui</code>, it starts on that remote machine's localhost, not your computer.</p>
<h4 id="step-1-start-mlflow-ui">Step 1: Start MLflow UI</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Must use 0.0.0.0 to allow external connections</span>
mlflow ui --host 0.0.0.0 --port 5000
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>[2024-10-27 10:30:00] [INFO] Listening at: http://0.0.0.0:5000
</div></code></pre>
<p><strong>Leave this terminal running!</strong> Open a new terminal for other commands.</p>
<h4 id="step-2-forward-port-in-codespaces">Step 2: Forward Port in CodeSpaces</h4>
<p><strong>In VS Code</strong>:</p>
<ol>
<li>
<p><strong>Look at bottom panel</strong> - Click <strong>PORTS</strong> tab</p>
<ul>
<li>Should see: <code>PROBLEMS | OUTPUT | DEBUG CONSOLE | TERMINAL | PORTS</code></li>
</ul>
</li>
<li>
<p><strong>Port 5000 should appear automatically</strong></p>
<ul>
<li>If not, click <strong>&quot;Forward a Port&quot;</strong> button (or <strong>+</strong> icon)</li>
<li>Enter: <code>5000</code></li>
<li>Press Enter</li>
</ul>
</li>
<li>
<p><strong>Open in Browser</strong>:</p>
<ul>
<li>Right-click on port 5000 row</li>
<li>Select <strong>&quot;Open in Browser&quot;</strong></li>
</ul>
<p>OR</p>
<ul>
<li>Hover over &quot;Forwarded Address&quot; column</li>
<li>Click globe icon 🌐</li>
</ul>
<p>OR</p>
<ul>
<li>Copy the URL (like <code>https://username-repo-abc123-5000.app.github.dev</code>)</li>
<li>Paste into new browser tab</li>
</ul>
</li>
</ol>
<h4 id="step-3-verify-mlflow-ui">Step 3: Verify MLflow UI</h4>
<p>You should see:</p>
<ul>
<li>Experiments list (left sidebar)</li>
<li>&quot;telecom-churn-prediction&quot; experiment</li>
<li>Runs table with your training runs</li>
<li>Metrics columns (accuracy, f1_score, etc.)</li>
</ul>
<p><strong>Troubleshooting</strong>:</p>
<ul>
<li>&quot;Can't reach this page&quot; → Check mlflow ui still running</li>
<li>Blank page → Try incognito/private window</li>
<li>Port not listed → Manually forward port 5000</li>
<li>Still not working → See troubleshooting section below</li>
</ul>
<hr>
<h3 id="part-86-comparing-experiments">Part 8.6: Comparing Experiments</h3>
<h4 id="step-1-train-multiple-models">Step 1: Train Multiple Models</h4>
<pre class="hljs"><code><div>python -m src.train --data data.npy --model random_forest
python -m src.train --data data.npy --model random_forest --tune
python -m src.train --data data.npy --model gradient_boosting
python -m src.train --data data.npy --model logistic_regression
</div></code></pre>
<h4 id="step-2-select-runs">Step 2: Select Runs</h4>
<p>In MLflow UI:</p>
<ol>
<li>Check boxes next to 2-4 runs</li>
<li>Click <strong>&quot;Compare&quot;</strong> button</li>
</ol>
<h4 id="step-3-view-comparison">Step 3: View Comparison</h4>
<p><strong>Comparison page shows</strong>:</p>
<ul>
<li><strong>Parallel Coordinates Plot</strong>: Visual comparison</li>
<li><strong>Scatter Plot</strong>: Parameter vs. metric correlation</li>
<li><strong>Parameters Table</strong>: Side-by-side parameter comparison</li>
<li><strong>Metrics Table</strong>: Side-by-side metric comparison</li>
</ul>
<p><strong>To find best model</strong>:</p>
<ol>
<li>Sort metrics table by &quot;accuracy&quot; (click column header)</li>
<li>Note the best run ID</li>
<li>Check other metrics (F1, ROC-AUC)</li>
<li>Consider training time</li>
</ol>
<hr>
<h3 id="part-87-loading-models-from-mlflow">Part 8.7: Loading Models from MLflow</h3>
<h4 id="method-1-load-by-run-id">Method 1: Load by Run ID</h4>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> mlflow.sklearn

<span class="hljs-comment"># Get run ID from MLflow UI</span>
run_id = <span class="hljs-string">"abc123def456..."</span>

<span class="hljs-comment"># Load model</span>
model = mlflow.sklearn.load_model(<span class="hljs-string">f"runs:/<span class="hljs-subst">{run_id}</span>/model"</span>)

<span class="hljs-comment"># Use model</span>
predictions = model.predict(X_test)
</div></code></pre>
<h4 id="method-2-load-best-model">Method 2: Load Best Model</h4>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> mlflow

<span class="hljs-comment"># Search for best run</span>
mlflow.set_experiment(<span class="hljs-string">"telecom-churn-prediction"</span>)
runs = mlflow.search_runs(
    order_by=[<span class="hljs-string">"metrics.accuracy DESC"</span>],
    max_results=<span class="hljs-number">1</span>
)

<span class="hljs-comment"># Load best model</span>
best_run_id = runs.iloc[<span class="hljs-number">0</span>][<span class="hljs-string">'run_id'</span>]
model = mlflow.sklearn.load_model(<span class="hljs-string">f"runs:/<span class="hljs-subst">{best_run_id}</span>/model"</span>)

print(<span class="hljs-string">f"Loaded model from run: <span class="hljs-subst">{best_run_id}</span>"</span>)
print(<span class="hljs-string">f"Accuracy: <span class="hljs-subst">{runs.iloc[<span class="hljs-number">0</span>][<span class="hljs-string">'metrics.accuracy'</span>]}</span>"</span>)
</div></code></pre>
<hr>
<h3 id="part-88-mlflow-best-practices">Part 8.8: MLflow Best Practices</h3>
<h4 id="what-to-log">What to Log</h4>
<p><strong>Always log</strong>:</p>
<ul>
<li>✅ Model type and version</li>
<li>✅ All hyperparameters (including defaults)</li>
<li>✅ Performance metrics (accuracy, precision, recall, F1)</li>
<li>✅ Training time</li>
<li>✅ Random seed</li>
</ul>
<p><strong>Consider logging</strong>:</p>
<ul>
<li>Cross-validation scores</li>
<li>Confusion matrix values</li>
<li>Feature importance</li>
<li>Data version (DVC hash)</li>
</ul>
<h4 id="naming-conventions">Naming Conventions</h4>
<p><strong>Experiments</strong>:</p>
<pre class="hljs"><code><div>✅ &quot;telecom-churn-prediction&quot;
✅ &quot;customer-churn-v2&quot;
❌ &quot;experiment1&quot;
❌ &quot;test&quot;
</div></code></pre>
<p><strong>Run names</strong>:</p>
<pre class="hljs"><code><div>✅ &quot;random_forest_20241027_103045&quot;
✅ &quot;rf_tuned_final&quot;
❌ &quot;run1&quot;
❌ &quot;final_final_v2&quot;
</div></code></pre>
<h4 id="organization-tips">Organization Tips</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Use tags for filtering</span>
mlflow.set_tag(<span class="hljs-string">"model_type"</span>, <span class="hljs-string">"random_forest"</span>)
mlflow.set_tag(<span class="hljs-string">"data_version"</span>, <span class="hljs-string">"v1.0"</span>)
mlflow.set_tag(<span class="hljs-string">"tuning"</span>, <span class="hljs-string">"enabled"</span>)
mlflow.set_tag(<span class="hljs-string">"environment"</span>, <span class="hljs-string">"development"</span>)
</div></code></pre>
<hr>
<h3 id="part-89-troubleshooting">Part 8.9: Troubleshooting</h3>
<h4 id="problem-mlflow-ui-not-accessible">Problem: MLflow UI Not Accessible</h4>
<p><strong>Symptoms</strong>: &quot;Can't reach this page&quot;, connection refused</p>
<p><strong>Solutions</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 1. Check MLflow is running</span>
<span class="hljs-comment"># Terminal should show "Listening at: http://0.0.0.0:5000"</span>

<span class="hljs-comment"># 2. Restart with correct host</span>
mlflow ui --host 0.0.0.0 --port 5000

<span class="hljs-comment"># 3. In PORTS tab:</span>
<span class="hljs-comment"># - Verify port 5000 is listed</span>
<span class="hljs-comment"># - Right-click → "Port Visibility" → "Public"</span>
<span class="hljs-comment"># - Click "Open in Browser"</span>
</div></code></pre>
<h4 id="problem-runs-not-showing">Problem: Runs Not Showing</h4>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Refresh browser (F5)</li>
<li>Check experiment name matches</li>
<li>Verify mlruns/ directory exists: <code>ls mlruns/</code></li>
</ol>
<h4 id="problem-large-mlruns-directory">Problem: Large mlruns/ Directory</h4>
<p><strong>Solution</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Delete old experiments</span>
mlflow experiments delete --experiment-id 1

<span class="hljs-comment"># Or delete via UI (select runs → delete)</span>
</div></code></pre>
<hr>
<h3 id="part-810-update-gitignore">Part 8.10: Update .gitignore</h3>
<p>Add to <code>.gitignore</code>:</p>
<pre class="hljs"><code><div># MLflow
mlruns/
mlflow-artifacts/
mlartifacts/
mlflow.db*
.mlflow/
</div></code></pre>
<p><strong>Commit</strong>:</p>
<pre class="hljs"><code><div>git add .gitignore
git commit -m <span class="hljs-string">"chore: Update .gitignore for MLflow"</span>
</div></code></pre>
<hr>
<h3 id="summary">Summary</h3>
<p>You've completed MLflow integration! You can now:</p>
<p>✅ Track experiments automatically<br>
✅ Compare models visually<br>
✅ Load best models by metric<br>
✅ Access MLflow UI in CodeSpaces<br>
✅ Reproduce results reliably</p>
<h4 id="next-steps">Next Steps</h4>
<p><strong>View your experiments</strong>:</p>
<pre class="hljs"><code><div>mlflow ui --host 0.0.0.0 --port 5000
<span class="hljs-comment"># PORTS tab → Forward 5000 → Open in Browser</span>
</div></code></pre>
<p><strong>Train and compare</strong>:</p>
<pre class="hljs"><code><div>python -m src.train --data data.npy --model all --tune
<span class="hljs-comment"># Then compare in MLflow UI</span>
</div></code></pre>
<hr>
<h2 id="part-9-automated-testing-with-pytest">Part 9: Automated Testing with pytest</h2>
<h3 id="testing-overview">Testing Overview</h3>
<p>This is the final piece of your production-ready ML project! In this section, you'll add <strong>automated testing</strong> to ensure your code is reliable, maintainable, and bug-free.</p>
<p>Testing is a critical part of software engineering that's often overlooked in ML projects. However, in production environments, untested code is a major risk. This lab will teach you industry-standard testing practices using <strong>pytest</strong>, the most popular Python testing framework.</p>
<h4 id="what-youve-built-so-far">What You've Built So Far</h4>
<p>Lab 02 has been an intensive journey:</p>
<ul>
<li>✅ <strong>Part 1-2</strong>: Virtual environments &amp; dependency management</li>
<li>✅ <strong>Part 3-4</strong>: CLI interfaces &amp; code enhancements</li>
<li>✅ <strong>Part 5-6</strong>: YAML configuration &amp; best practices</li>
<li>✅ <strong>Part 7</strong>: DVC for data version control</li>
<li>✅ <strong>Part 8</strong>: MLflow for experiment tracking</li>
<li>🎯 <strong>Part 9</strong>: Automated testing (You are here!)</li>
</ul>
<h4 id="testing-learning-objectives">Testing Learning Objectives</h4>
<p>By the end of this part, you will:</p>
<ol>
<li><strong>Understand testing fundamentals</strong> - Why test, what to test, how to test</li>
<li><strong>Master pytest</strong> - Write and run unit tests, integration tests, and parametrized tests</li>
<li><strong>Create test fixtures</strong> - Reusable test data and configurations</li>
<li><strong>Measure code coverage</strong> - Ensure your tests cover critical code paths</li>
<li><strong>Test ML pipelines</strong> - Special considerations for testing ML code</li>
<li><strong>Mock external dependencies</strong> - Test without hitting databases or APIs</li>
</ol>
<h4 id="why-testing-matters">Why Testing Matters</h4>
<p><strong>The cost of bugs in production</strong>:</p>
<pre class="hljs"><code><div>Development:   Bug costs $1 to fix
Testing:       Bug costs $10 to fix
Production:    Bug costs $100 to fix
</div></code></pre>
<p><strong>Real-world ML disasters caused by lack of testing</strong>:</p>
<ul>
<li>Amazon's AI recruiting tool showed bias against women (insufficient testing for fairness)</li>
<li>Knight Capital lost $440 million in 45 minutes (deployment without proper testing)</li>
<li>Mars Climate Orbiter crashed ($125M loss) due to unit conversion error</li>
</ul>
<p><strong>Testing benefits</strong>:</p>
<ul>
<li>✅ Catch bugs before production</li>
<li>✅ Refactor confidently</li>
<li>✅ Document expected behavior</li>
<li>✅ Enable continuous integration/deployment</li>
<li>✅ Improve code quality</li>
</ul>
<hr>
<h3 id="part-91-understanding-testing-fundamentals">Part 9.1: Understanding Testing Fundamentals</h3>
<h4 id="types-of-tests">Types of Tests</h4>
<table>
<thead>
<tr>
<th>Test Type</th>
<th>What it Tests</th>
<th>Example</th>
<th>Speed</th>
<th>Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Unit Tests</strong></td>
<td>Individual functions</td>
<td><code>test_load_data()</code> tests just <code>load_data()</code></td>
<td>⚡ Fast</td>
<td>Narrow</td>
</tr>
<tr>
<td><strong>Integration Tests</strong></td>
<td>Multiple components</td>
<td><code>test_preprocessing_pipeline()</code> tests full flow</td>
<td>🐢 Slower</td>
<td>Broad</td>
</tr>
<tr>
<td><strong>End-to-End Tests</strong></td>
<td>Complete workflows</td>
<td>CSV → Model → Predictions</td>
<td>🐌 Slowest</td>
<td>Complete</td>
</tr>
</tbody>
</table>
<h4 id="the-testing-pyramid">The Testing Pyramid</h4>
<pre class="hljs"><code><div>         ╱╲
        ╱  ╲     E2E Tests (Few, Slow, High-Level)
       ╱────╲
      ╱      ╲   Integration Tests (Some, Medium)
     ╱────────╲
    ╱          ╲ Unit Tests (Many, Fast, Low-Level)
   ──────────────
</div></code></pre>
<p><strong>Best practice</strong>: Write mostly unit tests, some integration tests, and few E2E tests.</p>
<h4 id="what-to-test-in-ml-projects">What to Test in ML Projects</h4>
<p><strong>Always test</strong>:</p>
<ul>
<li>✅ Data loading and validation</li>
<li>✅ Preprocessing functions</li>
<li>✅ Model training (that it completes without errors)</li>
<li>✅ Prediction functions</li>
<li>✅ Evaluation metrics</li>
<li>✅ Edge cases (empty data, missing values, wrong types)</li>
</ul>
<p><strong>Don't test</strong>:</p>
<ul>
<li>❌ Third-party library internals (scikit-learn already tests their code)</li>
<li>❌ Exact model accuracy (can vary, test that it's in a reasonable range)</li>
<li>❌ Hyperparameter tuning results (non-deterministic, too slow)</li>
</ul>
<h4 id="test-driven-development-tdd">Test-Driven Development (TDD)</h4>
<p><strong>TDD Cycle</strong> (optional, but recommended):</p>
<ol>
<li><strong>Red</strong>: Write a failing test</li>
<li><strong>Green</strong>: Write minimal code to make it pass</li>
<li><strong>Refactor</strong>: Improve the code while keeping tests passing</li>
</ol>
<hr>
<h3 id="part-92-setting-up-pytest">Part 9.2: Setting Up pytest</h3>
<h4 id="install-pytest">Install pytest</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Activate your virtual environment</span>
<span class="hljs-built_in">source</span> .venv/bin/activate

<span class="hljs-comment"># Install pytest and pytest-cov</span>
pip install pytest==8.4.2 pytest-cov==7.0.0

<span class="hljs-comment"># Verify installation</span>
pytest --version
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>pytest 8.4.2
</div></code></pre>
<h4 id="create-pytestini-configuration">Create pytest.ini Configuration</h4>
<p>Create a <code>pytest.ini</code> file in your project root:</p>
<pre class="hljs"><code><div>touch pytest.ini
</div></code></pre>
<p>Add this configuration:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># pytest.ini - pytest Configuration</span>

<span class="hljs-section">[pytest]</span>
<span class="hljs-comment"># Test discovery patterns</span>
<span class="hljs-attr">python_files</span> = test_*.py
<span class="hljs-attr">python_classes</span> = Test*
<span class="hljs-attr">python_functions</span> = test_*

<span class="hljs-comment"># Directories to search for tests</span>
<span class="hljs-attr">testpaths</span> = tests

<span class="hljs-comment"># Additional command line options</span>
<span class="hljs-attr">addopts</span> =
    -v
    --strict-markers
    --tb=short
    --disable-warnings
    -ra

<span class="hljs-comment"># Markers for organizing tests</span>
<span class="hljs-attr">markers</span> =
    unit: Unit tests for individual functions
    integration: Integration tests for workflows
    slow: Tests that take longer to run
    cli: Tests for command-line interfaces

<span class="hljs-comment"># Minimum Python version</span>
<span class="hljs-attr">minversion</span> = <span class="hljs-number">3.8</span>

<span class="hljs-comment"># Output options</span>
<span class="hljs-attr">console_output_style</span> = progress

<span class="hljs-comment"># Warnings</span>
<span class="hljs-attr">filterwarnings</span> =
    ignore::DeprecationWarning
</div></code></pre>
<p><strong>What this does</strong>:</p>
<ul>
<li><code>python_files = test_*.py</code>: Only run files starting with <code>test_</code></li>
<li><code>testpaths = tests</code>: Look for tests in <code>tests/</code> directory</li>
<li><code>-v</code>: Verbose output</li>
<li><code>markers</code>: Custom tags for organizing tests</li>
</ul>
<h4 id="create-tests-directory-structure">Create Tests Directory Structure</h4>
<pre class="hljs"><code><div>mkdir -p tests
touch tests/__init__.py
touch tests/conftest.py
</div></code></pre>
<p>Your project now looks like:</p>
<pre class="hljs"><code><div>your-project/
├── tests/
│   ├── __init__.py
│   └── conftest.py
├── pytest.ini
└── src/
    ├── preprocess.py
    ├── train.py
    ├── predict.py
    └── evaluate.py
</div></code></pre>
<hr>
<h3 id="part-93-writing-your-first-test">Part 9.3: Writing Your First Test</h3>
<h4 id="create-testpreprocesspy">Create test_preprocess.py</h4>
<p>Create <code>tests/test_preprocess.py</code>:</p>
<pre class="hljs"><code><div><span class="hljs-string">"""
Tests for src/preprocess.py module.
"""</span>

<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> pytest

<span class="hljs-keyword">from</span> src.preprocess <span class="hljs-keyword">import</span> load_data


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_load_data_success</span><span class="hljs-params">(tmp_path)</span>:</span>
    <span class="hljs-string">"""Test successful data loading from CSV."""</span>
    <span class="hljs-comment"># Create a temporary CSV file</span>
    csv_file = tmp_path / <span class="hljs-string">"test_data.csv"</span>
    csv_file.write_text(<span class="hljs-string">"col1,col2\\n1,2\\n3,4\\n"</span>)
    
    <span class="hljs-comment"># Load data</span>
    df = load_data(str(csv_file))
    
    <span class="hljs-comment"># Assertions</span>
    <span class="hljs-keyword">assert</span> isinstance(df, pd.DataFrame)
    <span class="hljs-keyword">assert</span> len(df) == <span class="hljs-number">2</span>
    <span class="hljs-keyword">assert</span> list(df.columns) == [<span class="hljs-string">'col1'</span>, <span class="hljs-string">'col2'</span>]


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_load_data_file_not_found</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-string">"""Test error handling for missing file."""</span>
    <span class="hljs-keyword">with</span> pytest.raises(FileNotFoundError):
        load_data(<span class="hljs-string">"nonexistent_file.csv"</span>)
</div></code></pre>
<h4 id="run-your-first-test">Run Your First Test</h4>
<pre class="hljs"><code><div>pytest tests/test_preprocess.py -v
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>tests/test_preprocess.py::test_load_data_success PASSED           [ 50%]
tests/test_preprocess.py::test_load_data_file_not_found PASSED   [100%]

==================== 2 passed in 0.05s ====================
</div></code></pre>
<p><strong>Congratulations! You just wrote and ran your first tests!</strong> 🎉</p>
<hr>
<h3 id="part-94-understanding-test-structure">Part 9.4: Understanding Test Structure</h3>
<h4 id="anatomy-of-a-test-function">Anatomy of a Test Function</h4>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_function_name</span><span class="hljs-params">()</span>:</span>              <span class="hljs-comment"># 1. Name starts with 'test_'</span>
    <span class="hljs-string">"""Docstring explaining what we test."""</span>  <span class="hljs-comment"># 2. Documentation</span>
    
    <span class="hljs-comment"># 3. Arrange: Set up test data</span>
    X = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])
    y = np.array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])
    
    <span class="hljs-comment"># 4. Act: Execute the code being tested</span>
    model = train_model(X, y)
    
    <span class="hljs-comment"># 5. Assert: Verify the results</span>
    <span class="hljs-keyword">assert</span> model <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
    <span class="hljs-keyword">assert</span> hasattr(model, <span class="hljs-string">'predict'</span>)
</div></code></pre>
<p><strong>The 3 A's of Testing</strong> (AAA Pattern):</p>
<ol>
<li><strong>Arrange</strong>: Set up test data and preconditions</li>
<li><strong>Act</strong>: Execute the function/code being tested</li>
<li><strong>Assert</strong>: Check that results match expectations</li>
</ol>
<h4 id="common-assertions">Common Assertions</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Equality</span>
<span class="hljs-keyword">assert</span> x == y
<span class="hljs-keyword">assert</span> x != y

<span class="hljs-comment"># Identity</span>
<span class="hljs-keyword">assert</span> x <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>
<span class="hljs-keyword">assert</span> x <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>

<span class="hljs-comment"># Membership</span>
<span class="hljs-keyword">assert</span> <span class="hljs-string">'key'</span> <span class="hljs-keyword">in</span> dictionary
<span class="hljs-keyword">assert</span> item <span class="hljs-keyword">in</span> list

<span class="hljs-comment"># Type checking</span>
<span class="hljs-keyword">assert</span> isinstance(obj, ClassName)

<span class="hljs-comment"># Comparisons</span>
<span class="hljs-keyword">assert</span> x &gt; y
<span class="hljs-keyword">assert</span> x &gt;= y
<span class="hljs-keyword">assert</span> x &lt; y
<span class="hljs-keyword">assert</span> x &lt;= y

<span class="hljs-comment"># Boolean</span>
<span class="hljs-keyword">assert</span> condition
<span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> condition

<span class="hljs-comment"># Exceptions</span>
<span class="hljs-keyword">with</span> pytest.raises(ValueError):
    function_that_should_raise()

<span class="hljs-comment"># Floating point (with tolerance)</span>
<span class="hljs-keyword">assert</span> abs(x - y) &lt; <span class="hljs-number">0.0001</span>
<span class="hljs-comment"># or</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
np.testing.assert_almost_equal(x, y, decimal=<span class="hljs-number">4</span>)
</div></code></pre>
<hr>
<h3 id="part-95-creating-test-fixtures">Part 9.5: Creating Test Fixtures</h3>
<h4 id="what-are-fixtures">What are Fixtures?</h4>
<p><strong>Fixtures</strong> are reusable test data or configurations. Instead of creating the same test data in every test, you create it once as a fixture.</p>
<h4 id="create-conftestpy-with-fixtures">Create conftest.py with Fixtures</h4>
<p>Edit <code>tests/conftest.py</code>:</p>
<pre class="hljs"><code><div><span class="hljs-string">"""
pytest fixtures for test suite.
"""</span>

<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> pytest


<span class="hljs-meta">@pytest.fixture</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sample_data</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-string">"""
    Create small sample dataset for testing.
    """</span>
    data = {
        <span class="hljs-string">'customerID'</span>: [<span class="hljs-string">'C001'</span>, <span class="hljs-string">'C002'</span>, <span class="hljs-string">'C003'</span>],
        <span class="hljs-string">'gender'</span>: [<span class="hljs-string">'Male'</span>, <span class="hljs-string">'Female'</span>, <span class="hljs-string">'Male'</span>],
        <span class="hljs-string">'tenure'</span>: [<span class="hljs-number">12</span>, <span class="hljs-number">24</span>, <span class="hljs-number">36</span>],
        <span class="hljs-string">'MonthlyCharges'</span>: [<span class="hljs-number">50.5</span>, <span class="hljs-number">70.25</span>, <span class="hljs-number">25.0</span>],
        <span class="hljs-string">'Churn'</span>: [<span class="hljs-string">'No'</span>, <span class="hljs-string">'No'</span>, <span class="hljs-string">'Yes'</span>]
    }
    <span class="hljs-keyword">return</span> pd.DataFrame(data)


<span class="hljs-meta">@pytest.fixture</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">processed_data</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-string">"""
    Create preprocessed training data.
    """</span>
    np.random.seed(<span class="hljs-number">42</span>)
    
    X_train = np.random.randn(<span class="hljs-number">80</span>, <span class="hljs-number">15</span>)
    X_test = np.random.randn(<span class="hljs-number">20</span>, <span class="hljs-number">15</span>)
    y_train = np.random.choice([<span class="hljs-string">'Yes'</span>, <span class="hljs-string">'No'</span>], <span class="hljs-number">80</span>)
    y_test = np.random.choice([<span class="hljs-string">'Yes'</span>, <span class="hljs-string">'No'</span>], <span class="hljs-number">20</span>)
    
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">'X_train'</span>: X_train,
        <span class="hljs-string">'X_test'</span>: X_test,
        <span class="hljs-string">'y_train'</span>: y_train,
        <span class="hljs-string">'y_test'</span>: y_test
    }


<span class="hljs-meta">@pytest.fixture</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">temp_output_dir</span><span class="hljs-params">(tmp_path)</span>:</span>
    <span class="hljs-string">"""
    Create temporary output directory.
    """</span>
    output_dir = tmp_path / <span class="hljs-string">"outputs"</span>
    output_dir.mkdir()
    <span class="hljs-keyword">return</span> output_dir
</div></code></pre>
<h3 id="using-fixtures-in-tests">Using Fixtures in Tests</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_with_fixture</span><span class="hljs-params">(sample_data)</span>:</span>
    <span class="hljs-string">"""Test using the sample_data fixture."""</span>
    <span class="hljs-comment"># sample_data is automatically passed by pytest</span>
    <span class="hljs-keyword">assert</span> len(sample_data) == <span class="hljs-number">3</span>
    <span class="hljs-keyword">assert</span> <span class="hljs-string">'Churn'</span> <span class="hljs-keyword">in</span> sample_data.columns


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_with_multiple_fixtures</span><span class="hljs-params">(sample_data, processed_data)</span>:</span>
    <span class="hljs-string">"""Test using multiple fixtures."""</span>
    <span class="hljs-keyword">assert</span> len(sample_data) &gt; <span class="hljs-number">0</span>
    <span class="hljs-keyword">assert</span> processed_data[<span class="hljs-string">'X_train'</span>].shape[<span class="hljs-number">0</span>] == <span class="hljs-number">80</span>
</div></code></pre>
<p><strong>pytest automatically</strong>:</p>
<ol>
<li>Sees fixture name in function parameters</li>
<li>Calls the fixture function</li>
<li>Passes the return value to your test</li>
</ol>
<hr>
<h3 id="part-96-testing-preprocessing-module">Part 9.6: Testing Preprocessing Module</h3>
<p>Create comprehensive tests for <code>test_preprocess.py</code>:</p>
<pre class="hljs"><code><div><span class="hljs-string">"""
Tests for src/preprocess.py module.
"""</span>

<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> pytest

<span class="hljs-keyword">from</span> src.preprocess <span class="hljs-keyword">import</span> (
    handle_missing_values,
    drop_unnecessary_columns,
    encode_target,
    split_features_target,
    preprocess_pipeline
)


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestLoadData</span>:</span>
    <span class="hljs-string">"""Tests for load_data function."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_load_data_success</span><span class="hljs-params">(self, tmp_path)</span>:</span>
        <span class="hljs-string">"""Test successful data loading."""</span>
        <span class="hljs-keyword">from</span> src.preprocess <span class="hljs-keyword">import</span> load_data
        
        csv_file = tmp_path / <span class="hljs-string">"data.csv"</span>
        csv_file.write_text(<span class="hljs-string">"col1,col2\\nval1,val2\\n"</span>)
        
        df = load_data(str(csv_file))
        
        <span class="hljs-keyword">assert</span> isinstance(df, pd.DataFrame)
        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> df.empty
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_load_data_file_not_found</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""Test error for missing file."""</span>
        <span class="hljs-keyword">from</span> src.preprocess <span class="hljs-keyword">import</span> load_data
        
        <span class="hljs-keyword">with</span> pytest.raises(FileNotFoundError):
            load_data(<span class="hljs-string">"nonexistent.csv"</span>)


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestHandleMissingValues</span>:</span>
    <span class="hljs-string">"""Tests for handle_missing_values function."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_no_missing_values</span><span class="hljs-params">(self, sample_data)</span>:</span>
        <span class="hljs-string">"""Test with no missing values."""</span>
        df_clean = handle_missing_values(sample_data)
        
        <span class="hljs-keyword">assert</span> df_clean.isnull().sum().sum() == <span class="hljs-number">0</span>
        <span class="hljs-keyword">assert</span> len(df_clean) == len(sample_data)
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_totalcharges_conversion</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""Test TotalCharges conversion."""</span>
        df = pd.DataFrame({
            <span class="hljs-string">'TotalCharges'</span>: [<span class="hljs-string">' '</span>, <span class="hljs-string">'100'</span>, <span class="hljs-string">'200'</span>]
        })
        
        df_clean = handle_missing_values(df)
        
        <span class="hljs-keyword">assert</span> df_clean[<span class="hljs-string">'TotalCharges'</span>].dtype <span class="hljs-keyword">in</span> [np.float64, np.float32]
        <span class="hljs-keyword">assert</span> df_clean[<span class="hljs-string">'TotalCharges'</span>].iloc[<span class="hljs-number">0</span>] == <span class="hljs-number">0.0</span>


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestDropUnnecessaryColumns</span>:</span>
    <span class="hljs-string">"""Tests for drop_unnecessary_columns function."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_drop_single_column</span><span class="hljs-params">(self, sample_data)</span>:</span>
        <span class="hljs-string">"""Test dropping single column."""</span>
        df_clean = drop_unnecessary_columns(sample_data, columns=[<span class="hljs-string">'customerID'</span>])
        
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'customerID'</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> df_clean.columns
        <span class="hljs-keyword">assert</span> len(df_clean.columns) == len(sample_data.columns) - <span class="hljs-number">1</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_drop_nonexistent_column</span><span class="hljs-params">(self, sample_data)</span>:</span>
        <span class="hljs-string">"""Test dropping column that doesn't exist."""</span>
        df_clean = drop_unnecessary_columns(sample_data, columns=[<span class="hljs-string">'fake_col'</span>])
        
        <span class="hljs-comment"># Should not raise error</span>
        <span class="hljs-keyword">assert</span> len(df_clean.columns) == len(sample_data.columns)


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestEncodeTarget</span>:</span>
    <span class="hljs-string">"""Tests for encode_target function."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_encode_target_success</span><span class="hljs-params">(self, sample_data)</span>:</span>
        <span class="hljs-string">"""Test target encoding."""</span>
        df_encoded, encoder = encode_target(sample_data, target_col=<span class="hljs-string">'Churn'</span>)
        
        <span class="hljs-keyword">assert</span> encoder <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
        <span class="hljs-keyword">assert</span> df_encoded[<span class="hljs-string">'Churn'</span>].dtype <span class="hljs-keyword">in</span> [np.int64, np.int32]
        <span class="hljs-keyword">assert</span> set(df_encoded[<span class="hljs-string">'Churn'</span>].unique()).issubset({<span class="hljs-number">0</span>, <span class="hljs-number">1</span>})


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestSplitFeaturesTarget</span>:</span>
    <span class="hljs-string">"""Tests for split_features_target function."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_split_success</span><span class="hljs-params">(self, sample_data)</span>:</span>
        <span class="hljs-string">"""Test splitting features and target."""</span>
        X, y = split_features_target(sample_data, target_col=<span class="hljs-string">'Churn'</span>)
        
        <span class="hljs-keyword">assert</span> isinstance(X, pd.DataFrame)
        <span class="hljs-keyword">assert</span> isinstance(y, pd.Series)
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'Churn'</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> X.columns
        <span class="hljs-keyword">assert</span> len(X) == len(y)
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_split_missing_target</span><span class="hljs-params">(self, sample_data)</span>:</span>
        <span class="hljs-string">"""Test error when target is missing."""</span>
        <span class="hljs-keyword">with</span> pytest.raises(ValueError, match=<span class="hljs-string">"not found"</span>):
            split_features_target(sample_data, target_col=<span class="hljs-string">'nonexistent'</span>)


<span class="hljs-meta">@pytest.mark.integration</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestPreprocessingPipeline</span>:</span>
    <span class="hljs-string">"""Integration test for complete preprocessing."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_full_pipeline</span><span class="hljs-params">(self, tmp_path)</span>:</span>
        <span class="hljs-string">"""Test complete preprocessing pipeline."""</span>
        <span class="hljs-comment"># Create test CSV</span>
        csv_file = tmp_path / <span class="hljs-string">"test.csv"</span>
        data = pd.DataFrame({
            <span class="hljs-string">'customerID'</span>: [<span class="hljs-string">'C1'</span>, <span class="hljs-string">'C2'</span>, <span class="hljs-string">'C3'</span>, <span class="hljs-string">'C4'</span>, <span class="hljs-string">'C5'</span>],
            <span class="hljs-string">'gender'</span>: [<span class="hljs-string">'Male'</span>, <span class="hljs-string">'Female'</span>, <span class="hljs-string">'Male'</span>, <span class="hljs-string">'Female'</span>, <span class="hljs-string">'Male'</span>],
            <span class="hljs-string">'tenure'</span>: [<span class="hljs-number">12</span>, <span class="hljs-number">24</span>, <span class="hljs-number">36</span>, <span class="hljs-number">6</span>, <span class="hljs-number">48</span>],
            <span class="hljs-string">'MonthlyCharges'</span>: [<span class="hljs-number">50</span>, <span class="hljs-number">70</span>, <span class="hljs-number">25</span>, <span class="hljs-number">45</span>, <span class="hljs-number">95</span>],
            <span class="hljs-string">'Churn'</span>: [<span class="hljs-string">'No'</span>, <span class="hljs-string">'No'</span>, <span class="hljs-string">'Yes'</span>, <span class="hljs-string">'No'</span>, <span class="hljs-string">'Yes'</span>]
        })
        data.to_csv(csv_file, index=<span class="hljs-literal">False</span>)
        
        <span class="hljs-comment"># Run pipeline</span>
        result = preprocess_pipeline(str(csv_file), scale=<span class="hljs-literal">True</span>, use_sklearn_pipeline=<span class="hljs-literal">True</span>)
        X_train, X_test, y_train, y_test, pipeline, encoder = result
        
        <span class="hljs-comment"># Verify results</span>
        <span class="hljs-keyword">assert</span> X_train.shape[<span class="hljs-number">0</span>] &gt; <span class="hljs-number">0</span>
        <span class="hljs-keyword">assert</span> X_test.shape[<span class="hljs-number">0</span>] &gt; <span class="hljs-number">0</span>
        <span class="hljs-keyword">assert</span> pipeline <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
        <span class="hljs-keyword">assert</span> encoder <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
</div></code></pre>
<h4 id="run-preprocessing-tests">Run Preprocessing Tests</h4>
<pre class="hljs"><code><div>pytest tests/test_preprocess.py -v
</div></code></pre>
<hr>
<h3 id="part-97-testing-training-module">Part 9.7: Testing Training Module</h3>
<p>Create <code>tests/test_train.py</code>:</p>
<pre class="hljs"><code><div><span class="hljs-string">"""
Tests for src/train.py module.
"""</span>

<span class="hljs-keyword">import</span> pytest
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier

<span class="hljs-keyword">from</span> src.train <span class="hljs-keyword">import</span> train_random_forest, save_model, evaluate_model


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestTrainRandomForest</span>:</span>
    <span class="hljs-string">"""Tests for train_random_forest function."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_train_with_default_params</span><span class="hljs-params">(self, processed_data)</span>:</span>
        <span class="hljs-string">"""Test training with default parameters."""</span>
        X_train = processed_data[<span class="hljs-string">'X_train'</span>]
        y_train = processed_data[<span class="hljs-string">'y_train'</span>]
        
        model = train_random_forest(X_train, y_train, tune_hyperparameters=<span class="hljs-literal">False</span>)
        
        <span class="hljs-keyword">assert</span> isinstance(model, RandomForestClassifier)
        <span class="hljs-keyword">assert</span> hasattr(model, <span class="hljs-string">'estimators_'</span>)
        <span class="hljs-keyword">assert</span> model.random_state == <span class="hljs-number">42</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_train_can_predict</span><span class="hljs-params">(self, processed_data)</span>:</span>
        <span class="hljs-string">"""Test that trained model can make predictions."""</span>
        X_train = processed_data[<span class="hljs-string">'X_train'</span>]
        y_train = processed_data[<span class="hljs-string">'y_train'</span>]
        X_test = processed_data[<span class="hljs-string">'X_test'</span>]
        
        model = train_random_forest(X_train, y_train, tune_hyperparameters=<span class="hljs-literal">False</span>)
        predictions = model.predict(X_test)
        
        <span class="hljs-keyword">assert</span> len(predictions) == len(X_test)


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestSaveModel</span>:</span>
    <span class="hljs-string">"""Tests for save_model function."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_save_model_creates_file</span><span class="hljs-params">(self, trained_model, tmp_path)</span>:</span>
        <span class="hljs-string">"""Test that save_model creates a file."""</span>
        model_path = save_model(trained_model, <span class="hljs-string">'test_model'</span>, str(tmp_path))
        
        <span class="hljs-keyword">assert</span> Path(model_path).exists()
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'.pkl'</span> <span class="hljs-keyword">in</span> model_path
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_saved_model_can_be_loaded</span><span class="hljs-params">(self, trained_model, tmp_path)</span>:</span>
        <span class="hljs-string">"""Test that saved model can be loaded."""</span>
        <span class="hljs-keyword">import</span> joblib
        
        model_path = save_model(trained_model, <span class="hljs-string">'test_model'</span>, str(tmp_path))
        loaded_model = joblib.load(model_path)
        
        <span class="hljs-keyword">assert</span> hasattr(loaded_model, <span class="hljs-string">'predict'</span>)


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestEvaluateModel</span>:</span>
    <span class="hljs-string">"""Tests for evaluate_model function."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_evaluate_returns_metrics</span><span class="hljs-params">(self, trained_model, processed_data)</span>:</span>
        <span class="hljs-string">"""Test that evaluate returns all metrics."""</span>
        X_test = processed_data[<span class="hljs-string">'X_test'</span>]
        y_test = processed_data[<span class="hljs-string">'y_test'</span>]
        
        metrics = evaluate_model(trained_model, X_test, y_test)
        
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'accuracy'</span> <span class="hljs-keyword">in</span> metrics
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'precision'</span> <span class="hljs-keyword">in</span> metrics
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'recall'</span> <span class="hljs-keyword">in</span> metrics
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'f1_score'</span> <span class="hljs-keyword">in</span> metrics
        <span class="hljs-keyword">assert</span> <span class="hljs-number">0.0</span> &lt;= metrics[<span class="hljs-string">'accuracy'</span>] &lt;= <span class="hljs-number">1.0</span>


<span class="hljs-meta">@pytest.mark.parametrize("model_type", [</span>
    <span class="hljs-string">'logistic_regression'</span>,
    <span class="hljs-string">'random_forest'</span>,
    <span class="hljs-string">'decision_tree'</span>
])
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_train_different_models</span><span class="hljs-params">(model_type, processed_data)</span>:</span>
    <span class="hljs-string">"""Parametrized test for different model types."""</span>
    <span class="hljs-keyword">from</span> src.train <span class="hljs-keyword">import</span> (
        train_logistic_regression,
        train_random_forest,
        train_decision_tree
    )
    
    X_train = processed_data[<span class="hljs-string">'X_train'</span>]
    y_train = processed_data[<span class="hljs-string">'y_train'</span>]
    
    train_functions = {
        <span class="hljs-string">'logistic_regression'</span>: train_logistic_regression,
        <span class="hljs-string">'random_forest'</span>: train_random_forest,
        <span class="hljs-string">'decision_tree'</span>: train_decision_tree
    }
    
    train_func = train_functions[model_type]
    model = train_func(X_train, y_train, tune_hyperparameters=<span class="hljs-literal">False</span>)
    
    <span class="hljs-keyword">assert</span> hasattr(model, <span class="hljs-string">'predict'</span>)
</div></code></pre>
<hr>
<h3 id="part-98-testing-prediction-module">Part 9.8: Testing Prediction Module</h3>
<p>Create <code>tests/test_predict.py</code>:</p>
<pre class="hljs"><code><div><span class="hljs-string">"""
Tests for src/predict.py module.
"""</span>

<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pytest

<span class="hljs-keyword">from</span> src.predict <span class="hljs-keyword">import</span> load_model, predict, ModelPredictor


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestLoadModel</span>:</span>
    <span class="hljs-string">"""Tests for load_model function."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_load_model_success</span><span class="hljs-params">(self, saved_model_file)</span>:</span>
        <span class="hljs-string">"""Test successful model loading."""</span>
        model = load_model(str(saved_model_file))
        
        <span class="hljs-keyword">assert</span> model <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
        <span class="hljs-keyword">assert</span> hasattr(model, <span class="hljs-string">'predict'</span>)
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_load_model_file_not_found</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""Test error for missing model file."""</span>
        <span class="hljs-keyword">with</span> pytest.raises(FileNotFoundError):
            load_model(<span class="hljs-string">"nonexistent_model.pkl"</span>)


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestPredict</span>:</span>
    <span class="hljs-string">"""Tests for predict function."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_predict_returns_array</span><span class="hljs-params">(self, trained_model, processed_data)</span>:</span>
        <span class="hljs-string">"""Test that predict returns numpy array."""</span>
        X_test = processed_data[<span class="hljs-string">'X_test'</span>]
        
        predictions = predict(trained_model, X_test)
        
        <span class="hljs-keyword">assert</span> isinstance(predictions, np.ndarray)
        <span class="hljs-keyword">assert</span> len(predictions) == len(X_test)
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_predict_empty_input_raises_error</span><span class="hljs-params">(self, trained_model)</span>:</span>
        <span class="hljs-string">"""Test that empty input raises ValueError."""</span>
        X_empty = np.array([])
        
        <span class="hljs-keyword">with</span> pytest.raises(ValueError, match=<span class="hljs-string">"empty"</span>):
            predict(trained_model, X_empty)


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestModelPredictor</span>:</span>
    <span class="hljs-string">"""Tests for ModelPredictor class."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_init</span><span class="hljs-params">(self, saved_model_file)</span>:</span>
        <span class="hljs-string">"""Test ModelPredictor initialization."""</span>
        predictor = ModelPredictor(str(saved_model_file))
        
        <span class="hljs-keyword">assert</span> predictor.model <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_predict</span><span class="hljs-params">(self, saved_model_file, processed_data)</span>:</span>
        <span class="hljs-string">"""Test prediction using ModelPredictor."""</span>
        predictor = ModelPredictor(str(saved_model_file))
        X_test = processed_data[<span class="hljs-string">'X_test'</span>]
        
        predictions = predictor.predict(X_test)
        
        <span class="hljs-keyword">assert</span> len(predictions) == len(X_test)
</div></code></pre>
<hr>
<h3 id="part-99-testing-evaluation-module">Part 9.9: Testing Evaluation Module</h3>
<p>Create <code>tests/test_evaluate.py</code>:</p>
<pre class="hljs"><code><div><span class="hljs-string">"""
Tests for src/evaluate.py module.
Note: Uses 'y' for true labels and 'yhat' for predictions.
"""</span>

<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pytest

<span class="hljs-keyword">from</span> src.evaluate <span class="hljs-keyword">import</span> calculate_accuracy, calculate_metrics, evaluate_model


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestCalculateAccuracy</span>:</span>
    <span class="hljs-string">"""Tests for calculate_accuracy function."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_perfect_accuracy</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""Test with perfect predictions."""</span>
        y = np.array([<span class="hljs-string">'Yes'</span>, <span class="hljs-string">'No'</span>, <span class="hljs-string">'Yes'</span>, <span class="hljs-string">'No'</span>])
        yhat = np.array([<span class="hljs-string">'Yes'</span>, <span class="hljs-string">'No'</span>, <span class="hljs-string">'Yes'</span>, <span class="hljs-string">'No'</span>])
        
        accuracy = calculate_accuracy(y, yhat)
        
        <span class="hljs-keyword">assert</span> accuracy == <span class="hljs-number">1.0</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_half_accuracy</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""Test with 50% accuracy."""</span>
        y = np.array([<span class="hljs-string">'Yes'</span>, <span class="hljs-string">'No'</span>, <span class="hljs-string">'Yes'</span>, <span class="hljs-string">'No'</span>])
        yhat = np.array([<span class="hljs-string">'Yes'</span>, <span class="hljs-string">'No'</span>, <span class="hljs-string">'No'</span>, <span class="hljs-string">'Yes'</span>])
        
        accuracy = calculate_accuracy(y, yhat)
        
        <span class="hljs-keyword">assert</span> accuracy == <span class="hljs-number">0.5</span>


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestCalculateMetrics</span>:</span>
    <span class="hljs-string">"""Tests for calculate_metrics function."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_all_metrics_present</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""Test that all metrics are calculated."""</span>
        y = np.array([<span class="hljs-string">'Yes'</span>, <span class="hljs-string">'No'</span>, <span class="hljs-string">'Yes'</span>, <span class="hljs-string">'No'</span>] * <span class="hljs-number">10</span>)
        yhat = np.array([<span class="hljs-string">'Yes'</span>, <span class="hljs-string">'No'</span>, <span class="hljs-string">'Yes'</span>, <span class="hljs-string">'Yes'</span>] * <span class="hljs-number">10</span>)
        
        metrics = calculate_metrics(y, yhat)
        
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'accuracy'</span> <span class="hljs-keyword">in</span> metrics
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'precision'</span> <span class="hljs-keyword">in</span> metrics
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'recall'</span> <span class="hljs-keyword">in</span> metrics
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'f1_score'</span> <span class="hljs-keyword">in</span> metrics
        <span class="hljs-keyword">assert</span> all(<span class="hljs-number">0.0</span> &lt;= v &lt;= <span class="hljs-number">1.0</span> <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> metrics.values())


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestEvaluateModel</span>:</span>
    <span class="hljs-string">"""Tests for evaluate_model function."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_complete_evaluation</span><span class="hljs-params">(self, trained_model, processed_data)</span>:</span>
        <span class="hljs-string">"""Test complete model evaluation."""</span>
        X_test = processed_data[<span class="hljs-string">'X_test'</span>]
        y_test = processed_data[<span class="hljs-string">'y_test'</span>]
        
        results = evaluate_model(trained_model, X_test, y_test)
        
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'accuracy'</span> <span class="hljs-keyword">in</span> results
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'confusion_matrix'</span> <span class="hljs-keyword">in</span> results
        <span class="hljs-keyword">assert</span> <span class="hljs-string">'classification_report'</span> <span class="hljs-keyword">in</span> results
</div></code></pre>
<hr>
<h3 id="part-910-integration-tests">Part 9.10: Integration Tests</h3>
<p>Create <code>tests/test_integration.py</code>:</p>
<pre class="hljs"><code><div><span class="hljs-string">"""
Integration tests for complete ML workflows.
"""</span>

<span class="hljs-keyword">import</span> pytest
<span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path

<span class="hljs-keyword">from</span> src.preprocess <span class="hljs-keyword">import</span> preprocess_pipeline
<span class="hljs-keyword">from</span> src.train <span class="hljs-keyword">import</span> train_random_forest, save_model
<span class="hljs-keyword">from</span> src.predict <span class="hljs-keyword">import</span> ModelPredictor, predict
<span class="hljs-keyword">from</span> src.evaluate <span class="hljs-keyword">import</span> evaluate_model


<span class="hljs-meta">@pytest.mark.integration</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestCompleteMLPipeline</span>:</span>
    <span class="hljs-string">"""Tests for complete ML pipeline."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_csv_to_predictions</span><span class="hljs-params">(self, sample_csv_file_large, tmp_path)</span>:</span>
        <span class="hljs-string">"""Test complete workflow: CSV → preprocessing → training → prediction."""</span>
        <span class="hljs-comment"># Preprocess</span>
        result = preprocess_pipeline(
            str(sample_csv_file_large),
            scale=<span class="hljs-literal">True</span>,
            use_sklearn_pipeline=<span class="hljs-literal">True</span>
        )
        X_train, X_test, y_train, y_test, pipeline, encoder = result
        
        <span class="hljs-comment"># Train</span>
        model = train_random_forest(X_train, y_train, tune_hyperparameters=<span class="hljs-literal">False</span>)
        
        <span class="hljs-comment"># Predict</span>
        predictions = predict(model, X_test)
        
        <span class="hljs-comment"># Evaluate</span>
        results = evaluate_model(model, X_test, y_test)
        
        <span class="hljs-comment"># Verify</span>
        <span class="hljs-keyword">assert</span> len(predictions) == len(X_test)
        <span class="hljs-keyword">assert</span> <span class="hljs-number">0.0</span> &lt;= results[<span class="hljs-string">'accuracy'</span>] &lt;= <span class="hljs-number">1.0</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_save_load_workflow</span><span class="hljs-params">(self, sample_csv_file_large, tmp_path)</span>:</span>
        <span class="hljs-string">"""Test saving and loading all components."""</span>
        <span class="hljs-comment"># Preprocess</span>
        X_train, X_test, y_train, y_test, pipeline, encoder = preprocess_pipeline(
            str(sample_csv_file_large),
            scale=<span class="hljs-literal">True</span>,
            use_sklearn_pipeline=<span class="hljs-literal">True</span>
        )
        
        <span class="hljs-comment"># Train and save</span>
        model = train_random_forest(X_train, y_train, tune_hyperparameters=<span class="hljs-literal">False</span>)
        model_path = save_model(model, <span class="hljs-string">'test_model'</span>, str(tmp_path))
        
        <span class="hljs-comment"># Load and predict</span>
        predictor = ModelPredictor(model_path)
        predictions = predictor.predict(X_test)
        
        <span class="hljs-keyword">assert</span> len(predictions) == len(X_test)
</div></code></pre>
<hr>
<h3 id="part-911-running-tests-and-code-coverage">Part 9.11: Running Tests and Code Coverage</h3>
<h4 id="run-all-tests">Run All Tests</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Run all tests</span>
pytest

<span class="hljs-comment"># Run with verbose output</span>
pytest -v

<span class="hljs-comment"># Run specific test file</span>
pytest tests/test_preprocess.py

<span class="hljs-comment"># Run specific test</span>
pytest tests/test_preprocess.py::test_load_data_success

<span class="hljs-comment"># Run tests by marker</span>
pytest -m unit
pytest -m integration
</div></code></pre>
<h4 id="measure-code-coverage">Measure Code Coverage</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Run tests with coverage</span>
pytest --cov=src --cov-report=html --cov-report=term

<span class="hljs-comment"># Open coverage report in browser</span>
open htmlcov/index.html  <span class="hljs-comment"># Mac</span>
xdg-open htmlcov/index.html  <span class="hljs-comment"># Linux</span>
start htmlcov/index.html  <span class="hljs-comment"># Windows</span>
</div></code></pre>
<p><strong>Expected output</strong>:</p>
<pre class="hljs"><code><div>----------- coverage: platform linux, python 3.12.12-final-0 -----------
Name                       Stmts   Miss  Cover
----------------------------------------------
src/__init__.py                1      0   100%
src/evaluate.py              150     15    90%
src/predict.py               120     10    92%
src/preprocess.py            200     20    90%
src/train.py                 250     25    90%
src/utils/config.py           10      0   100%
----------------------------------------------
TOTAL                        731     70    90%
</div></code></pre>
<p><strong>Coverage targets</strong>:</p>
<ul>
<li><strong>80%+</strong>: Acceptable</li>
<li><strong>90%+</strong>: Good</li>
<li><strong>95%+</strong>: Excellent</li>
<li><strong>100%</strong>: Overkill (don't aim for this)</li>
</ul>
<h4 id="understanding-coverage-report">Understanding Coverage Report</h4>
<p>The HTML report shows:</p>
<ul>
<li><strong>Green lines</strong>: Covered by tests</li>
<li><strong>Red lines</strong>: Not covered</li>
<li><strong>Yellow lines</strong>: Partially covered</li>
</ul>
<p>Focus on covering:</p>
<ul>
<li>✅ Critical business logic</li>
<li>✅ Error handling paths</li>
<li>✅ Edge cases</li>
</ul>
<p>Don't worry about:</p>
<ul>
<li>❌ Simple getters/setters</li>
<li>❌ <code>__init__</code> methods with just assignments</li>
<li>❌ Logging statements</li>
</ul>
<hr>
<h3 id="part-912-best-practices-for-ml-testing">Part 9.12: Best Practices for ML Testing</h3>
<h4 id="1-use-small-fast-test-data">1. Use Small, Fast Test Data</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># ❌ Bad: Using full dataset</span>
<span class="hljs-meta">@pytest.fixture</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_data</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-keyword">return</span> pd.read_csv(<span class="hljs-string">'data/raw/full_dataset.csv'</span>)  <span class="hljs-comment"># 1GB file!</span>

<span class="hljs-comment"># ✅ Good: Using small synthetic data</span>
<span class="hljs-meta">@pytest.fixture</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_data</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-keyword">return</span> pd.DataFrame({
        <span class="hljs-string">'feature1'</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>],
        <span class="hljs-string">'feature2'</span>: [<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>, <span class="hljs-string">'e'</span>],
        <span class="hljs-string">'target'</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]
    })
</div></code></pre>
<h4 id="2-mock-external-dependencies">2. Mock External Dependencies</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Mock MLflow to test without actually logging</span>
<span class="hljs-meta">@patch('mlflow.log_param')</span>
<span class="hljs-meta">@patch('mlflow.log_metric')</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_train_with_mlflow</span><span class="hljs-params">(mock_metric, mock_param)</span>:</span>
    model = train_model(X, y)
    
    <span class="hljs-keyword">assert</span> mock_param.called
    <span class="hljs-keyword">assert</span> mock_metric.called
</div></code></pre>
<h4 id="3-test-edge-cases">3. Test Edge Cases</h4>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_empty_dataframe</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-string">"""Test with empty input."""</span>
    df = pd.DataFrame()
    
    <span class="hljs-keyword">with</span> pytest.raises(ValueError, match=<span class="hljs-string">"empty"</span>):
        preprocess(df)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_single_sample</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-string">"""Test with single sample."""</span>
    X = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])
    predictions = model.predict(X)
    
    <span class="hljs-keyword">assert</span> len(predictions) == <span class="hljs-number">1</span>
</div></code></pre>
<h4 id="4-use-parametrized-tests">4. Use Parametrized Tests</h4>
<pre class="hljs"><code><div><span class="hljs-meta">@pytest.mark.parametrize("input_val,expected", [</span>
    (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>),
    (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),
    (<span class="hljs-number">5</span>, <span class="hljs-number">25</span>),
    (<span class="hljs-number">-2</span>, <span class="hljs-number">4</span>)
])
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_square</span><span class="hljs-params">(input_val, expected)</span>:</span>
    <span class="hljs-keyword">assert</span> square(input_val) == expected
</div></code></pre>
<h4 id="5-organize-tests-into-classes">5. Organize Tests into Classes</h4>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestDataLoading</span>:</span>
    <span class="hljs-string">"""All tests for data loading."""</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_load_csv</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">pass</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_load_excel</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">pass</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_load_json</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">pass</span>
</div></code></pre>
<h4 id="6-dont-test-implementation-details">6. Don't Test Implementation Details</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># ❌ Bad: Testing internal variables</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_internal_state</span><span class="hljs-params">()</span>:</span>
    model = Model()
    <span class="hljs-keyword">assert</span> model._internal_counter == <span class="hljs-number">0</span>  <span class="hljs-comment"># Don't test this</span>

<span class="hljs-comment"># ✅ Good: Testing behavior</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_prediction_output</span><span class="hljs-params">()</span>:</span>
    model = Model()
    result = model.predict(X)
    <span class="hljs-keyword">assert</span> len(result) == len(X)  <span class="hljs-comment"># Test this</span>
</div></code></pre>
<hr>
<h3 id="part-913-continuous-integration-preview">Part 9.13: Continuous Integration (Preview)</h3>
<h3 id="what-is-ci">What is CI?</h3>
<p><strong>Continuous Integration (CI)</strong> automatically runs your tests every time you push code to GitHub.</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>✅ Catch bugs before merging</li>
<li>✅ Ensure all contributors run tests</li>
<li>✅ Maintain code quality</li>
<li>✅ Prevent broken code in main branch</li>
</ul>
<h3 id="preview-github-actions">Preview: GitHub Actions</h3>
<p>In Lab 06, you'll set up CI with GitHub Actions. Here's a preview:</p>
<p><code>.github/workflows/tests.yml</code>:</p>
<pre class="hljs"><code><div><span class="hljs-attr">name:</span> <span class="hljs-string">Tests</span>

<span class="hljs-attr">on:</span> <span class="hljs-string">[push,</span> <span class="hljs-string">pull_request]</span>

<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">test:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    
    <span class="hljs-attr">steps:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v2</span>
    
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Set</span> <span class="hljs-string">up</span> <span class="hljs-string">Python</span>
      <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/setup-python@v2</span>
      <span class="hljs-attr">with:</span>
        <span class="hljs-attr">python-version:</span> <span class="hljs-string">'3.12'</span>
    
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">dependencies</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">|
        pip install -r requirements.txt
</span>    
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">tests</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">|
        pytest --cov=src --cov-report=term
</span></div></code></pre>
<p><strong>This will</strong>:</p>
<ul>
<li>Run tests on every push</li>
<li>Show test results in GitHub</li>
<li>Block merges if tests fail</li>
</ul>
<hr>
<h3 id="part-914-troubleshooting-common-issues">Part 9.14: Troubleshooting Common Issues</h3>
<h4 id="issue-tests-not-discovered">Issue: Tests not discovered</h4>
<p><strong>Problem</strong>: <code>pytest</code> finds no tests</p>
<p><strong>Solution</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Check test discovery</span>
pytest --collect-only

<span class="hljs-comment"># Ensure files start with 'test_'</span>
mv my_tests.py test_my_module.py

<span class="hljs-comment"># Ensure functions start with 'test_'</span>
def test_my_function():  <span class="hljs-comment"># Good</span>
def my_test():  <span class="hljs-comment"># Bad - won't be discovered</span>
</div></code></pre>
<h4 id="issue-import-errors">Issue: Import errors</h4>
<p><strong>Problem</strong>: <code>ModuleNotFoundError: No module named 'src'</code></p>
<p><strong>Solution</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Run pytest from project root</span>
<span class="hljs-built_in">cd</span> /path/to/project
pytest

<span class="hljs-comment"># Or install your package in editable mode</span>
pip install -e .
</div></code></pre>
<h4 id="issue-fixture-not-found">Issue: Fixture not found</h4>
<p><strong>Problem</strong>: <code>fixture 'sample_data' not found</code></p>
<p><strong>Solution</strong>:</p>
<ul>
<li>Ensure fixture is in <code>conftest.py</code></li>
<li>Check fixture name matches parameter name</li>
<li>Verify <code>conftest.py</code> is in tests directory</li>
</ul>
<h4 id="issue-tests-pass-locally-but-fail-in-ci">Issue: Tests pass locally but fail in CI</h4>
<p><strong>Problem</strong>: Different behavior in CI</p>
<p><strong>Common causes</strong>:</p>
<ul>
<li>Different Python version</li>
<li>Different package versions</li>
<li>Different random seeds</li>
<li>Timezone differences</li>
</ul>
<p><strong>Solution</strong>: Use same Python version and pin package versions</p>
<hr>
<h3 id="part-915-summary-and-next-steps">Part 9.15: Summary and Next Steps</h3>
<h4 id="what-youve-accomplished">What You've Accomplished</h4>
<p>Congratulations! You've now completed Lab 02 in its entirety. Your project now has:</p>
<p>✅ <strong>Virtual environments</strong> for reproducibility
✅ <strong>CLI interfaces</strong> for easy usage
✅ <strong>Hyperparameter tuning</strong> for better models
✅ <strong>YAML configuration</strong> for flexibility
✅ <strong>DVC</strong> for data version control
✅ <strong>MLflow</strong> for experiment tracking
✅ <strong>Comprehensive test suite</strong> with pytest</p>
<p><strong>Your test suite includes</strong>:</p>
<ul>
<li>~2,700 lines of test code</li>
<li>Unit tests for all modules</li>
<li>Integration tests for complete workflows</li>
<li>Test fixtures for reusable data</li>
<li>Code coverage measurement</li>
<li>Organized test structure</li>
</ul>
<h4 id="quality-metrics">Quality Metrics</h4>
<p>Your project should now achieve:</p>
<ul>
<li><strong>Code coverage</strong>: &gt;80%</li>
<li><strong>Tests</strong>: 50+ test functions</li>
<li><strong>Test files</strong>: 6 comprehensive test modules</li>
<li><strong>Documentation</strong>: Tests serve as examples</li>
</ul>
<h4 id="commands-reference">Commands Reference</h4>
<pre class="hljs"><code><div><span class="hljs-comment"># Run all tests</span>
pytest

<span class="hljs-comment"># Run with coverage</span>
pytest --cov=src --cov-report=html --cov-report=term

<span class="hljs-comment"># Run specific tests</span>
pytest tests/test_preprocess.py
pytest tests/test_train.py -v
pytest -m integration

<span class="hljs-comment"># Run tests matching pattern</span>
pytest -k <span class="hljs-string">"test_load"</span>

<span class="hljs-comment"># Show slowest tests</span>
pytest --durations=10
</div></code></pre>
<h4 id="project-status-lab-02-complete-%F0%9F%8E%89">Project Status: Lab 02 Complete! 🎉</h4>
<p><strong>Version</strong>: 2.3.0 (with Testing)</p>
<p><strong>Next Labs</strong>:</p>
<ul>
<li><strong>Lab 03</strong>: REST API Development (Flask/FastAPI)</li>
<li><strong>Lab 04</strong>: Containerization (Docker)</li>
<li><strong>Lab 05</strong>: Cloud Deployment (AWS/GCP/Azure)</li>
<li><strong>Lab 06</strong>: CI/CD &amp; Monitoring (GitHub Actions)</li>
</ul>
<h4 id="commit-your-work">Commit Your Work</h4>
<pre class="hljs"><code><div>git add tests/ pytest.ini
git commit -m <span class="hljs-string">"feat: Add comprehensive pytest test suite (Lab 02 Part 9)

**Testing Infrastructure**:
- Add pytest and pytest-cov to requirements.txt
- Create tests/ directory with fixtures
- Configure pytest with pytest.ini

**Test Coverage**:
- test_preprocess.py: Data loading, pipelines, encoding
- test_train.py: Model training, MLflow integration
- test_predict.py: Predictions, file I/O
- test_evaluate.py: Metrics calculation
- test_integration.py: End-to-end workflows

**Documentation**:
- Add Lab 02 Part 9 testing tutorial
- Update README with testing section

**Results**:
- Coverage: &gt;80%
- All tests pass
- Ready for CI/CD (Lab 06)

Lab 02 Complete: Structure + DVC + MLflow + Testing
Version: 2.3.0"</span>

git push
</div></code></pre>
<hr>
<h3 id="additional-resources">Additional Resources</h3>
<h4 id="testing-books">Testing Books</h4>
<ul>
<li>&quot;Test-Driven Development with Python&quot; by Harry Percival</li>
<li>&quot;Python Testing with pytest&quot; by Brian Okken</li>
</ul>
<h4 id="online-resources">Online Resources</h4>
<ul>
<li>pytest Documentation: <a href="https://docs.pytest.org/">https://docs.pytest.org/</a></li>
<li>Real Python pytest Tutorial: <a href="https://realpython.com/pytest-python-testing/">https://realpython.com/pytest-python-testing/</a></li>
<li>Effective Python Testing: <a href="https://effectivepython.com/">https://effectivepython.com/</a></li>
</ul>
<h4 id="testing-tools">Testing Tools</h4>
<ul>
<li><strong>pytest</strong>: Main testing framework</li>
<li><strong>pytest-cov</strong>: Coverage measurement</li>
<li><strong>pytest-mock</strong>: Mocking utilities</li>
<li><strong>hypothesis</strong>: Property-based testing</li>
<li><strong>tox</strong>: Testing across multiple environments</li>
</ul>
<h2 id="part-10-submission">Part 10: Submission</h2>
<h3 id="what-to-submit">What to Submit</h3>
<p>For this lab assignment, you must <strong>submit a single zip file</strong> containing the required screenshots to the course Moodle page. Additionally, ensure you complete the Git commit and DVC access requirements directly in their respective platforms (GitHub and DagsHub/Google Drive).</p>
<ol>
<li>
<p><strong>Git Commit</strong>:</p>
<ul>
<li>Make one or more <strong>meaningful commits</strong> to your GitHub Classroom repository that include all the code changes implementing the requirements of this lab (CLI interfaces, DVC integration, MLflow integration, updated <code>requirements.txt</code>, etc., tests are optional). Your instructor will review your repository directly.</li>
</ul>
</li>
<li>
<p><strong>DVC Remote Storage Access</strong>:</p>
<ul>
<li><strong>If using DagsHub (Recommended)</strong>: Add your instructor as a collaborator to your DagsHub repository. Your instructor's DagsHub username is <code>ajallooe</code>.</li>
<li><strong>If using Google Drive</strong>: Share the Google Drive folder you configured as your DVC remote with your instructor (provide their email address if needed, or follow course instructions for sharing).</li>
<li>Ensure your data has been successfully pushed using <code>dvc push</code>.</li>
</ul>
</li>
<li>
<p><strong>Screenshots (Include in Zip File)</strong>:</p>
<ul>
<li><strong>MLflow UI - Multiple Runs</strong>: A screenshot showing the MLflow UI with your experiment selected, displaying the table view with at least <strong>2-3 distinct runs</strong> resulting from different parameters or configurations.</li>
<li><strong>MLflow UI - Single Run Details</strong>: A screenshot showing the detail page for <strong>one specific run</strong>. This screenshot must clearly display the logged <strong>Parameters</strong>, <strong>Metrics</strong>, and <strong>Artifacts</strong> (showing at least the logged model folder, e.g., &quot;model&quot;).</li>
<li><strong>DVC Push Confirmation</strong>: A screenshot of your terminal after successfully running <code>dvc push</code>. The output should clearly indicate that your data files were pushed or are already up-to-date with the remote.
<ul>
<li><em>How to get this:</em> After making sure your DVC remote is configured and you have tracked your data (<code>dvc add data/raw data/processed</code>), run <code>dvc push</code> in your terminal. Capture the output that typically looks like <code>Pushing...</code>, <code>X files pushed</code>, or confirms files are <code>Alreadyincache/Up-to-date</code>.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>Submission Summary:</strong></p>
<ul>
<li><strong>On Moodle</strong>: Submit <strong>one zip file</strong> containing the three required screenshots (MLflow multiple runs, MLflow single run details, DVC push confirmation).</li>
<li><strong>On GitHub</strong>: Ensure your latest code, including DVC (<code>.dvc</code>, <code>.gitignore</code>) and MLflow integration, is <strong>committed and pushed</strong> to your GitHub Classroom repository.</li>
<li><strong>On DagsHub/Google Drive</strong>: Ensure your DVC remote is <strong>shared with/accessible to</strong> the instructor (<code>ajallooe</code> for DagsHub).</li>
</ul>
<p><strong>Note:</strong> Grading is based on the successful implementation of CLI, DVC, MLflow, and testing as reflected in your GitHub repository, the confirmation of DVC remote setup and push, and the evidence provided in the MLflow screenshots.</p>
<hr>
<h2 id="congratulations-%F0%9F%8E%89">Congratulations! 🎉</h2>
<p>You've completed the second major step in your ML deployment journey! You've taken your structured project and elevated it with professional-grade tools:</p>
<p>✅ <strong>Data Version Control (DVC)</strong> for reproducible data management<br>
✅ <strong>Experiment Tracking (MLflow)</strong> for tracking, comparing, and managing models<br>
✅ <strong>Automated Testing (pytest)</strong> for reliable, maintainable code<br>
✅ <strong>Functional CLIs</strong> to make your scripts usable and configurable</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>✅ <strong>Git is for code, DVC is for data.</strong> Never commit large data files to Git.</li>
<li>✅ <strong>If you can't reproduce it, it's not finished.</strong> DVC ensures your data is versioned just like your code.</li>
<li>✅ <strong>Track <em>every</em> experiment.</strong> MLflow is your lab notebook, preventing &quot;magic&quot; models you can't recreate.</li>
<li>✅ <strong>If it's not tested, it's broken.</strong> Tests are your safety net, allowing you to refactor and add features without fear.</li>
<li>✅ <strong>Scripts should be configurable.</strong> CLIs (<code>argparse</code>) separate configuration from code logic.</li>
</ol>
<p>You're now ready to:</p>
<ul>
<li><strong>Lab 3</strong>: Serve your model as a <strong>REST API</strong> with Flask/FastAPI</li>
<li><strong>Lab 4</strong>: Package your API into a <strong>Docker container</strong></li>
<li><strong>Lab 5</strong>: <strong>Deploy</strong> your container to the cloud</li>
<li><strong>Lab 6</strong>: Implement <strong>CI/CD</strong> and <strong>Monitoring</strong></li>
</ul>
<p>The foundation you've built and strengthened in this lab is what separates a data science experiment from a deployable machine learning product.</p>
<hr>
<p><strong>Lab 2 Complete!</strong> 🎊</p>
<p>Your code is production-ready, maintainable, and reliable. Well done!</p>
<hr>
<p><em>Lab 2 Instructions</em><br>
<em>CMPT 2500: Machine Learning Deployment and Software Development</em><br>
<em>NorQuest College</em></p>

</body>
</html>
