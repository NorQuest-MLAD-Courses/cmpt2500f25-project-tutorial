# configs/preprocess_config.yaml

# Data configuration
data:
  input_file: data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv
  output_dir: data/processed
  target_column: Churn
  id_column: customerID

# DVC configuration
dvc:
  track_raw: true
  track_processed: true
  remote: origin  # DagsHub remote
  data_version: v1.0

# MLflow configuration
mlflow:
  track_preprocessing: false  # Set to true to log preprocessing as MLflow run
  experiment_name: telecom-churn-preprocessing
  log_artifacts: true  # Log preprocessing artifacts (pipeline, encoders)

# Missing value handling
missing_values:
  strategy: drop
  threshold: 0.5

# Feature scaling
scaling:
  method: standard
  with_mean: true
  with_std: true

# Feature columns
features:
  categorical:
    - gender
    - Partner
    - Dependents
    - PhoneService
    - MultipleLines
    - InternetService
    - OnlineSecurity
    - OnlineBackup
    - DeviceProtection
    - TechSupport
    - StreamingTV
    - StreamingMovies
    - Contract
    - PaperlessBilling
    - PaymentMethod
  
  numerical:
    - SeniorCitizen
    - tenure
    - MonthlyCharges
    - TotalCharges

# Train-test split
split:
  test_size: 0.2
  random_state: 42
  stratify: true

# Pipeline options
pipeline:
  use_sklearn_pipeline: true
  save_artifacts: true
  output_files:
    data: preprocessed_data.npy
    pipeline: preprocessing_pipeline.pkl
    label_encoder: label_encoder.pkl
# configs/train_config.yaml (Version 3: After MLflow - FINAL VERSION)

# Model configuration
model:
  type: random_forest
  random_state: 42

# Model hyperparameters
hyperparameters:
  logistic_regression:
    C: 1.0
    max_iter: 1000
    penalty: l2
    solver: lbfgs
  
  random_forest:
    n_estimators: 100
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    max_features: sqrt
  
  decision_tree:
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    criterion: gini
  
  adaboost:
    n_estimators: 50
    learning_rate: 1.0
    algorithm: SAMME.R
  
  gradient_boosting:
    n_estimators: 100
    learning_rate: 0.1
    max_depth: 3
    min_samples_split: 2
    subsample: 1.0
  
  voting_classifier:
    voting: soft
    estimators:
      - gradient_boosting
      - logistic_regression
      - adaboost

# Training configuration
training:
  tune_hyperparameters: false
  test_size: 0.2
  random_state: 42
  stratify: true

# Paths
paths:
  data:
    input: data/processed/preprocessed_data.npy  # DVC-tracked
    raw: data/raw  # DVC-tracked
    processed: data/processed  # DVC-tracked
  models:
    output_dir: models  # Local storage + MLflow artifacts

# DVC configuration
dvc:
  remote: origin  # DagsHub remote
  data_version: v1.0
  track_models: false  # Use MLflow for models

# MLflow configuration
mlflow:
  tracking_uri: null  # null = local mlruns/, or set to DagsHub URL
  experiment_name: telecom-churn-prediction
  run_name_prefix: null  # null = auto-generate, or set custom prefix
  artifact_location: null  # null = use default (mlruns/), or set custom
  log_models: true  # Log models to MLflow
  log_artifacts: true  # Log additional artifacts (plots, etc.)
  tags:
    project: telecom-churn
    team: ml-team
    environment: development
  autolog: false  # Use manual logging for control

# Hyperparameter tuning grids
tuning_grids:
  random_forest:
    n_estimators: [50, 100, 200]
    max_depth: [10, 20, 30, null]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
    max_features: [sqrt, log2]
  
  logistic_regression:
    C: [0.001, 0.01, 0.1, 1, 10, 100]
    penalty: [l1, l2]
    solver: [liblinear, saga]
  
  gradient_boosting:
    n_estimators: [50, 100, 200]
    learning_rate: [0.01, 0.1, 0.2]
    max_depth: [3, 5, 7]
    min_samples_split: [2, 5, 10]
    subsample: [0.8, 0.9, 1.0]

# Cross-validation settings
cross_validation:
  cv_folds: 5
  scoring: accuracy
  n_jobs: -1
  verbose: 1