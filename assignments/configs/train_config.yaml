# configs/train_config.yaml

# Model configuration
model:
  type: random_forest
  random_state: 42

# Model hyperparameters
hyperparameters:
  logistic_regression:
    C: 1.0
    max_iter: 1000
    penalty: l2
    solver: lbfgs
  
  random_forest:
    n_estimators: 100
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    max_features: sqrt
  
  decision_tree:
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    criterion: gini
  
  adaboost:
    n_estimators: 50
    learning_rate: 1.0
    algorithm: SAMME.R
  
  gradient_boosting:
    n_estimators: 100
    learning_rate: 0.1
    max_depth: 3
    min_samples_split: 2
    subsample: 1.0
  
  voting_classifier:
    voting: soft
    estimators:
      - gradient_boosting
      - logistic_regression
      - adaboost

# Training configuration
training:
  tune_hyperparameters: false
  test_size: 0.2
  random_state: 42
  stratify: true

# Paths (DVC-tracked data)
paths:
  data:
    input: data/processed/preprocessed_data.npy  # Tracked by DVC
    raw: data/raw  # Tracked by DVC
    processed: data/processed  # Tracked by DVC
  models:
    output_dir: models  # Can be tracked by DVC or MLflow

# DVC configuration
dvc:
  remote: origin  # DagsHub remote
  data_version: v1.0  # Tag for this data version
  track_models: false  # If true, add models to DVC; if false, use MLflow

# Hyperparameter tuning grids
tuning_grids:
  random_forest:
    n_estimators: [50, 100, 200]
    max_depth: [10, 20, 30, null]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
    max_features: [sqrt, log2]
  
  logistic_regression:
    C: [0.001, 0.01, 0.1, 1, 10, 100]
    penalty: [l1, l2]
    solver: [liblinear, saga]
  
  gradient_boosting:
    n_estimators: [50, 100, 200]
    learning_rate: [0.01, 0.1, 0.2]
    max_depth: [3, 5, 7]
    min_samples_split: [2, 5, 10]
    subsample: [0.8, 0.9, 1.0]

# Cross-validation settings
cross_validation:
  cv_folds: 5
  scoring: accuracy
  n_jobs: -1
  verbose: 1
