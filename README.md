# CMPT 2500 Project: Telecom Customer Churn Prediction

A production-ready machine learning project demonstrating industry best practices for MLOps, including modular code organization, data version control (DVC), experiment tracking (MLflow), automated testing, and a REST API for model serving.

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.7.2-orange.svg)](https://scikit-learn.org/)
[![DVC](https://img.shields.io/badge/DVC-3.63.0-945DD6.svg)](https://dvc.org/)
[![MLflow](https://img.shields.io/badge/MLflow-3.5.1-0194E2.svg)](https://mlflow.org/)
[![Flask](https://img.shields.io/badge/Flask-000000?style=flat&logo=flask&logoColor=white)](https://flask.palletsprojects.com/)
[![Swagger](https://img.shields.io/badge/Swagger-85EA2D?style=flat&logo=swagger&logoColor=black)](https://swagger.io/)

## Overview

Customer churn prediction for telecommunications companies. By predicting which customers are likely to leave, companies can take proactive retention measuresâ€”reducing costs since customer acquisition is typically 5Ã— more expensive than retention.

This project demonstrates a complete ML workflow from data preprocessing to model deployment, incorporating:

- ğŸ—ï¸ **Modular architecture** with separation of concerns
- ğŸ–¥ï¸ **CLI interfaces** for all operations
- ğŸ”§ **Hyperparameter tuning** for optimal performance
- ğŸ“ **YAML configuration** for flexible deployment
- ğŸ“¦ **Data version control (DVC)** with DagsHub remote
- ğŸ“Š **Experiment tracking (MLflow)** with comprehensive logging
- ğŸ§ª **Automated testing (pytest)** for scripts and the live API
- ğŸ **Virtual environments** for reproducibility
- ğŸ³ **REST API** for serving models (Flask & Flasgger).

---

## API Documentation

This project is served via a REST API. For complete details on installation, running, and all available endpoints, please see the **[API Documentation](./API_Documentation.md)**.

For a live, interactive API specification (Swagger UI), run the server and navigate to:
`http://127.0.0.1:5000/apidocs/`

---

## Quick Start

### 1. Install Dependencies

Clone the repository and set up your virtual environment.

```sh
git clone https://github.com/ajallooe/cmpt2500f25-project-tutorial.git
cd cmpt2500f25-project-tutorial
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. How to setup DVC

This project uses DVC to manage large data files and ML models. You must pull the data from the remote storage.

**(One-Time Setup)**: If this is a new environment (like a fresh Codespace), you must first add your DagsHub credentials:

```sh
dvc remote modify origin --local access_key_id <YOUR_DVC_ACCESS_KEY_ID>
dvc remote modify origin --local secret_access_key <YOUR_DVC_SECRET_ACCESS_KEY>
```

**Pull Data**:

```sh
dvc pull
```

This will download `preprocessing_pipeline.pkl`, `label_encoder.pkl`, and `preprocessed_data.npy` from the DagsHub remote.

### 3. How to run training

You can run the full training pipeline using the CLI. This will train all models, perform hyperparameter tuning, and track every run in MLflow.

```sh
python -m src.train --data data/processed/preprocessed_data.npy --model all --tune
```

To view the results, run the MLflow UI:

```sh
mlflow ui
```

### 4. How to run tests

This project uses `pytest` for automated testing. You can run all tests (including the new API tests) with a single command.

```sh
pytest
```

### 5. How to run the API Server (Lab 03)

To serve your trained models, run the Flask API server.

```sh
python src/app.py
```

The server will start on `http://127.0.0.1:5000`.

You can test if it's running in a new terminal:

```sh
curl http://127.0.0.1:5000/health
```

**Expected Output:**

```json
{"status":"ok"}
```

---

## Project Structure

```output
cmpt2500f25-project-tutorial/
â”œâ”€â”€ .dvc/                  # DVC metadata
â”œâ”€â”€ .github/               # GitHub Actions (CI/CD) workflows (Future)
â”œâ”€â”€ .venv/                 # Python virtual environment (Ignored)
â”œâ”€â”€ assignments/           # Lab instructions and guides
â”œâ”€â”€ configs/               # YAML configuration files
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/         # DVC-tracked processed data and pipelines
â”‚   â””â”€â”€ raw/               # DVC-tracked raw data
â”œâ”€â”€ models/                # Locally-saved models (e.g., model_v1.pkl)
â”œâ”€â”€ mlruns/                # MLflow experiment tracking data (Ignored)
â”œâ”€â”€ notebooks/             # Jupyter notebooks for exploration (EDA)
â”œâ”€â”€ src/                   # Main source code
â”‚   â”œâ”€â”€ utils/             # Utility functions and config
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ config.py      # Project constants and feature lists
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py             # Lab 03: Flask API server
â”‚   â”œâ”€â”€ evaluate.py        # CLI script for model evaluation
â”‚   â”œâ”€â”€ predict.py         # CLI script for making predictions
â”‚   â”œâ”€â”€ preprocess.py      # CLI script for data preprocessing
â”‚   â””â”€â”€ train.py           # CLI script for model training
â”œâ”€â”€ tests/                 # Automated tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py        # Pytest fixtures
â”‚   â”œâ”€â”€ test_api.py        # Lab 03: Automated tests for the Flask API
â”‚   â”œâ”€â”€ test_evaluate.py   # Unit tests for evaluate.py
â”‚   â”œâ”€â”€ test_integration.py# Integration tests for full workflows
â”‚   â”œâ”€â”€ test_predict.py    # Unit tests for predict.py
â”‚   â”œâ”€â”€ test_preprocess.py # Unit tests for preprocess.py
â”‚   â””â”€â”€ test_train.py      # Unit tests for train.py
â”œâ”€â”€ .dvcignore             # DVC ignore file
â”œâ”€â”€ .gitignore             # Git ignore file
â”œâ”€â”€ API_Documentation.md   # Lab 03: High-level API manual
â”œâ”€â”€ Makefile               # (Optional) Helper commands
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ pytest.ini             # Pytest configuration
â””â”€â”€ requirements.txt       # Project dependencies
```

---

## Project Status

### âœ… Completed (Lab 01)

- Modular project structure
- OOP principles applied
- Initial scripts (`preprocess.py`, `train.py`, etc.)
- `requirements.txt` and `.gitignore`

### âœ… Completed (Lab 02)

- Virtual environment setup
- Dependency management (`pip install -r requirements.txt`)
- CLI interfaces (`argparse`)
- Hyperparameter tuning (`GridSearchCV`)
- Scikit-learn pipelines (`pipeline.pkl`)
- YAML configuration (`src/utils/config.py`)
- Unit testing setup (`pytest`)
- DVC setup with DagsHub (`dvc pull`)
- MLflow integration (`mlflow ui`)

### âœ… Completed (Lab 03)

- REST API development (`src/app.py` with Flask)
- API documentation (`flasgger` for `/apidocs/`)
- Manual API documentation (`API_Documentation.md`)
- Model serving (v1 and v2 endpoints)
- API testing (`tests/test_api.py`)

### ğŸ”œ Upcoming (Future Labs)

- Containerization (Docker)
- Cloud deployment (AWS/GCP/Azure)
- CI/CD pipelines (GitHub Actions)
- Monitoring and logging

---

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## Acknowledgments

- **Dataset**: [Telco Customer Churn](https://www.kaggle.com/blastchar/telco-customer-churn) from Kaggle
- **MLflow**: Databricks for the open-source experiment tracking platform
- **DVC**: Iterative for data version control
- **DagsHub**: For providing free DVC remote storage and MLflow hosting

---

## Contact

**Instructor**: Mohammad M. Ajallooeian
**Course**: CMPT 2500 - Machine Learning Deployment and Software Development
**Institution**: NorQuest College

---

**Version**: 3.0.0
**Last Updated**: 2025-10-30
